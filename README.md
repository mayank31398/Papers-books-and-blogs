This repository contains a list of the books, blogs, research papers and white papers that I have read and found interesting.

[Book]: https://img.shields.io/static/v1?label=&message=Book&color=red
[Blog]: https://img.shields.io/static/v1?label=&message=Blog&color=brightgreen
[Paper]: https://img.shields.io/static/v1?label=&message=Paper&color=blueviolet
[Whitepaper]: https://img.shields.io/static/v1?label=&message=Whitepaper&color=yellow
[Activation Function]: https://img.shields.io/static/v1?label=&message=Activation%20Function&color=blue
[Adversarial Examples]: https://img.shields.io/static/v1?label=&message=Adversarial%20Examples&color=blue
[Adversarial Learning]: https://img.shields.io/static/v1?label=&message=Adversarial%20Learning&color=blue
[Animation]: https://img.shields.io/static/v1?label=&message=Animation&color=blue
[Approximate Inference]: https://img.shields.io/static/v1?label=&message=Approximate%20Inference&color=blue
[Audio]: https://img.shields.io/static/v1?label=&message=Audio&color=blue
[Backpropagation]: https://img.shields.io/static/v1?label=&message=Backpropagation&color=blue
[Behavior and Control]: https://img.shields.io/static/v1?label=&message=Behavior%20and%20Control&color=blue
[Biology]: https://img.shields.io/static/v1?label=&message=Biology&color=blue
[Boltzmann Machines]: https://img.shields.io/static/v1?label=&message=Boltzmann%20Machines&color=blue
[Cloning]: https://img.shields.io/static/v1?label=&message=Cloning&color=blue
[Collaborative Filtering]: https://img.shields.io/static/v1?label=&message=Collaborative%20Filtering&color=blue
[Complex Numbers]: https://img.shields.io/static/v1?label=&message=Complex%20Numbers&color=blue
[Compression]: https://img.shields.io/static/v1?label=&message=Compression&color=blue
[Computer Architecture]: https://img.shields.io/static/v1?label=&message=Computer%20Architecture&color=blue
[Computer Graphics]: https://img.shields.io/static/v1?label=&message=Computer%20Graphics&color=blue
[Computer Vision]: https://img.shields.io/static/v1?label=&message=Computer%20Vision&color=blue
[Constituency Parsing]: https://img.shields.io/static/v1?label=&message=Constituency%20Parsing&color=blue
[Convergence]: https://img.shields.io/static/v1?label=&message=Convergence&color=blue
[Curriculum Learning]: https://img.shields.io/static/v1?label=&message=Curriculum%20Learning&color=blue
[Data Visualization]: https://img.shields.io/static/v1?label=&message=Data%20Visualization&color=blue
[Dataset]: https://img.shields.io/static/v1?label=&message=Dataset&color=blue
[Deep Learning]: https://img.shields.io/static/v1?label=&message=Deep%20Learning&color=blue
[Dialog]: https://img.shields.io/static/v1?label=&message=Dialog&color=blue
[Differential Equations]: https://img.shields.io/static/v1?label=&message=Differential%20Equations&color=blue
[Discrete Optimization]: https://img.shields.io/static/v1?label=&message=Discrete%20Optimization&color=blue
[Disentanglement]: https://img.shields.io/static/v1?label=&message=Disentanglement&color=blue
[Distillation]: https://img.shields.io/static/v1?label=&message=Distillation&color=blue
[Distributed Training]: https://img.shields.io/static/v1?label=&message=Distributed%20Training&color=blue
[Embeddings]: https://img.shields.io/static/v1?label=&message=Embeddings&color=blue
[Empirical Risk Minimization]: https://img.shields.io/static/v1?label=&message=Empirical%20Risk%20Minimization&color=blue
[Energy-based Models]: https://img.shields.io/static/v1?label=&message=Energy-based%20Models&color=blue
[Enery based Models]: https://img.shields.io/static/v1?label=&message=Enery%20based%20Models&color=blue
[Entity Linking]: https://img.shields.io/static/v1?label=&message=Entity%20Linking&color=blue
[Error Correction]: https://img.shields.io/static/v1?label=&message=Error%20Correction&color=blue
[Error Detection]: https://img.shields.io/static/v1?label=&message=Error%20Detection&color=blue
[Fake Content Detection]: https://img.shields.io/static/v1?label=&message=Fake%20Content%20Detection&color=blue
[Few Shot]: https://img.shields.io/static/v1?label=&message=Few%20Shot&color=blue
[Generative Models]: https://img.shields.io/static/v1?label=&message=Generative%20Models&color=blue
[Genetic Algorithms]: https://img.shields.io/static/v1?label=&message=Genetic%20Algorithms&color=blue
[Gradient Estimation]: https://img.shields.io/static/v1?label=&message=Gradient%20Estimation&color=blue
[Graph Neural Networks]: https://img.shields.io/static/v1?label=&message=Graph%20Neural%20Networks&color=blue
[Hallucination]: https://img.shields.io/static/v1?label=&message=Hallucination&color=blue
[Image Classification]: https://img.shields.io/static/v1?label=&message=Image%20Classification&color=blue
[Image Super Resolution]: https://img.shields.io/static/v1?label=&message=Image%20Super%20Resolution&color=blue
[Information Retrieval]: https://img.shields.io/static/v1?label=&message=Information%20Retrieval&color=blue
[Information Theory]: https://img.shields.io/static/v1?label=&message=Information%20Theory&color=blue
[Knowledge Graphs]: https://img.shields.io/static/v1?label=&message=Knowledge%20Graphs&color=blue
[Large Models]: https://img.shields.io/static/v1?label=&message=Large%20Models&color=blue
[Machine Translation]: https://img.shields.io/static/v1?label=&message=Machine%20Translation&color=blue
[Memory]: https://img.shields.io/static/v1?label=&message=Memory&color=blue
[Meta Learning]: https://img.shields.io/static/v1?label=&message=Meta%20Learning&color=blue
[Mixture of Experts]: https://img.shields.io/static/v1?label=&message=Mixture%20of%20Experts&color=blue
[Molecular Chemistry]: https://img.shields.io/static/v1?label=&message=Molecular%20Chemistry&color=blue
[Multi-modal]: https://img.shields.io/static/v1?label=&message=Multi-modal&color=blue
[NLP]: https://img.shields.io/static/v1?label=&message=NLP&color=blue
[NP-Hard]: https://img.shields.io/static/v1?label=&message=NP-Hard&color=blue
[Normalization]: https://img.shields.io/static/v1?label=&message=Normalization&color=blue
[Object Detection]: https://img.shields.io/static/v1?label=&message=Object%20Detection&color=blue
[Optimization]: https://img.shields.io/static/v1?label=&message=Optimization&color=blue
[Out-of-Distribution Detection]: https://img.shields.io/static/v1?label=&message=Out-of-Distribution%20Detection&color=blue
[Prompting]: https://img.shields.io/static/v1?label=&message=Prompting&color=blue
[Pruning]: https://img.shields.io/static/v1?label=&message=Pruning&color=blue
[QUBO]: https://img.shields.io/static/v1?label=&message=QUBO&color=blue
[Quantization]: https://img.shields.io/static/v1?label=&message=Quantization&color=blue
[Quantum Algorithms]: https://img.shields.io/static/v1?label=&message=Quantum%20Algorithms&color=blue
[Quantum Computing]: https://img.shields.io/static/v1?label=&message=Quantum%20Computing&color=blue
[Quantum Teleportation]: https://img.shields.io/static/v1?label=&message=Quantum%20Teleportation&color=blue
[Question Answering]: https://img.shields.io/static/v1?label=&message=Question%20Answering&color=blue
[Reading Comprehension]: https://img.shields.io/static/v1?label=&message=Reading%20Comprehension&color=blue
[Recommender Systems]: https://img.shields.io/static/v1?label=&message=Recommender%20Systems&color=blue
[Regularization]: https://img.shields.io/static/v1?label=&message=Regularization&color=blue
[Reinforcement Learning]: https://img.shields.io/static/v1?label=&message=Reinforcement%20Learning&color=blue
[Relational Reasoning]: https://img.shields.io/static/v1?label=&message=Relational%20Reasoning&color=blue
[Retrieval-Augmented Generation]: https://img.shields.io/static/v1?label=&message=Retrieval-Augmented%20Generation&color=blue
[Robotics]: https://img.shields.io/static/v1?label=&message=Robotics&color=blue
[Robustness]: https://img.shields.io/static/v1?label=&message=Robustness&color=blue
[Saliency Detection]: https://img.shields.io/static/v1?label=&message=Saliency%20Detection&color=blue
[Security]: https://img.shields.io/static/v1?label=&message=Security&color=blue
[Spatio-Temporal Reasoning]: https://img.shields.io/static/v1?label=&message=Spatio-Temporal%20Reasoning&color=blue
[Speech]: https://img.shields.io/static/v1?label=&message=Speech&color=blue
[Story Generation]: https://img.shields.io/static/v1?label=&message=Story%20Generation&color=blue
[Style Transfer]: https://img.shields.io/static/v1?label=&message=Style%20Transfer&color=blue
[Summarization]: https://img.shields.io/static/v1?label=&message=Summarization&color=blue
[Systems]: https://img.shields.io/static/v1?label=&message=Systems&color=blue
[Text Classification]: https://img.shields.io/static/v1?label=&message=Text%20Classification&color=blue
[Theory of Computation]: https://img.shields.io/static/v1?label=&message=Theory%20of%20Computation&color=blue
[Time Series]: https://img.shields.io/static/v1?label=&message=Time%20Series&color=blue
[Transfer Learning]: https://img.shields.io/static/v1?label=&message=Transfer%20Learning&color=blue
[Transformers]: https://img.shields.io/static/v1?label=&message=Transformers&color=blue
[Unsupervised Learning]: https://img.shields.io/static/v1?label=&message=Unsupervised%20Learning&color=blue
[Variational Inference]: https://img.shields.io/static/v1?label=&message=Variational%20Inference&color=blue
[Zero Shot]: https://img.shields.io/static/v1?label=&message=Zero%20Shot&color=blue

[AAAI 2020]: https://img.shields.io/static/v1?label=&message=AAAI%202020&color=grey
[ACL 2017]: https://img.shields.io/static/v1?label=&message=ACL%202017&color=grey
[ACL 2018]: https://img.shields.io/static/v1?label=&message=ACL%202018&color=grey
[ACL 2019]: https://img.shields.io/static/v1?label=&message=ACL%202019&color=grey
[ACL 2020]: https://img.shields.io/static/v1?label=&message=ACL%202020&color=grey
[ACL 2021]: https://img.shields.io/static/v1?label=&message=ACL%202021&color=grey
[ACL 2022]: https://img.shields.io/static/v1?label=&message=ACL%202022&color=grey
[ACM SIGGRAPH Computer Graphics 1987]: https://img.shields.io/static/v1?label=&message=ACM%20SIGGRAPH%20Computer%20Graphics%201987&color=grey
[ACM Transactions on Graphics]: https://img.shields.io/static/v1?label=&message=ACM%20Transactions%20on%20Graphics&color=grey
[Amazon]: https://img.shields.io/static/v1?label=&message=Amazon&color=grey
[CVPR 1983]: https://img.shields.io/static/v1?label=&message=CVPR%201983&color=grey
[CVPR 2015]: https://img.shields.io/static/v1?label=&message=CVPR%202015&color=grey
[CVPR 2016]: https://img.shields.io/static/v1?label=&message=CVPR%202016&color=grey
[CVPR 2019]: https://img.shields.io/static/v1?label=&message=CVPR%202019&color=grey
[CoNLL 2016]: https://img.shields.io/static/v1?label=&message=CoNLL%202016&color=grey
[ECCV 2016]: https://img.shields.io/static/v1?label=&message=ECCV%202016&color=grey
[EMNLP 2016]: https://img.shields.io/static/v1?label=&message=EMNLP%202016&color=grey
[EMNLP 2018]: https://img.shields.io/static/v1?label=&message=EMNLP%202018&color=grey
[EMNLP 2019]: https://img.shields.io/static/v1?label=&message=EMNLP%202019&color=grey
[EMNLP 2020]: https://img.shields.io/static/v1?label=&message=EMNLP%202020&color=grey
[EMNLP 2021]: https://img.shields.io/static/v1?label=&message=EMNLP%202021&color=grey
[Frontiers in Computational Neuroscience 2017]: https://img.shields.io/static/v1?label=&message=Frontiers%20in%20Computational%20Neuroscience%202017&color=grey
[HuggingFace]: https://img.shields.io/static/v1?label=&message=HuggingFace&color=grey
[IBM Journal of Research and Development 2004]: https://img.shields.io/static/v1?label=&message=IBM%20Journal%20of%20Research%20and%20Development%202004&color=grey
[ICCV 2017]: https://img.shields.io/static/v1?label=&message=ICCV%202017&color=grey
[ICLR 2013]: https://img.shields.io/static/v1?label=&message=ICLR%202013&color=grey
[ICLR 2014]: https://img.shields.io/static/v1?label=&message=ICLR%202014&color=grey
[ICLR 2015]: https://img.shields.io/static/v1?label=&message=ICLR%202015&color=grey
[ICLR 2016]: https://img.shields.io/static/v1?label=&message=ICLR%202016&color=grey
[ICLR 2017]: https://img.shields.io/static/v1?label=&message=ICLR%202017&color=grey
[ICLR 2018]: https://img.shields.io/static/v1?label=&message=ICLR%202018&color=grey
[ICLR 2019]: https://img.shields.io/static/v1?label=&message=ICLR%202019&color=grey
[ICLR 2020]: https://img.shields.io/static/v1?label=&message=ICLR%202020&color=grey
[ICLR 2021]: https://img.shields.io/static/v1?label=&message=ICLR%202021&color=grey
[ICLR 2022]: https://img.shields.io/static/v1?label=&message=ICLR%202022&color=grey
[ICML 2003]: https://img.shields.io/static/v1?label=&message=ICML%202003&color=grey
[ICML 2007]: https://img.shields.io/static/v1?label=&message=ICML%202007&color=grey
[ICML 2009]: https://img.shields.io/static/v1?label=&message=ICML%202009&color=grey
[ICML 2015]: https://img.shields.io/static/v1?label=&message=ICML%202015&color=grey
[ICML 2017]: https://img.shields.io/static/v1?label=&message=ICML%202017&color=grey
[ICML 2018]: https://img.shields.io/static/v1?label=&message=ICML%202018&color=grey
[ICML 2020]: https://img.shields.io/static/v1?label=&message=ICML%202020&color=grey
[ICML 2021]: https://img.shields.io/static/v1?label=&message=ICML%202021&color=grey
[ICML 2022]: https://img.shields.io/static/v1?label=&message=ICML%202022&color=grey
[IEEE HPCA 2014]: https://img.shields.io/static/v1?label=&message=IEEE%20HPCA%202014&color=grey
[IEEE ISCA 2012]: https://img.shields.io/static/v1?label=&message=IEEE%20ISCA%202012&color=grey
[IEEE ISCA 2014]: https://img.shields.io/static/v1?label=&message=IEEE%20ISCA%202014&color=grey
[IEEE Information Theory Workshop 2015]: https://img.shields.io/static/v1?label=&message=IEEE%20Information%20Theory%20Workshop%202015&color=grey
[IEEE International Memory Worksop 2013]: https://img.shields.io/static/v1?label=&message=IEEE%20International%20Memory%20Worksop%202013&color=grey
[IEEE MICRO 2007]: https://img.shields.io/static/v1?label=&message=IEEE%20MICRO%202007&color=grey
[IEEE Symposium on Security and Privacy 2017]: https://img.shields.io/static/v1?label=&message=IEEE%20Symposium%20on%20Security%20and%20Privacy%202017&color=grey
[IEEE TASLP 2019]: https://img.shields.io/static/v1?label=&message=IEEE%20TASLP%202019&color=grey
[IEEE TASLP 2022]: https://img.shields.io/static/v1?label=&message=IEEE%20TASLP%202022&color=grey
[IJCAI 2018]: https://img.shields.io/static/v1?label=&message=IJCAI%202018&color=grey
[IJCAI 2022]: https://img.shields.io/static/v1?label=&message=IJCAI%202022&color=grey
[JMLR 2008]: https://img.shields.io/static/v1?label=&message=JMLR%202008&color=grey
[Journal of Visual Communication and Image Representation 2016]: https://img.shields.io/static/v1?label=&message=Journal%20of%20Visual%20Communication%20and%20Image%20Representation%202016&color=grey
[Journal of Visual Communication and Image Representation 2019]: https://img.shields.io/static/v1?label=&message=Journal%20of%20Visual%20Communication%20and%20Image%20Representation%202019&color=grey
[MCSS 1989]: https://img.shields.io/static/v1?label=&message=MCSS%201989&color=grey
[Meta]: https://img.shields.io/static/v1?label=&message=Meta&color=grey
[Microsoft]: https://img.shields.io/static/v1?label=&message=Microsoft&color=grey
[NAACL 2019]: https://img.shields.io/static/v1?label=&message=NAACL%202019&color=grey
[NAACL 2021]: https://img.shields.io/static/v1?label=&message=NAACL%202021&color=grey
[Nature 1982]: https://img.shields.io/static/v1?label=&message=Nature%201982&color=grey
[Nature 1986]: https://img.shields.io/static/v1?label=&message=Nature%201986&color=grey
[NeurIPS 1987]: https://img.shields.io/static/v1?label=&message=NeurIPS%201987&color=grey
[NeurIPS 2012]: https://img.shields.io/static/v1?label=&message=NeurIPS%202012&color=grey
[NeurIPS 2013]: https://img.shields.io/static/v1?label=&message=NeurIPS%202013&color=grey
[NeurIPS 2014]: https://img.shields.io/static/v1?label=&message=NeurIPS%202014&color=grey
[NeurIPS 2015]: https://img.shields.io/static/v1?label=&message=NeurIPS%202015&color=grey
[NeurIPS 2016]: https://img.shields.io/static/v1?label=&message=NeurIPS%202016&color=grey
[NeurIPS 2017]: https://img.shields.io/static/v1?label=&message=NeurIPS%202017&color=grey
[NeurIPS 2018]: https://img.shields.io/static/v1?label=&message=NeurIPS%202018&color=grey
[NeurIPS 2020]: https://img.shields.io/static/v1?label=&message=NeurIPS%202020&color=grey
[PNAS 1982]: https://img.shields.io/static/v1?label=&message=PNAS%201982&color=grey
[Physical Review Journals 1935]: https://img.shields.io/static/v1?label=&message=Physical%20Review%20Journals%201935&color=grey
[Physical Review Journals 1993]: https://img.shields.io/static/v1?label=&message=Physical%20Review%20Journals%201993&color=grey
[Physical Review Letters 1998]: https://img.shields.io/static/v1?label=&message=Physical%20Review%20Letters%201998&color=grey
[Proceedings of the Royal Society 1985]: https://img.shields.io/static/v1?label=&message=Proceedings%20of%20the%20Royal%20Society%201985&color=grey
[Proceedings of the Royal Society 1992]: https://img.shields.io/static/v1?label=&message=Proceedings%20of%20the%20Royal%20Society%201992&color=grey
[SC 2020]: https://img.shields.io/static/v1?label=&message=SC%202020&color=grey
[SC 2021]: https://img.shields.io/static/v1?label=&message=SC%202021&color=grey
[SIAM Journal on Computing 1997]: https://img.shields.io/static/v1?label=&message=SIAM%20Journal%20on%20Computing%201997&color=grey
[SIGDIAL 2020]: https://img.shields.io/static/v1?label=&message=SIGDIAL%202020&color=grey
[SIGIR 2020]: https://img.shields.io/static/v1?label=&message=SIGIR%202020&color=grey
[STOC 1996]: https://img.shields.io/static/v1?label=&message=STOC%201996&color=grey
[SysML 2018]: https://img.shields.io/static/v1?label=&message=SysML%202018&color=grey
[The Bell System Technical Journal 1950]: https://img.shields.io/static/v1?label=&message=The%20Bell%20System%20Technical%20Journal%201950&color=grey
[Tsinghua University]: https://img.shields.io/static/v1?label=&message=Tsinghua%20University&color=grey
[USENIX Security Symposium 2007]: https://img.shields.io/static/v1?label=&message=USENIX%20Security%20Symposium%202007&color=grey
[Yandex]: https://img.shields.io/static/v1?label=&message=Yandex&color=grey

### Table of contents
- [AI, DL, NLP and RL](#ai-dl-nlp-and-rl)
- [Calculus](#calculus)
- [Computer architecture](#computer-architecture)
- [Computer graphics](#computer-graphics)
- [Data structures and algorithms](#data-structures-and-algorithms)
- [Digital electronics](#digital-electronics)
- [Graph theory](#graph-theory)
- [Information theory](#information-theory)
- [Linear algebra](#linear-algebra)
- [Measure theory](#measure-theory)
- [Optimization theory](#optimization-theory)
- [Probability and stochastic processes](#probability-and-stochastic-processes)
- [Quantum computing](#quantum-computing)
- [Signal processing](#signal-processing)

# AI, DL, NLP and RL
1. Artificial Intelligence: a modern approach  
*Stuart Russell, Peter Norvig*  
![image][Book]
1. Deep learning  
*Ian Goodfellow, Yoshua Bengio, Aaron Courville*  
![image][Book]
1. Genetic algorithms in search, optimization and machine learning  
*David E. Goldberg*  
![image][Book]
1. Machine learning  
*Tom M. Mitchell*  
![image][Book]
1. Machine learning: a probabilistic perspective  
*Kevin P. Murphy*  
![image][Book]
1. Neural network methods for natural language processing  
*Yaov Goldberg*  
![image][Book]
1. Neural networks for pattern recognition  
*Christopher M. Bishop*  
![image][Book]
1. Pattern classification  
*Richard O. Duda, Peter E. Hart, David G. Stork*  
![image][Book]
1. Pattern recognition and machine learning  
*Christopher M. Bishop*  
![image][Book]
1. Reinforcement Learning: An Introduction  
*Richard S. Sutton, Andrew G. Barto*  
![image][Book]
1. Speech and language processing  
*Daniel Jurafsky, James H. Martin*  
![image][Book]
1. The elements of statistical learning: data mining, inference and prediction  
*Trevor Hastie, Robert Tibshirani, Jerome Friedman*  
![image][Book]
1. [A gentle introduction to 8-bit matrix multiplication for transformers at scale using Hugging Face transformers, accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)  
*Younes Belkada, Tim Dettmers*  
![image][Blog] ![image][HuggingFace] ![image][Deep Learning] ![image][Quantization] ![image][Transformers]
1. [DeepSpeed compression: a composable library for extreme compression and zero-cost quantization](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)  
*DeepSpeed Team, Andrey Proskurin*  
![image][Blog] ![image][Microsoft] ![image][Compression] ![image][Deep Learning] ![image][Quantization]
1. [DeepSpeed powers 8x larger MoE model training with high performance](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)  
*DeepSpeed Team, Z-code Team*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts]
1. [DeepSpeed: accelerating large-scale model inference and training via system optimizations and compression](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/#:~:text=DeepSpeed%20Inference%20also%20supports%20fast,multiple%20GPUs%20for%20parallel%20execution.)  
*DeepSpeed Team, Rangan Majumder, Andrey Proskurin*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Large Models]
1. [DeepSpeed: advancing MoE inference and training to power next-generation AI scale](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)  
*DeepSpeed Team, Andrey Proskurin*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts]
1. [GLM-130B: an open bilingual pre-trained model](https://keg.cs.tsinghua.edu.cn/glm-130b/#fn:1)  
![image][Blog] ![image][Tsinghua University] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Incredibly fast BLOOM inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)  
*Stas Bekman, Sylvain Gugger*  
![image][Blog] ![image][HuggingFace] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Introducing Turing image super resolution: AI powered image enhancements for Microsoft Edge and Bing maps](https://blogs.bing.com/search-quality-insights/may-2022/Turing-Image-Super-Resolution)  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Image Super Resolution]
1. [Making DeepSpeed ZeRO run efficiently on more-affordable hardware](https://www.amazon.science/blog/making-deepspeed-zero-run-efficiently-on-more-affordable-hardware)  
*Justin Chiu, Shuai Zheng*  
![image][Blog] ![image][Amazon] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Supporting efficient large model training on AMD Instinct<sup>TM</sup> GPUs with DeepSpeed](https://cloudblogs.microsoft.com/opensource/2022/03/21/supporting-efficient-large-model-training-on-amd-instinct-gpus-with-deepspeed/)  
*Olatunji Ruwase, Jeff Rasley*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Turing-NLG: a 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)  
*Corby Rosset*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Large Models] ![image][Transformers]
1. [Understanding the Open Pre-Trained Transformers (OPT) library](https://towardsdatascience.com/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)  
*Cameron Wolfe*  
![image][Blog] ![image][Meta] ![image][Deep Learning] ![image][Transformers]
1. [Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, the world’s largest and most powerful generative language model](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)  
*Ali Alvi, Paresh Kharya*  
![image][Blog] ![image][Microsoft] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Yandex publishes YaLM 100B. It’s the largest GPT-like neural network in open source](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6)  
*Mikhail Khrushchev*  
![image][Blog] ![image][Yandex] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [ZeRO & DeepSpeed: new system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)  
*DeepSpeed Team, Rangan Majumder, Junhua Wang*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [ZeRO-2 & DeepSpeed: shattering barriers of deep learning speed & scale](https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/)  
*DeepSpeed Team, Rangan Majumder, Junhua Wang*  
![image][Blog] ![image][Microsoft] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [1-bit Adam: communication efficient large-scale training with Adam’s convergence speed](https://arxiv.org/abs/2102.02888)  
*Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He*  
![image][Paper] ![image][Deep Learning] ![image][Distributed Training]
1. [8-bit optimizers via block-wise quantization](https://arxiv.org/abs/2110.02861)  
*Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer*  
![image][Paper] ![image][ICLR 2022] ![image][Compression] ![image][Deep Learning] ![image][Quantization]
1. [A 'neural' network that learns to play Backgammon](https://papers.nips.cc/paper/30-a-neural-network-that-learns-to-play-backgammon)  
*Gerald Tesauro, Terrence J. Sejnowski*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning]
1. [A deep reinforced model for abstractive summarization](https://arxiv.org/abs/1705.04304)  
*Romain Paulus, Caiming Xiong, Richard Socher*  
![image][Paper] ![image][ICLR 2018] ![image][NLP] ![image][Reinforcement Learning] ![image][Summarization]
1. [A dynamical approach to temporal pattern processing](https://papers.nips.cc/paper/76-a-dynamical-approach-to-temporal-pattern-processing)  
*W. Scott Stornetta, Tad Hogg, Bernardo A. Huberman*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning]
1. [A few more examples may be worth billions of parameters](https://arxiv.org/abs/2110.04374)  
*Yuval Kirstain, Patrick Lewis, Sebastian Riedel, Omer Levy*  
![image][Paper] ![image][Few Shot] ![image][NLP]
1. [A general and adaptive robust loss function](https://arxiv.org/abs/1701.03077)  
*Jonathan T. Barron*  
![image][Paper] ![image][CVPR 2019] ![image][Deep Learning]
1. [A note on the evaluation of generative models](https://arxiv.org/abs/1511.01844)  
*Lucas Theis, Aäron van den Oord, Matthias Bethge*  
![image][Paper] ![image][ICLR 2016] ![image][Deep Learning] ![image][Generative Models]
1. [A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings](https://arxiv.org/abs/1805.06297)  
*Mikel Artetxe, Gorka Labaka, Eneko Agirre*  
![image][Paper] ![image][ACL 2018] ![image][Embeddings] ![image][NLP]
1. [A simple but tough-to-beat baseline for sentence embeddings](https://openreview.net/forum?id=SyK00v5xx)  
*Sanjeev Arora, Yingyu Liang, Tengyu Ma*  
![image][Paper] ![image][ICLR 2017] ![image][Embeddings] ![image][NLP]
1. [A simple language model for task-oriented dialogue](https://arxiv.org/abs/2005.00796)  
*Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, Richard Socher*  
![image][Paper] ![image][NeurIPS 2020] ![image][Dialog] ![image][NLP]
1. [A simple neural attentive meta-learner](https://arxiv.org/abs/1707.03141)  
*Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel*  
![image][Paper] ![image][ICLR 2018] ![image][Deep Learning] ![image][Few Shot] ![image][Meta Learning]
1. [A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)  
*Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap*  
![image][Paper] ![image][NeurIPS 2017] ![image][Deep Learning] ![image][Relational Reasoning]
1. [A style-based generator architecture for generative adversarial networks](https://arxiv.org/abs/1812.04948)  
*Tero Karras, Samuli Laine, Timo Aila*  
![image][Paper] ![image][CVPR 2019] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [A stylometric inquiry into hyperpartisan and fake news](https://arxiv.org/abs/1702.05638)  
*Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, Benno Stein*  
![image][Paper] ![image][ACL 2018] ![image][Fake Content Detection] ![image][NLP]
1. [A3T: adversarially augmented adversarial training](https://arxiv.org/abs/1801.04055)  
*Akram Erraqabi, Aristide Baratin, Yoshua Bengio, Simon Lacoste-Julien*  
![image][Paper] ![image][NeurIPS 2017] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Adversarial approximate inference for speech to electroglottograph conversion](https://arxiv.org/abs/1903.12248)  
*Prathosh A. P., Varun Srivastava, Mayank Mishra*  
![image][Paper] ![image][IEEE TASLP 2019] ![image][Adversarial Learning] ![image][Approximate Inference] ![image][Deep Learning] ![image][Generative Models] ![image][Speech]
1. [Adversarial autoencoders](https://arxiv.org/abs/1511.05644)  
*Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey*  
![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Adversarial examples that fool both computer vision and time-limited humans](https://arxiv.org/abs/1802.08195)  
*Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein*  
![image][Paper] ![image][NeurIPS 2018] ![image][Adversarial Examples] ![image][Deep Learning]
1. [Adversarial feature learning](https://arxiv.org/abs/1605.09782)  
*Jeff Donahue, Philipp Krähenbühl, Trevor Darrell*  
![image][Paper] ![image][ICLR 2017] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Adversarial generation of natural language](https://arxiv.org/abs/1705.10929)  
*Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, Aaron Courville*  
![image][Paper] ![image][ACL 2017] ![image][Adversarial Learning] ![image][Generative Models] ![image][NLP]
1. [Adversarial information factorization](https://arxiv.org/abs/1711.05175)  
*Antonia Creswell, Yumnah Mohamied, Biswa Sengupta, Anil A Bharath*  
![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Adversarially learned inference](https://arxiv.org/abs/1606.00704)  
*Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, Aaron Courville*  
![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [AlexaTM 20B: few-shot learning using a large-scale multilingual seq2seq model](https://arxiv.org/abs/2208.01448)  
*Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan*  
![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Amazon SageMaker model parallelism: a general and flexible framework for large model training](https://arxiv.org/abs/2111.05972)  
*Can Karakus, Rahul Huilgol, Fei Wu, Anirudh Subramanian, Cade Daniel, Derya Cavdar, Teng Xu, Haohan Chen, Arash Rahnama, Luis Quintela*  
![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [An image is worth 16x16 words: transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)  
*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby*  
![image][Paper] ![image][ICLR 2021] ![image][Computer Vision]
1. [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)  
*Sebastian Ruder*  
![image][Paper] ![image][Deep Learning] ![image][Optimization]
1. [Analysing mathematical reasoning abilities of neural models](https://arxiv.org/abs/1904.01557)  
*David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli*  
![image][Paper] ![image][ICLR 2019] ![image][Deep Learning]
1. [Approximation by superpositions of sigmoidal function](https://link.springer.com/article/10.1007/BF02551274)  
*George Cybenko*  
![image][Paper] ![image][MCSS 1989] ![image][Deep Learning]
1. [Aspect based sentiment analysis with gated convolutional networks](https://arxiv.org/abs/1805.07043)  
*Wei Xue, Tao Li*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Text Classification]
1. [Attention is all you need](https://arxiv.org/abs/1706.03762)  
*Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin*  
![image][Paper] ![image][NeurIPS 2017] ![image][Deep Learning] ![image][Transformers]
1. [Auto-encoding variational Bayes](https://arxiv.org/abs/1312.6114)  
*Diederik P. Kingma, Max Welling*  
![image][Paper] ![image][ICLR 2014] ![image][Deep Learning] ![image][Generative Models] ![image][Variational Inference]
1. [Backpropagation through the void: optimizing control variates for black-box gradient estimation](https://arxiv.org/abs/1711.00123)  
*Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, David Duvenaud*  
![image][Paper] ![image][ICLR 2018] ![image][Backpropagation] ![image][Deep Learning] ![image][Discrete Optimization] ![image][Gradient Estimation] ![image][Optimization] ![image][Reinforcement Learning] ![image][Variational Inference]
1. [BART: denoising sequence-to-sequence pre-training for natural language generation, translation and comprehension](https://arxiv.org/abs/1910.13461)  
*Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer*  
![image][Paper] ![image][ACL 2020] ![image][NLP] ![image][Transformers]
1. [Batch normalization: accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167)  
*Sergey Ioffe, Christian Szegedy*  
![image][Paper] ![image][ICML 2015] ![image][Deep Learning] ![image][Normalization] ![image][Optimization]
1. [Behavioral cloning from observation](https://arxiv.org/abs/1805.01954)  
*Faraz Torabi, Garrett Warnell, Peter Stone*  
![image][Paper] ![image][IJCAI 2018] ![image][Behavior and Control] ![image][Deep Learning] ![image][Genetic Algorithms] ![image][Robotics]
1. [BERT: pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)  
*Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova*  
![image][Paper] ![image][NAACL 2019] ![image][NLP] ![image][Transformers]
1. [Beyond domain APIs: Task-oriented conversational modeling with unstructured knowledge access](https://arxiv.org/abs/2006.03533)  
*Seokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, Dilek Hakkani-Tur*  
![image][Paper] ![image][SIGDIAL 2020] ![image][Dialog] ![image][NLP]
1. [Bootstrapping entity alignment with knowledge graph embedding](https://www.ijcai.org/proceedings/2018/611)  
*Zequn Sun, Wei Hu, Qingheng Zhang, Yuzhong Qu*  
![image][Paper] ![image][IJCAI 2018] ![image][Embeddings] ![image][Knowledge Graphs] ![image][NLP]
1. [Bridging the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation](https://www.aclweb.org/anthology/2020.emnlp-main.275/)  
*Xiuyi Chen, Fandong Meng, Peng Li, Feilong Chen, Shuang Xu, Bo Xu, Jie Zhou*  
![image][Paper] ![image][EMNLP 2020] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [ColBERT: efficient and effective passage search via contextualized late interaction over BERT](https://arxiv.org/abs/2004.12832)  
*Omar Khattab, Matei Zaharia*  
![image][Paper] ![image][SIGIR 2020] ![image][Information Retrieval] ![image][NLP]
1. [Colossal-AI: a unified deep learning system for large-scale parallel training](https://arxiv.org/abs/2110.14883)  
*Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuanrui Wang, Fan Cui, Yang You*  
![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Compiling machine learning programs via high-level tracing](https://research.google/pubs/pub47008/)  
*Roy Frostig, Matthew Johnson, Chris Leary*  
![image][Paper] ![image][SysML 2018] ![image][Deep Learning] ![image][Systems]
1. [Conceptual captions: a cleaned, hypernymed, image alt-text dataset for automatic image captioning](https://aclanthology.org/P18-1238/)  
*Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut*  
![image][Paper] ![image][ACL 2018] ![image][Computer Vision] ![image][Dataset] ![image][NLP]
1. [Conditional image synthesis with auxilliary classifier GANs](https://arxiv.org/abs/1610.09585)  
*Augustus Odena, Christopher Olah, Jonathon Shlens*  
![image][Paper] ![image][ICML 2017] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Connectivity versus entropy](https://papers.nips.cc/paper/63-connectivity-versus-entropy)  
*Yaser S. Abu-Mostafa*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning]
1. [Constituency parsing with a self-attentive encoder](https://arxiv.org/abs/1805.01052)  
*Nikita Kitaev, Dan Klein*  
![image][Paper] ![image][ACL 2018] ![image][Constituency Parsing] ![image][NLP]
1. [Constraint based knowledge base distillation in end-to-end task oriented dialogs](https://arxiv.org/abs/2109.07396)  
*Dinesh Raghu, Atishya Jain, Mausam, Sachindra Joshi*  
![image][Paper] ![image][ACL 2021] ![image][Dialog] ![image][NLP]
1. [Convolutional networks for graphs for learning molecular fingerprints](https://arxiv.org/abs/1509.09292)  
*David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, Ryan P. Adams*  
![image][Paper] ![image][NeurIPS 2015] ![image][Deep Learning] ![image][Molecular Chemistry]
1. [Convolutional neural network language models](https://aclanthology.org/D16-1123/)  
*Ngoc-Quan Pham, Germán Kruszewski, Gemma Boleda*  
![image][Paper] ![image][EMNLP 2016] ![image][NLP]
1. [Countering adversarial images using input transformations](https://arxiv.org/abs/1711.00117)  
*Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Curriculum learning](https://dl.acm.org/citation.cfm?id=1553380)  
*Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston*  
![image][Paper] ![image][ICML 2009] ![image][Curriculum Learning] ![image][Deep Learning]
1. [Cutting down on prompts and parameters: simple few-shot learning with language models](https://arxiv.org/abs/2106.13353)  
*Robert L. Logan IV, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel*  
![image][Paper] ![image][ACL 2022] ![image][Few Shot] ![image][NLP]
1. [Deep complex networks](https://arxiv.org/abs/1705.09792)  
*Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, João Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, Christopher J Pal*  
![image][Paper] ![image][ICLR 2018] ![image][Complex Numbers] ![image][Deep Learning]
1. [Deep learning and the information bottleneck principle](https://arxiv.org/abs/1503.02406)  
*Naftali Tishby, Noga Zaslavsky*  
![image][Paper] ![image][IEEE Information Theory Workshop 2015] ![image][Deep Learning] ![image][Information Theory]
1. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385)  
*Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun*  
![image][Paper] ![image][CVPR 2016] ![image][Computer Vision] ![image][Image Classification]
1. [Deep text classification can be fooled](https://arxiv.org/abs/1704.08006)  
*Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, Wenchang Shi*  
![image][Paper] ![image][IJCAI 2018] ![image][Adversarial Examples] ![image][NLP]
1. [DeepSpeed Inference: enabling efficient inference of transformer models at unprecedented scale](https://arxiv.org/abs/2207.00032)  
*Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He*  
![image][Paper] ![image][Deep Learning] ![image][Large Models]
1. [Denoising distantly supervised open-domain question answering](https://aclanthology.org/P18-1161/)  
*Yankai Lin, Haozhe Ji, Zhiyuan Liu, Maosong Sun*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Question Answering]
1. [Diffusion convolutional recurrent neural network: data-driven traffic forecasting](https://arxiv.org/abs/1707.01926)  
*Yaguang Li, Rose Yu, Cyrus Shahabi, Yan Liu*  
![image][Paper] ![image][ICLR 2018] ![image][Deep Learning] ![image][Graph Neural Networks] ![image][Spatio-Temporal Reasoning] ![image][Time Series]
1. [Discrete variational autoencoders](https://arxiv.org/abs/1609.02200)  
*Jason Tyler Rolfe*  
![image][Paper] ![image][ICLR 2017] ![image][Deep Learning] ![image][Generative Models] ![image][Variational Inference]
1. [Disentangling by factorising](https://arxiv.org/abs/1802.05983)  
*Hyunjik Kim, Andriy Mnih*  
![image][Paper] ![image][ICML 2018] ![image][Deep Learning] ![image][Disentanglement] ![image][Generative Models] ![image][Variational Inference]
1. [Disentangling language and knowledge in task-oriented dialogs](https://arxiv.org/abs/1805.01216)  
*Dinesh Raghu, Nikhil Gupta, Mausam*  
![image][Paper] ![image][NAACL 2019] ![image][Dialog] ![image][Disentanglement] ![image][NLP]
1. [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781)  
*Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean*  
![image][Paper] ![image][ICLR 2013] ![image][Embeddings] ![image][NLP]
1. [Efficient large scale language modeling with mixtures of experts](https://arxiv.org/abs/2112.10684)  
*Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov*  
![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Efficient large-scale language model training on GPU clusters using Megatron-LM](https://arxiv.org/abs/2104.04473)  
*Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia*  
![image][Paper] ![image][SC 2021] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Enchancing the reliability of out-of-distribution image detection in neural networks](https://arxiv.org/abs/1706.02690)  
*Shiyu Liang, Yixuan Li, R. Srikant*  
![image][Paper] ![image][ICLR 2018] ![image][Deep Learning] ![image][Out-of-Distribution Detection]
1. [End-to-end task-oriented dialog modeling with semi-structured knowledge management](https://arxiv.org/abs/2106.11796)  
*Silin Gao, Ryuichi Takanobu, Antoine Bosselut, Minlie Huang*  
![image][Paper] ![image][IEEE TASLP 2022] ![image][Dialog] ![image][NLP]
1. [Ensemble adversarial training: attacks and defenses](https://arxiv.org/abs/1705.07204)  
*Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Equilibrium propagation: bridging the gap between energy-based models and backpropagation](https://arxiv.org/abs/1602.05179)  
*Benjamin Scellier, Yoshua Bengio*  
![image][Paper] ![image][Frontiers in Computational Neuroscience 2017] ![image][Backpropagation] ![image][Deep Learning] ![image][Enery based Models]
1. [Exemplar encoder-decoder for neural conversation generation](https://www.aclweb.org/anthology/P18-1123/)  
*Gaurav Pandey, Danish Contractor, Vineet Kumar, Sachindra Joshi*  
![image][Paper] ![image][ACL 2018] ![image][Dialog] ![image][NLP]
1. [Exploring deep recurrent models with reinforcement learning for molecule design](https://openreview.net/forum?id=HkcTe-bR-)  
*Daniel Neil, Marwin Segler, Laura Guasch, Mohamed Ahmed, Dean Plumbley, Matthew Sellwood, Nathan Brown*  
![image][Paper] ![image][ICLR 2018] ![image][Molecular Chemistry] ![image][Reinforcement Learning]
1. [Extreme compression for pre-trained transformers made simple and efficient](https://arxiv.org/abs/2206.01859)  
*Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He*  
![image][Paper] ![image][Compression] ![image][Deep Learning] ![image][Distillation] ![image][Large Models] ![image][Pruning] ![image][Transformers]
1. [Fast abstractive summarization with reinforce-selected sentence rewriting](https://arxiv.org/abs/1805.11080)  
*Yen-Chun Chen, Mohit Bansal*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Reinforcement Learning] ![image][Summarization]
1. [FFJORD: Free-form continuous dynamics for scalable reversible generative models](https://arxiv.org/abs/1810.01367)  
*Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud*  
![image][Paper] ![image][ICLR 2019] ![image][Deep Learning] ![image][Generative Models]
1. [Finetuned language models are zero-shot learners](https://arxiv.org/abs/2109.01652)  
*Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le*  
![image][Paper] ![image][ICLR 2022] ![image][NLP] ![image][Transformers] ![image][Zero Shot]
1. [Generating adversarial examples with adversarial networks](https://www.ijcai.org/proceedings/2018/543)  
*Chaowei Xiao, Bo Li, Jun-yan Zhu, Warren He, Mingyan Liu, Dawn Song*  
![image][Paper] ![image][IJCAI 2018] ![image][Adversarial Examples] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Generating sentences from a continuous space](https://arxiv.org/abs/1511.06349)  
*Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio*  
![image][Paper] ![image][CoNLL 2016] ![image][NLP]
1. [Generation-augmented retrieval for open-domain question answering](https://arxiv.org/abs/2009.08553)  
*Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen*  
![image][Paper] ![image][ACL 2021] ![image][NLP] ![image][Question Answering]
1. [Generative adversarial nets](https://arxiv.org/abs/1406.2661)  
*Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio*  
![image][Paper] ![image][NeurIPS 2014] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [GeoMAN: multi-level attention networks for geo-sensory time series prediction](https://www.ijcai.org/proceedings/2018/476)  
*Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, Yu Zheng*  
![image][Paper] ![image][IJCAI 2018] ![image][Deep Learning] ![image][Spatio-Temporal Reasoning] ![image][Time Series]
1. [Going deeper with convolutions](https://arxiv.org/abs/1409.4842)  
*Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich*  
![image][Paper] ![image][CVPR 2015] ![image][Computer Vision] ![image][Image Classification]
1. [GPT-NeoX-20B: an open-source autoregressive language model](https://arxiv.org/abs/2204.06745)  
*Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach*  
![image][Paper] ![image][ACL 2022] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Gradient-based hyperparameter optimization through reversible learning](https://arxiv.org/abs/1502.03492)  
*Dougal Maclaurin, David Duvenaud, Ryan P. Adams*  
![image][Paper] ![image][ICML 2015] ![image][Deep Learning] ![image][Meta Learning]
1. [Graph attention networks](https://arxiv.org/abs/1710.10903)  
*Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio*  
![image][Paper] ![image][ICLR 2018] ![image][Deep Learning] ![image][Graph Neural Networks]
1. [Hierarchical neural story generation](https://arxiv.org/abs/1805.04833)  
*Angela Fan, Mike Lewis, Yann Dauphin*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Story Generation]
1. [Hindsight: posterior-guided training of retrievers for improved open-ended generation](https://arxiv.org/abs/2110.07752)  
*Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning*  
![image][Paper] ![image][ICLR 2022] ![image][NLP] ![image][Retrieval-Augmented Generation] ![image][Variational Inference]
1. [HotFlip: white-box adversarial examples for text classification](https://arxiv.org/abs/1712.06751)  
*Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou*  
![image][Paper] ![image][ACL 2018] ![image][Adversarial Examples] ![image][NLP]
1. [HyKnow: end-to-end task-oriented dialog modeling with hybrid knowledge management](https://arxiv.org/abs/2105.06041)  
*Silin Gao, Ryuichi Takanobu, Wei Peng, Qun Liu, Minlie Huang*  
![image][Paper] ![image][ACL 2021] ![image][Dialog] ![image][NLP]
1. [Image-to-image translation with conditional generative adversarial networks](https://arxiv.org/abs/1611.07004)  
*Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros*  
![image][Paper]
1. [ImageNet classification using deep convolutional neural networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)  
*Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton*  
![image][Paper] ![image][NeurIPS 2012] ![image][Computer Vision] ![image][Image Classification]
1. [Improving entity linking by modeling latent relations between mentions](https://arxiv.org/abs/1804.10637)  
*Phong Le, Ivan Titov*  
![image][Paper] ![image][ACL 2018] ![image][Entity Linking] ![image][NLP]
1. [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)  
*Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre*  
![image][Paper] ![image][Hallucination] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Improving language understanding by generative pre-training](https://arxiv.org/abs/2112.04426)  
*Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever*  
![image][Paper] ![image][ICML 2022] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Inference suboptimality in variational autoencoders](https://arxiv.org/abs/1801.03558)  
*Chris Cremer, Xuechen Li, David Duvenaud*  
![image][Paper] ![image][ICLR 2018] ![image][Approximate Inference] ![image][Deep Learning] ![image][Variational Inference]
1. [InfoGAN: interpretable representation learning by information maximizing generative adversarial nets](https://arxiv.org/abs/1606.03657)  
*Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel*  
![image][Paper] ![image][NeurIPS 2016] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models] ![image][Information Theory]
1. [Interpretable convolutional neural networks via feedforward design](https://arxiv.org/abs/1810.02786)  
*C.-C. Jay Kuo, Min Zhang, Siyang Li, Jiali Duan, Yueru Chen*  
![image][Paper] ![image][Journal of Visual Communication and Image Representation 2019] ![image][Computer Vision]
1. [Know what you don't know: unanswerable questions for SQuAD](https://arxiv.org/abs/1806.03822)  
*Pranav Rajpurkar, Robin Jia, Percy Liang*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Question Answering]
1. [Knowledge-grounded dialogue generation with pre-trained language models](https://arxiv.org/abs/2010.08824)  
*Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan*  
![image][Paper] ![image][EMNLP 2020] ![image][Dialog] ![image][NLP]
1. [Language modelling with pixels](https://arxiv.org/abs/2207.06991)  
*Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, Desmond Elliott*  
![image][Paper] ![image][Computer Vision] ![image][NLP]
1. [Language models are unsupervised multitask learners](https://openai.com/blog/better-language-models/)  
*Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever*  
![image][Paper] ![image][NLP] ![image][Transformers]
1. [Learning activation functions to improve deep neural networks](https://arxiv.org/abs/1412.6830)  
*Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi*  
![image][Paper] ![image][Deep Learning]
1. [Learning discourse-level diversity for neural dialog models using conditional variational autoencoders](https://arxiv.org/abs/1703.10960)  
*Tiancheng Zhao, Ran Zhao, Maxine Eskenazi*  
![image][Paper] ![image][ACL 2017] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Learning on a general network](https://papers.nips.cc/paper/9-learning-on-a-general-network)  
*Amir F. Atiya*  
![image][Paper] ![image][NeurIPS 1987] ![image][Backpropagation] ![image][Deep Learning]
1. [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)  
*David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams*  
![image][Paper] ![image][Nature 1986] ![image][Backpropagation] ![image][Deep Learning]
1. [Learning transferable visual models from natural language supervision](https://arxiv.org/abs/2103.00020)  
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever*  
![image][Paper] ![image][ICML 2021] ![image][Computer Vision]
1. [Learning word embeddings efficiently with noise-contrastive estimation](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation)  
*Andriy Mnih, Koray Kavukcuoglu*  
![image][Paper] ![image][NeurIPS 2013] ![image][Embeddings] ![image][NLP]
1. [LLM.int8(): 8-bit matrix multiplication for transformers at scale](https://arxiv.org/abs/2208.07339)  
*Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer*  
![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [Mask & focus: conversation modelling by learning concepts](https://arxiv.org/abs/2003.04976)  
*Gaurav Pandey, Dinesh Raghu, Sachindra Joshi*  
![image][Paper] ![image][AAAI 2020] ![image][Dialog] ![image][NLP]
1. [Maximizing communication efficiency for large-scale training via 0/1 Adam](https://arxiv.org/abs/2202.06009)  
*Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He*  
![image][Paper] ![image][Deep Learning] ![image][Distributed Training]
1. [Megatron-LM: training multi-billion parameter language models using model parallelism](https://arxiv.org/abs/1909.08053)  
*Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro*  
![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Memory-efficient pipeline-parallel DNN training](https://arxiv.org/abs/2006.09503)  
*Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, Matei Zaharia*  
![image][Paper] ![image][ICML 2021] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [MinTL: minimalist transfer learning for task-oriented dialogue systems](https://arxiv.org/abs/2009.12005)  
*Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung*  
![image][Paper] ![image][EMNLP 2020] ![image][Dialog] ![image][NLP]
1. [Mixed precision training](https://arxiv.org/abs/1710.03740)  
*Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu*  
![image][Paper] ![image][ICLR 2018] ![image][Deep Learning] ![image][Large Models]
1. [mixup: beyond empirical risk minimization](https://arxiv.org/abs/1710.09412v1)  
*Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Examples] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Empirical Risk Minimization] ![image][Generative Models]
1. [MMCoQA: conversational question answering over text, tables and images](https://aclanthology.org/2022.acl-long.290/)  
*Yongqi Li, Wenjie Li, Liqiang Nie*  
![image][Paper] ![image][ACL 2022] ![image][Dataset] ![image][Dialog] ![image][NLP] ![image][Question Answering]
1. [Mode matching in GANs through latent space learning and inversion](https://arxiv.org/abs/1811.03692)  
*Deepak Mishra, Prathosh A. P., Aravind Jayendran, Varun Srivastava, Santanu Chaudhury*  
![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Multi-level memory for task oriented dialogs](https://arxiv.org/abs/1810.10647)  
*Revanth Reddy, Danish Contractor, Dinesh Raghu, Sachindra Joshi*  
![image][Paper] ![image][NAACL 2019] ![image][Dialog] ![image][NLP]
1. [MultiWOZ - A large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling](https://arxiv.org/abs/1810.00278)  
*Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, Milica Gašić*  
![image][Paper] ![image][EMNLP 2018] ![image][Dataset] ![image][Dialog] ![image][NLP]
1. [Mutual information neural estimation](https://arxiv.org/abs/1801.04062)  
*Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, R Devon Hjelm*  
![image][Paper] ![image][ICML 2018] ![image][Deep Learning] ![image][Information Theory]
1. [Neural GPUs learn algorithms](https://arxiv.org/abs/1511.08228)  
*Łukasz Kaiser, Ilya Sutskever*  
![image][Paper] ![image][Deep Learning] ![image][Theory of Computation]
1. [Neural networks and physical systems with emergent collective computational abilities](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/)  
*J. J. Hopfield*  
![image][Paper] ![image][PNAS 1982] ![image][Biology] ![image][Deep Learning] ![image][Energy-based Models]
1. [Neural ordinary differential equations](https://arxiv.org/abs/1806.07366)  
*Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud*  
![image][Paper] ![image][NeurIPS 2018] ![image][Deep Learning] ![image][Differential Equations]
1. [Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples](https://arxiv.org/abs/1802.00420)  
*Anish Athalye, Nicholas Carlini, David Wagner*  
![image][Paper] ![image][ICML 2018] ![image][Adversarial Examples] ![image][Deep Learning]
1. [On the convergence of Adam and beyond](https://arxiv.org/abs/1904.09237)  
*Sashank J. Reddi, Satyen Kale, Sanjiv Kumar*  
![image][Paper] ![image][ICLR 2018] ![image][Convergence] ![image][Deep Learning] ![image][Optimization]
1. [On the power of neural networks for solving hard problems](https://papers.nips.cc/paper/70-on-the-power-of-neural-networks-for-solving-hard-problems)  
*Jehoshua Bruck, Joseph W. Goodman*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning] ![image][NP-Hard]
1. [One model to learn them all](https://arxiv.org/abs/1706.05137)  
*Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, Jakob Uszkoreit*  
![image][Paper] ![image][Deep Learning] ![image][Multi-modal]
1. [Open domain question answering over tables via dense retrieval](https://arxiv.org/abs/2103.12011)  
*Jonathan Herzig, Thomas Müller, Syrine Krichene, Julian Eisenschlos*  
![image][Paper] ![image][NAACL 2021] ![image][NLP] ![image][Question Answering]
1. [Open question answering over tables and text](https://openreview.net/forum?id=MmCRswl1UYl)  
*Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Yang Wang, William W. Cohen*  
![image][Paper] ![image][ICLR 2021] ![image][NLP] ![image][Question Answering]
1. [OPT: open pre-trained transformer language models](https://arxiv.org/abs/2205.01068)  
*Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer*  
![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Optimal perceptual inference](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.149.6007&rank=1&q=optimal%20perceptual%20inference&osm=&ossid=)  
*Geoffrey E. Hinton, Terrence J. Sejnowski*  
![image][Paper] ![image][CVPR 1983] ![image][Deep Learning] ![image][Energy-based Models]
1. [Outer product-based neural collaborative filtering](https://arxiv.org/abs/1808.03912)  
*Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang, Tat-Seng Chua*  
![image][Paper] ![image][IJCAI 2018] ![image][Collaborative Filtering] ![image][Deep Learning] ![image][Recommender Systems]
1. [PaLM: scaling language modeling with pathways](https://arxiv.org/abs/2204.02311)  
*Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel*  
![image][Paper] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Perceptual losses for real-time style transfer and super-resolution](https://arxiv.org/abs/1603.08155)  
*Justin Johnson, Alexandre Alahi, Li Fei-Fei*  
![image][Paper] ![image][ECCV 2016] ![image][Computer Vision] ![image][Image Super Resolution]
1. [Personalizing dialogue agents: I have a dog, do you have pets too?](https://arxiv.org/abs/1801.07243)  
*Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston*  
![image][Paper] ![image][ACL 2018] ![image][Dialog] ![image][NLP]
1. [Phase-functioned neural networks for character control](https://dl.acm.org/citation.cfm?id=3073663)  
*Daniel Holden, Taku Komura, Jun Saito*  
![image][Paper] ![image][ACM Transactions on Graphics] ![image][Computer Graphics] ![image][Deep Learning]
1. [Playing Atari with deep reinforcement learning](https://arxiv.org/abs/1312.5602)  
*Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller*  
![image][Paper] ![image][Reinforcement Learning]
1. [Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing](https://arxiv.org/abs/2107.13586)  
*Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig*  
![image][Paper] ![image][Few Shot] ![image][NLP] ![image][Prompting]
1. [Prefix-tuning: optimizing continuous prompts for generation](https://arxiv.org/abs/2101.00190)  
*Xiang Lisa Li, Percy Liang*  
![image][Paper] ![image][ACL 2021] ![image][Few Shot] ![image][NLP] ![image][Prompting]
1. [Probabilistic latent semantic analysis](https://arxiv.org/abs/1301.6705)  
*Thomas Hofmann*  
![image][Paper] ![image][Information Retrieval] ![image][NLP]
1. [Progressive growing of GANs from improved quality, stability and variation](https://arxiv.org/abs/1710.10196)  
*Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [PullNet: open domain question answering with iterative retrieval on knowledge bases and text](https://arxiv.org/abs/1904.09537)  
*Haitian Sun, Tania Bedrax-Weiss, William Cohen*  
![image][Paper] ![image][EMNLP 2019] ![image][NLP] ![image][Question Answering]
1. [Q-BERT: Hessian based ultra low precision quantization of BERT](https://arxiv.org/abs/1909.05840)  
*Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer*  
![image][Paper] ![image][AAAI 2020] ![image][NLP] ![image][Quantization]
1. [R<sup>3</sup>Net: recurrent residual refinement network for saliency detection](https://www.ijcai.org/proceedings/2018/95)  
*Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guoqiang Han, Pheng-Ann Heng*  
![image][Paper] ![image][IJCAI 2018] ![image][Computer Vision] ![image][Saliency Detection]
1. [Reading Wikipedia to answer open-domain questions](https://arxiv.org/abs/1704.00051)  
*Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes*  
![image][Paper] ![image][ACL 2017] ![image][NLP] ![image][Question Answering]
1. [REALM: Retrieval-augmented language model pretraining](https://arxiv.org/abs/2002.08909)  
*Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang*  
![image][Paper] ![image][ICML 2020] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Recurrent models of visual attention](https://arxiv.org/abs/1406.6247)  
*Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu*  
![image][Paper]
1. [Regularizing and optimizing LSTM language models](https://arxiv.org/abs/1708.02182)  
*Stephen Merity, Nitish Shirish Keskar, Richard Socher*  
![image][Paper] ![image][ICLR 2018] ![image][Deep Learning] ![image][Optimization] ![image][Regularization]
1. [Restricted Boltzmann machines for collaborative filtering](https://dl.acm.org/citation.cfm?doid=1273496.1273596)  
*Ruslan Salakhutdinov, Andriy Mnih, Geoffrey Hinton*  
![image][Paper] ![image][ICML 2007] ![image][Boltzmann Machines] ![image][Collaborative Filtering] ![image][Deep Learning] ![image][Energy-based Models] ![image][Recommender Systems]
1. [Retrieval augmentation reduces hallucination in conversation](https://arxiv.org/abs/2104.07567)  
*Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston*  
![image][Paper] ![image][EMNLP 2021] ![image][Dialog] ![image][Hallucination] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Retrieval-augmented generation for knowledge-intensive NLP tasks](https://arxiv.org/abs/2005.11401)  
*Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela*  
![image][Paper] ![image][NeurIPS 2020] ![image][Hallucination] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Revisiting classifier two-sample tests](https://arxiv.org/abs/1610.06545)  
*David Lopez-Paz, Maxime Oquab*  
![image][Paper] ![image][ICLR 2017] ![image][Deep Learning] ![image][Unsupervised Learning]
1. [Self-normalizing neural networks](https://arxiv.org/abs/1706.02515)  
*Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter*  
![image][Paper] ![image][NeurIPS 2017] ![image][Activation Function] ![image][Deep Learning] ![image][Normalization]
1. [Semantically equivalent adversarial rules for debugging NLP models](https://aclanthology.org/P18-1079/)  
*Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin*  
![image][Paper] ![image][ACL 2018] ![image][NLP]
1. [Sequential latent knowledge selection for knowledge-grounded dialogue](https://arxiv.org/abs/2002.07510)  
*Byeongchang Kim, Jaewoo Ahn, Gunhee Kim*  
![image][Paper] ![image][ICLR 2020] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Simple and effective multi-paragraph reading comprehension](https://arxiv.org/abs/1710.10723)  
*Christopher Clark, Matt Gardner*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Reading Comprehension]
1. [Soft filter pruning for accelerating deep convolutional neural networks](https://arxiv.org/abs/1808.06866)  
*Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang*  
![image][Paper] ![image][IJCAI 2018] ![image][Deep Learning] ![image][Pruning]
1. [SOLOIST: building task bots at scale with transfer learning and machine teaching](https://arxiv.org/abs/2005.05298)  
*Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Jianfeng Gao*  
![image][Paper] ![image][Dialog] ![image][Few Shot] ![image][NLP] ![image][Transfer Learning]
1. [Solving quantitative reasoning problems with language models](https://arxiv.org/abs/2206.14858)  
*Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra*  
![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting](https://arxiv.org/abs/1709.04875)  
*Bing Yu, Haoteng Yin, Zhanxing Zhu*  
![image][Paper] ![image][IJCAI 2018] ![image][Deep Learning] ![image][Graph Neural Networks] ![image][Spatio-Temporal Reasoning] ![image][Time Series]
1. [Spectral normalization for generative adversarial networks](https://arxiv.org/abs/1802.05957)  
*Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Learning] ![image][Deep Learning]
1. [Sticking the landing: simple, lower-variance gradient estimators for variational inference](https://arxiv.org/abs/1703.09194)  
*Geoffrey Roeder, Yuhuai Wu, David K. Duvenaud*  
![image][Paper] ![image][NeurIPS 2017] ![image][Deep Learning] ![image][Gradient Estimation] ![image][Variational Inference]
1. [Stochastic hyperparameter optimization through hypernetworks](https://arxiv.org/abs/1802.09419)  
*Jonathan Lorraine, David Duvenaud*  
![image][Paper] ![image][NeurIPS 2018] ![image][Deep Learning] ![image][Meta Learning]
1. [Strategies for teaching layered networks classification tasks](https://papers.nips.cc/paper/85-strategies-for-teaching-layered-networks-classification-tasks)  
*Ben S. Wittner, John S. Denker*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning]
1. [Style transfer from non-parallel text by cross-alignment](https://arxiv.org/abs/1705.09655)  
*Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola*  
![image][Paper] ![image][NeurIPS 2017] ![image][NLP] ![image][Style Transfer]
1. [Subword regularization: improving neural network translation models with multiple subword candidates](https://arxiv.org/abs/1804.10959)  
*Taku Kudo*  
![image][Paper] ![image][ACL 2018] ![image][Machine Translation] ![image][NLP] ![image][Regularization]
1. [Supervised learning of probability distributions by neural networks](https://papers.nips.cc/paper/3-supervised-learning-of-probability-distributions-by-neural-networks)  
*Eric B. Baum, Frank Wilczek*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning]
1. [Synchronization in neural nets](https://papers.nips.cc/paper/32-synchronization-in-neural-nets)  
*Jacques J. Vidal, John Haggerty*  
![image][Paper] ![image][NeurIPS 1987] ![image][Deep Learning]
1. [Tackling the poor assumptions of Naive Bayes text classifiers](https://dl.acm.org/citation.cfm?id=3041916)  
*Jason D. M. Rennie, Lawrence Shih, Jaime Teevan, David R. Karger*  
![image][Paper] ![image][ICML 2003] ![image][NLP]
1. [The best of both worlds: combining recent advances in neural machine translation](https://arxiv.org/abs/1804.09849)  
*Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, Macduff Hughes*  
![image][Paper] ![image][ACL 2018] ![image][Machine Translation] ![image][NLP]
1. [The information bottleneck method](https://arxiv.org/abs/physics/0004057)  
*Naftali Tishby, Fernando C. Pereira, William Bialek*  
![image][Paper] ![image][Information Theory]
1. [The Pile: an 800GB dataset of diverse text for language modeling](https://arxiv.org/abs/2101.00027)  
*Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy*  
![image][Paper] ![image][Dataset] ![image][NLP]
1. [The power of scale for parameter-efficient prompt tuning](https://arxiv.org/abs/2104.08691)  
*Brian Lester, Rami Al-Rfou, Noah Constant*  
![image][Paper] ![image][EMNLP 2021] ![image][NLP] ![image][Prompting]
1. [Thermometer encoding: one hot way to resist adversarial examples](https://openreview.net/forum?id=S18Su--CW)  
*Jacob Buckman, Aurko Roy, Colin Raffel, Ian Goodfellow*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [To regularize or not to regularize? The bias variance trade-off in regularized AEs](https://arxiv.org/abs/2006.05838)  
*Arnab Kumar Mondal, Himanshu Asnani, Parag Singla, Prathosh AP*  
![image][Paper] ![image][Deep Learning] ![image][Regularization]
1. [Towards deep learning models resilient to adversarial attacks](https://arxiv.org/abs/1706.06083)  
*Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu*  
![image][Paper] ![image][ICLR 2018] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Towards evaluating the robustness of neural networks](https://arxiv.org/abs/1608.04644)  
*Nicholas Carlini, David Wagner*  
![image][Paper] ![image][IEEE Symposium on Security and Privacy 2017] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/abs/2108.12409)  
*Ofir Press, Noah Smith, Mike Lewis*  
![image][Paper] ![image][ICLR 2022] ![image][Embeddings] ![image][NLP] ![image][Transformers]
1. [Transformer memory as a differentiable search index](https://arxiv.org/abs/2202.06991)  
*Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler*  
![image][Paper] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Understanding convolutional neural networks with a mathematical model](https://arxiv.org/abs/1609.04112)  
*C.-C. Jay Kuo*  
![image][Paper] ![image][Journal of Visual Communication and Image Representation 2016] ![image][Computer Vision]
1. [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)  
*Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, Alexander Lerchner*  
![image][Paper] ![image][Deep Learning] ![image][Disentanglement] ![image][Variational Inference]
1. [Unit tests for stochastic optimization](https://arxiv.org/abs/1312.6055)  
*Tom Schaul, Ioannis Antonoglou, David Silver*  
![image][Paper] ![image][ICLR 2014] ![image][Deep Learning] ![image][Optimization]
1. [Universal language model fine-tuning for text classification](https://arxiv.org/abs/1801.06146)  
*Jeremy Howard, Sebastian Ruder*  
![image][Paper] ![image][ACL 2018] ![image][NLP] ![image][Text Classification]
1. [Unpaired image-to-image translation using cycle-consistent adversarial networks](https://arxiv.org/abs/1703.10593)  
*Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros*  
![image][Paper] ![image][ICCV 2017] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Unsupervised machine translation using monolingual corpora only](https://arxiv.org/abs/1711.00043)  
*Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato*  
![image][Paper] ![image][ACL 2019] ![image][Machine Translation] ![image][NLP] ![image][Unsupervised Learning]
1. [Unsupervised representation learning by predicting image rotations](https://arxiv.org/abs/1803.07728)  
*Spyros Gidaris, Praveer Singh, Nikos Komodakis*  
![image][Paper] ![image][ICLR 2018] ![image][Computer Vision] ![image][Unsupervised Learning]
1. [Variational inference using implicit distributions](https://arxiv.org/abs/1702.08235)  
*Ferenc Huszár*  
![image][Paper] ![image][Deep Learning] ![image][Variational Inference]
1. [Variational inference with latent space quantization for adversarial resilience](https://arxiv.org/abs/1903.09940)  
*Vinay Kyatham, Mayank Mishra, Tarun Kumar Yadav, Deepak Mishra, Prathosh AP*  
![image][Paper] ![image][Deep Learning] ![image][Quantization] ![image][Robustness] ![image][Variational Inference]
1. [Variational learning for unsupervised knowledge grounded dialogs](https://arxiv.org/abs/2112.00653)  
*Mayank Mishra, Dhiraj Madan, Gaurav Pandey, Danish Contractor*  
![image][Paper] ![image][IJCAI 2022] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Variational lossy autoencoder](https://arxiv.org/abs/1611.02731)  
*Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel*  
![image][Paper] ![image][ICLR 2017] ![image][Deep Learning] ![image][Variational Inference]
1. [VEEGAN: reducing mode collapse in GANs using implicit variational learning](https://arxiv.org/abs/1705.07761)  
*Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, Charles Sutton*  
![image][Paper] ![image][NeurIPS 2017] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556)  
*Karen Simonyan, Andrew Zisserman*  
![image][Paper] ![image][ICLR 2015] ![image][Computer Vision] ![image][Image Classification]
1. [Visualizing data using t-SNE](http://www.jmlr.org/papers/v9/vandermaaten08a.html)  
*Laurens van der Maaten, Geoffrey Hinton*  
![image][Paper] ![image][JMLR 2008] ![image][Data Visualization] ![image][Deep Learning]
1. [Wasserstein GAN](https://arxiv.org/abs/1701.07875)  
*Martin Arjovsky, Soumith Chintala, Léon Bottou*  
![image][Paper] ![image][ICML 2017] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Generative Models]
1. [Wavenet: a generative model for raw audio](https://arxiv.org/abs/1609.03499)  
*Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu*  
![image][Paper] ![image][Audio] ![image][Deep Learning] ![image][Generative Models]
1. [What language model to train if you have one million GPU hours?](https://openreview.net/forum?id=rI7BL3fHIZq)  
*Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, Iz Beltagy*  
![image][Paper] ![image][ACL 2022] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Word translation without parallel data](https://openreview.net/forum?id=H196sainb)  
*Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, Hervé Jégou*  
![image][Paper] ![image][ICLR 2018] ![image][Machine Translation] ![image][NLP]
1. [You only look once: unified, real-time object detection](https://arxiv.org/abs/1506.02640)  
*Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi*  
![image][Paper] ![image][CVPR 2016] ![image][Computer Vision] ![image][Object Detection]
1. [ZeRO-Infinity: breaking the GPU memory wall for extreme scale deep learning](https://arxiv.org/abs/2104.07857)  
*Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He*  
![image][Paper] ![image][SC 2021] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Zero-shot text-to-image generation](http://proceedings.mlr.press/v139/ramesh21a.html)  
*Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever*  
![image][Paper] ![image][ICML 2021] ![image][Deep Learning] ![image][Generative Models] ![image][Variational Inference] ![image][Zero Shot]
1. [ZeRO: memory optimizations toward training trillion parameter models](https://arxiv.org/abs/1910.02054)  
*Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He*  
![image][Paper] ![image][SC 2020] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [ZeroQuant: efficient and affordable post-training quantization for large-scale transformers](https://arxiv.org/abs/2206.01861)  
*Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He*  
![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [β-VAE: learning basic visual concepts with a constrained variational framework](https://openreview.net/forum?id=Sy2fzU9gl)  
*Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner*  
![image][Paper] ![image][ICLR 2017] ![image][Deep Learning] ![image][Variational Inference]

# Calculus
1. Calculus of variations  
*I. M. Gelfand, S. V. Fomin*  
![image][Book]
1. Thomas' calculus  
*George B. Thomas Jr., Maurice D. Weir*  
![image][Book]

# Computer architecture
1. Computer architecture: a quantitative approach  
*John L. Hennessy, David A. Patterson*  
![image][Book]
1. Computer organization and design ARM edition: the hardware software interface  
*David A. Patterson, John L. Hennessy*  
![image][Book]
1. [Flipping bits in memory without accessing them: an experimental study of DRAM disturbance errors](https://ieeexplore.ieee.org/document/6853210)  
*Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, Donghyuk Lee, Chris Wilkerson, Konrad Lai, Onur Mutlu*  
![image][Paper] ![image][IEEE ISCA 2014] ![image][Computer Architecture] ![image][Memory] ![image][Security]
1. [Improving DRAM performance by parallelizing refreshes with accesses](https://ieeexplore.ieee.org/document/6835946)  
*Kevin Kai-Wei Chang, Donghyuk Lee, Zeshan Chishti, Alaa R. Alameldeen, Chris Wilkerson, Yoongu Kim, Onur Mutlu*  
![image][Paper] ![image][IEEE HPCA 2014] ![image][Computer Architecture] ![image][Memory] ![image][Security]
1. [Memory performance attacks: denial of memory service in multi-core systems](https://www.usenix.org/conference/16th-usenix-security-symposium/memory-performance-attacks-denial-memory-service-multi)  
*Thomas Moscibroda, Onur Mutlu*  
![image][Paper] ![image][USENIX Security Symposium 2007] ![image][Computer Architecture] ![image][Memory] ![image][Security]
1. [Memory scaling: a systems architecture perspective](https://ieeexplore.ieee.org/document/6582088)  
*Onur Mutlu*  
![image][Paper] ![image][IEEE International Memory Worksop 2013] ![image][Computer Architecture] ![image][Memory]
1. [Millicode in an IBM zSeries processor](https://ieeexplore.ieee.org/document/5388884)  
*L. C. Heller, M. S. Farrell*  
![image][Paper] ![image][IBM Journal of Research and Development 2004] ![image][Computer Architecture]
1. [RAIDR: Retention-Aware Intelligent DRAM Refresh](https://dl.acm.org/doi/10.5555/2337159.2337161)  
*Jamie Liu, Ben Jaiyen, Richard Veras, Onur Mutlu*  
![image][Paper] ![image][IEEE ISCA 2012] ![image][Computer Architecture] ![image][Memory]
1. [Stall-time fair memory access scheduling for chip multiprocessors](https://ieeexplore.ieee.org/document/4408252)  
*Onur Mutlu, Thomas Moscibroda*  
![image][Paper] ![image][IEEE MICRO 2007] ![image][Computer Architecture] ![image][Memory]

# Computer graphics
1. [Principles of traditional animation applied to 3D computer animation](https://dl.acm.org/doi/10.1145/37402.37407)  
*John Lasseter*  
![image][Paper] ![image][ACM SIGGRAPH Computer Graphics 1987] ![image][Animation] ![image][Computer Graphics]

# Data structures and algorithms
1. Data structures and algorithms in Java  
*Michael T. Goodrich, Roberto Tamassia, Michael H. Goldwasser*  
![image][Book]
1. Introduction to algorithms  
*Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein*  
![image][Book]

# Digital electronics
1. Digital design: with an introduction to the Verilog HDL  
*M. Morris Mano, Michael D. Ciletti*  
![image][Book]

# Graph theory
1. Introduction to graph theory  
*Robin Wilson*  
![image][Book]

# Information theory
1. Elements of information theory  
*Thomas M. Cover, Joy A. Thomas*  
![image][Book]
1. [Error detecting and error correcting codes](https://ieeexplore.ieee.org/document/6772729)  
*R. W. Hamming*  
![image][Paper] ![image][The Bell System Technical Journal 1950] ![image][Error Correction] ![image][Error Detection] ![image][Information Theory]

# Linear algebra
1. Linear algebra and its applications  
*Gilbert Strang*  
![image][Book]
1. Matrix analysis and applied linear algebra  
*Carl D. Meyer*  
![image][Book]
1. The matrix cookbook  
*Kaare Brandt Petersen, Michael Syskind Pedersen*  
![image][Book]

# Measure theory
1. Measure theory  
*Donald L. Cohn*  
![image][Book]

# Optimization theory
1. Convex Optimization  
*Stephen Boyd, Lieven Vandenberghe*  
![image][Book]
1. [Distributed optimization and statistical learning via the alternating direction method of multipliers](https://ieeexplore.ieee.org/document/8186925)  
*Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein*  
![image][Book]

# Probability and stochastic processes
1. Introduction to probability and stochastic processes with applications  
*Liliana Blanco Castaneda, Viswanathan Arunachalam, Delvamuthu Dharmaraja*  
![image][Book]

# Quantum computing
1. Quantum computation and quantum information  
*Michael A. Nielsen, Isaac L. Chuang*  
![image][Book]
1. Quantum computing: a gentle introduction  
*Eleanor Rieffel, Wolfgang Polak*  
![image][Book]
1. [A fast quantum mechanical algorithm for database search](https://arxiv.org/abs/quant-ph/9605043)  
*Lov K. Grover*  
![image][Paper] ![image][STOC 1996] ![image][Quantum Algorithms] ![image][Quantum Computing]
1. [A single quantum cannot be cloned](https://www.nature.com/articles/299802a0)  
*W. K. Wootters, W. H. Zurek*  
![image][Paper] ![image][Nature 1982] ![image][Quantum Computing]
1. [Can quantum-mechanical description of physical reality be considered complete](https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777)  
*Albert Einstein, Boris Podolsky, Nathan Rosen*  
![image][Paper] ![image][Physical Review Journals 1935] ![image][Quantum Computing]
1. [Image recognition with an adiabatic quantum computer I. mapping to quadratic unconstrained binary optimization](https://arxiv.org/abs/0804.4457)  
*Hartmut Neven, Geordie Rose, William G. Macready*  
![image][Paper] ![image][Image Classification] ![image][QUBO] ![image][Quantum Computing]
1. [Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer](https://arxiv.org/abs/quant-ph/9508027)  
*Peter W. Shor*  
![image][Paper] ![image][SIAM Journal on Computing 1997] ![image][Quantum Algorithms] ![image][Quantum Computing]
1. [Probabilistic cloning and identification of linearly independent quantum states](https://arxiv.org/abs/quant-ph/9804064)  
*Lu-Ming Duan, Guang-Can Guo*  
![image][Paper] ![image][Physical Review Letters 1998] ![image][Cloning] ![image][Quantum Computing]
1. [Quantum theory, the Church-Turing principle and the universal quantum computer](https://royalsocietypublishing.org/doi/10.1098/rspa.1985.0070)  
*David Deutsch*  
![image][Paper] ![image][Proceedings of the Royal Society 1985] ![image][Quantum Computing] ![image][Theory of Computation]
1. [Rapid solution of problems by quantum computation](https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1992.0167)  
*David Deutsche, Richard Jozsa*  
![image][Paper] ![image][Proceedings of the Royal Society 1992] ![image][Quantum Algorithms] ![image][Quantum Computing]
1. [Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.70.1895)  
*Charles H. Bennett, Gilles Brassard, Claude Crépeau, Richard Jozsa, Asher Peres, William K. Wootters*  
![image][Paper] ![image][Physical Review Journals 1993] ![image][Quantum Computing] ![image][Quantum Teleportation]
1. [Integer optimization toolbox (minimizing polynomials over integer lattices using quantum annealing)](https://1qbit.com/whitepaper/integer-optimization-toolbox/)  
*Pooya Ronagh*  
![image][Whitepaper]
1. [Limits on parallel speedup for classical Ising model solvers](https://www.dwavesys.com/resources/white-paper/limits-on-parallel-speedup-for-classical-ising-model-solvers/)  
![image][Whitepaper]
1. [Partitioning optimization problems for hybrid classical/quantum execution](https://docs.ocean.dwavesys.com/projects/qbsolv/en/latest/_downloads/bd15a2d8f32e587e9e5997ce9d5512cc/qbsolv_techReport.pdf)  
*Michael Booth, Steven P. Reinhardt, Aidan Roy*  
![image][Whitepaper]
1. [Programming with D-Wave: map coloring problem](https://www.dwavesys.com/resources/white-paper/programming-with-d-wave-map-coloring-problem/)  
*E. D. Dahl*  
![image][Whitepaper]
1. [Quantum performance evaluation: a short reading list](https://www.dwavesys.com/resources/white-paper/quantum-performance-evaluation-a-short-reading-list/)  
![image][Whitepaper]

# Signal processing
1. Discrete-time signal processing  
*Alan V. Oppenheim, Ronald W. Schafer*  
![image][Book]
1. Foundations of Signal Processing  
*Martin Vetterli, Jelena Kovačević, Vivek K Goyal*  
![image][Book]
1. Signals and systems  
*Alan V. Oppenheim*  
![image][Book]
1. Understanding digital signal processing  
*Richard G. Lyons*  
![image][Book]

