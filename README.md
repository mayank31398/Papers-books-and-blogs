This repository contains a list of the books, blogs, research papers and white papers that I have read and found interesting.

[book]: https://img.shields.io/static/v1?label=&message=Book&color=red
[blog]: https://img.shields.io/static/v1?label=&message=Blog&color=brightgreen
[paper]: https://img.shields.io/static/v1?label=&message=Paper&color=blueviolet
[whitepaper]: https://img.shields.io/static/v1?label=&message=Whitepaper&color=yellow
[ai for code]: https://img.shields.io/static/v1?label=&message=AI%20for%20code&color=blue
[activation function]: https://img.shields.io/static/v1?label=&message=Activation%20Function&color=blue
[adversarial examples]: https://img.shields.io/static/v1?label=&message=Adversarial%20Examples&color=blue
[adversarial learning]: https://img.shields.io/static/v1?label=&message=Adversarial%20Learning&color=blue
[algorithms]: https://img.shields.io/static/v1?label=&message=Algorithms&color=blue
[animation]: https://img.shields.io/static/v1?label=&message=Animation&color=blue
[approximate inference]: https://img.shields.io/static/v1?label=&message=Approximate%20Inference&color=blue
[audio]: https://img.shields.io/static/v1?label=&message=Audio&color=blue
[backpropagation]: https://img.shields.io/static/v1?label=&message=Backpropagation&color=blue
[behavior and control]: https://img.shields.io/static/v1?label=&message=Behavior%20and%20Control&color=blue
[biology]: https://img.shields.io/static/v1?label=&message=Biology&color=blue
[boltzmann machines]: https://img.shields.io/static/v1?label=&message=Boltzmann%20Machines&color=blue
[cloning]: https://img.shields.io/static/v1?label=&message=Cloning&color=blue
[collaborative filtering]: https://img.shields.io/static/v1?label=&message=Collaborative%20Filtering&color=blue
[complex numbers]: https://img.shields.io/static/v1?label=&message=Complex%20Numbers&color=blue
[compression]: https://img.shields.io/static/v1?label=&message=Compression&color=blue
[computer architecture]: https://img.shields.io/static/v1?label=&message=Computer%20Architecture&color=blue
[computer graphics]: https://img.shields.io/static/v1?label=&message=Computer%20Graphics&color=blue
[computer vision]: https://img.shields.io/static/v1?label=&message=Computer%20Vision&color=blue
[constituency parsing]: https://img.shields.io/static/v1?label=&message=Constituency%20Parsing&color=blue
[convergence]: https://img.shields.io/static/v1?label=&message=Convergence&color=blue
[curriculum learning]: https://img.shields.io/static/v1?label=&message=Curriculum%20Learning&color=blue
[data visualization]: https://img.shields.io/static/v1?label=&message=Data%20Visualization&color=blue
[dataset]: https://img.shields.io/static/v1?label=&message=Dataset&color=blue
[decentralized training]: https://img.shields.io/static/v1?label=&message=Decentralized%20Training&color=blue
[deep]: https://img.shields.io/static/v1?label=&message=Deep&color=blue
[deep learning]: https://img.shields.io/static/v1?label=&message=Deep%20Learning&color=blue
[dialog]: https://img.shields.io/static/v1?label=&message=Dialog&color=blue
[differential equations]: https://img.shields.io/static/v1?label=&message=Differential%20Equations&color=blue
[discrete optimization]: https://img.shields.io/static/v1?label=&message=Discrete%20Optimization&color=blue
[disentanglement]: https://img.shields.io/static/v1?label=&message=Disentanglement&color=blue
[distillation]: https://img.shields.io/static/v1?label=&message=Distillation&color=blue
[distributed training]: https://img.shields.io/static/v1?label=&message=Distributed%20Training&color=blue
[efficient finetuning]: https://img.shields.io/static/v1?label=&message=Efficient%20Finetuning&color=blue
[embeddings]: https://img.shields.io/static/v1?label=&message=Embeddings&color=blue
[empirical risk minimization]: https://img.shields.io/static/v1?label=&message=Empirical%20Risk%20Minimization&color=blue
[energy-based models]: https://img.shields.io/static/v1?label=&message=Energy-based%20Models&color=blue
[enery based models]: https://img.shields.io/static/v1?label=&message=Enery%20based%20Models&color=blue
[entity linking]: https://img.shields.io/static/v1?label=&message=Entity%20Linking&color=blue
[error correction]: https://img.shields.io/static/v1?label=&message=Error%20Correction&color=blue
[error detection]: https://img.shields.io/static/v1?label=&message=Error%20Detection&color=blue
[ethical impacts of ai]: https://img.shields.io/static/v1?label=&message=Ethical%20Impacts%20of%20AI&color=blue
[fake content detection]: https://img.shields.io/static/v1?label=&message=Fake%20Content%20Detection&color=blue
[few shot]: https://img.shields.io/static/v1?label=&message=Few%20Shot&color=blue
[generative models]: https://img.shields.io/static/v1?label=&message=Generative%20Models&color=blue
[genetic algorithms]: https://img.shields.io/static/v1?label=&message=Genetic%20Algorithms&color=blue
[gradient estimation]: https://img.shields.io/static/v1?label=&message=Gradient%20Estimation&color=blue
[graph neural networks]: https://img.shields.io/static/v1?label=&message=Graph%20Neural%20Networks&color=blue
[green ai]: https://img.shields.io/static/v1?label=&message=Green%20AI&color=blue
[hallucination]: https://img.shields.io/static/v1?label=&message=Hallucination&color=blue
[human feedback]: https://img.shields.io/static/v1?label=&message=Human%20Feedback&color=blue
[hyperparameter search]: https://img.shields.io/static/v1?label=&message=Hyperparameter%20Search&color=blue
[image classification]: https://img.shields.io/static/v1?label=&message=Image%20Classification&color=blue
[image super resolution]: https://img.shields.io/static/v1?label=&message=Image%20Super%20Resolution&color=blue
[information retrieval]: https://img.shields.io/static/v1?label=&message=Information%20Retrieval&color=blue
[information theory]: https://img.shields.io/static/v1?label=&message=Information%20Theory&color=blue
[instruction finetuning]: https://img.shields.io/static/v1?label=&message=Instruction%20Finetuning&color=blue
[knowledge graphs]: https://img.shields.io/static/v1?label=&message=Knowledge%20Graphs&color=blue
[large models]: https://img.shields.io/static/v1?label=&message=Large%20Models&color=blue
[machine translation]: https://img.shields.io/static/v1?label=&message=Machine%20Translation&color=blue
[mathematical reasoning]: https://img.shields.io/static/v1?label=&message=Mathematical%20Reasoning&color=blue
[memory]: https://img.shields.io/static/v1?label=&message=Memory&color=blue
[meta learning]: https://img.shields.io/static/v1?label=&message=Meta%20Learning&color=blue
[mixture of experts]: https://img.shields.io/static/v1?label=&message=Mixture%20of%20Experts&color=blue
[model saving]: https://img.shields.io/static/v1?label=&message=Model%20Saving&color=blue
[molecular chemistry]: https://img.shields.io/static/v1?label=&message=Molecular%20Chemistry&color=blue
[multi-modal]: https://img.shields.io/static/v1?label=&message=Multi-modal&color=blue
[nlp]: https://img.shields.io/static/v1?label=&message=NLP&color=blue
[np-hard]: https://img.shields.io/static/v1?label=&message=NP-Hard&color=blue
[normalization]: https://img.shields.io/static/v1?label=&message=Normalization&color=blue
[object detection]: https://img.shields.io/static/v1?label=&message=Object%20Detection&color=blue
[optimization]: https://img.shields.io/static/v1?label=&message=Optimization&color=blue
[out-of-distribution detection]: https://img.shields.io/static/v1?label=&message=Out-of-Distribution%20Detection&color=blue
[prompting]: https://img.shields.io/static/v1?label=&message=Prompting&color=blue
[pruning]: https://img.shields.io/static/v1?label=&message=Pruning&color=blue
[qubo]: https://img.shields.io/static/v1?label=&message=QUBO&color=blue
[quantization]: https://img.shields.io/static/v1?label=&message=Quantization&color=blue
[quantum algorithms]: https://img.shields.io/static/v1?label=&message=Quantum%20Algorithms&color=blue
[quantum computing]: https://img.shields.io/static/v1?label=&message=Quantum%20Computing&color=blue
[quantum teleportation]: https://img.shields.io/static/v1?label=&message=Quantum%20Teleportation&color=blue
[question answering]: https://img.shields.io/static/v1?label=&message=Question%20Answering&color=blue
[reading comprehension]: https://img.shields.io/static/v1?label=&message=Reading%20Comprehension&color=blue
[recommender systems]: https://img.shields.io/static/v1?label=&message=Recommender%20Systems&color=blue
[regularization]: https://img.shields.io/static/v1?label=&message=Regularization&color=blue
[reinforcement learning]: https://img.shields.io/static/v1?label=&message=Reinforcement%20Learning&color=blue
[relational reasoning]: https://img.shields.io/static/v1?label=&message=Relational%20Reasoning&color=blue
[retrieval-augmented generation]: https://img.shields.io/static/v1?label=&message=Retrieval-Augmented%20Generation&color=blue
[robotics]: https://img.shields.io/static/v1?label=&message=Robotics&color=blue
[robustness]: https://img.shields.io/static/v1?label=&message=Robustness&color=blue
[saliency detection]: https://img.shields.io/static/v1?label=&message=Saliency%20Detection&color=blue
[scaling laws]: https://img.shields.io/static/v1?label=&message=Scaling%20Laws&color=blue
[security]: https://img.shields.io/static/v1?label=&message=Security&color=blue
[sparse matrices]: https://img.shields.io/static/v1?label=&message=Sparse%20Matrices&color=blue
[spatio-temporal]: https://img.shields.io/static/v1?label=&message=Spatio-Temporal&color=blue
[speech]: https://img.shields.io/static/v1?label=&message=Speech&color=blue
[speedup]: https://img.shields.io/static/v1?label=&message=Speedup&color=blue
[story generation]: https://img.shields.io/static/v1?label=&message=Story%20Generation&color=blue
[style transfer]: https://img.shields.io/static/v1?label=&message=Style%20Transfer&color=blue
[summarization]: https://img.shields.io/static/v1?label=&message=Summarization&color=blue
[systems]: https://img.shields.io/static/v1?label=&message=Systems&color=blue
[text classification]: https://img.shields.io/static/v1?label=&message=Text%20Classification&color=blue
[theory of computation]: https://img.shields.io/static/v1?label=&message=Theory%20of%20Computation&color=blue
[time series]: https://img.shields.io/static/v1?label=&message=Time%20Series&color=blue
[transfer learning]: https://img.shields.io/static/v1?label=&message=Transfer%20Learning&color=blue
[transformers]: https://img.shields.io/static/v1?label=&message=Transformers&color=blue
[unsupervised learning]: https://img.shields.io/static/v1?label=&message=Unsupervised%20Learning&color=blue
[variational inference]: https://img.shields.io/static/v1?label=&message=Variational%20Inference&color=blue
[zero shot]: https://img.shields.io/static/v1?label=&message=Zero%20Shot&color=blue
[aaai 2018]: https://img.shields.io/static/v1?label=&message=AAAI%202018&color=grey
[aaai 2020]: https://img.shields.io/static/v1?label=&message=AAAI%202020&color=grey
[acl 2017]: https://img.shields.io/static/v1?label=&message=ACL%202017&color=grey
[acl 2018]: https://img.shields.io/static/v1?label=&message=ACL%202018&color=grey
[acl 2019]: https://img.shields.io/static/v1?label=&message=ACL%202019&color=grey
[acl 2020]: https://img.shields.io/static/v1?label=&message=ACL%202020&color=grey
[acl 2021]: https://img.shields.io/static/v1?label=&message=ACL%202021&color=grey
[acl 2022]: https://img.shields.io/static/v1?label=&message=ACL%202022&color=grey
[acm siggraph computer graphics 1987]: https://img.shields.io/static/v1?label=&message=ACM%20SIGGRAPH%20Computer%20Graphics%201987&color=grey
[acm transactions on graphics]: https://img.shields.io/static/v1?label=&message=ACM%20Transactions%20on%20Graphics&color=grey
[aistats 2009]: https://img.shields.io/static/v1?label=&message=AISTATS%202009&color=grey
[amazon]: https://img.shields.io/static/v1?label=&message=Amazon&color=grey
[cvpr 1983]: https://img.shields.io/static/v1?label=&message=CVPR%201983&color=grey
[cvpr 2015]: https://img.shields.io/static/v1?label=&message=CVPR%202015&color=grey
[cvpr 2016]: https://img.shields.io/static/v1?label=&message=CVPR%202016&color=grey
[cvpr 2019]: https://img.shields.io/static/v1?label=&message=CVPR%202019&color=grey
[conll 2016]: https://img.shields.io/static/v1?label=&message=CoNLL%202016&color=grey
[eccv 2016]: https://img.shields.io/static/v1?label=&message=ECCV%202016&color=grey
[emnlp 2016]: https://img.shields.io/static/v1?label=&message=EMNLP%202016&color=grey
[emnlp 2018]: https://img.shields.io/static/v1?label=&message=EMNLP%202018&color=grey
[emnlp 2019]: https://img.shields.io/static/v1?label=&message=EMNLP%202019&color=grey
[emnlp 2020]: https://img.shields.io/static/v1?label=&message=EMNLP%202020&color=grey
[emnlp 2021]: https://img.shields.io/static/v1?label=&message=EMNLP%202021&color=grey
[emnlp 2022]: https://img.shields.io/static/v1?label=&message=EMNLP%202022&color=grey
[frontiers in computational neuroscience 2017]: https://img.shields.io/static/v1?label=&message=Frontiers%20in%20Computational%20Neuroscience%202017&color=grey
[huggingface]: https://img.shields.io/static/v1?label=&message=HuggingFace&color=grey
[ibm journal of research and development 2004]: https://img.shields.io/static/v1?label=&message=IBM%20Journal%20of%20Research%20and%20Development%202004&color=grey
[iccv 2017]: https://img.shields.io/static/v1?label=&message=ICCV%202017&color=grey
[iclr 2013]: https://img.shields.io/static/v1?label=&message=ICLR%202013&color=grey
[iclr 2014]: https://img.shields.io/static/v1?label=&message=ICLR%202014&color=grey
[iclr 2015]: https://img.shields.io/static/v1?label=&message=ICLR%202015&color=grey
[iclr 2016]: https://img.shields.io/static/v1?label=&message=ICLR%202016&color=grey
[iclr 2017]: https://img.shields.io/static/v1?label=&message=ICLR%202017&color=grey
[iclr 2018]: https://img.shields.io/static/v1?label=&message=ICLR%202018&color=grey
[iclr 2019]: https://img.shields.io/static/v1?label=&message=ICLR%202019&color=grey
[iclr 2020]: https://img.shields.io/static/v1?label=&message=ICLR%202020&color=grey
[iclr 2021]: https://img.shields.io/static/v1?label=&message=ICLR%202021&color=grey
[iclr 2022]: https://img.shields.io/static/v1?label=&message=ICLR%202022&color=grey
[iclr 2023]: https://img.shields.io/static/v1?label=&message=ICLR%202023&color=grey
[icml 2003]: https://img.shields.io/static/v1?label=&message=ICML%202003&color=grey
[icml 2007]: https://img.shields.io/static/v1?label=&message=ICML%202007&color=grey
[icml 2009]: https://img.shields.io/static/v1?label=&message=ICML%202009&color=grey
[icml 2015]: https://img.shields.io/static/v1?label=&message=ICML%202015&color=grey
[icml 2017]: https://img.shields.io/static/v1?label=&message=ICML%202017&color=grey
[icml 2018]: https://img.shields.io/static/v1?label=&message=ICML%202018&color=grey
[icml 2020]: https://img.shields.io/static/v1?label=&message=ICML%202020&color=grey
[icml 2021]: https://img.shields.io/static/v1?label=&message=ICML%202021&color=grey
[icml 2022]: https://img.shields.io/static/v1?label=&message=ICML%202022&color=grey
[ieee hpca 2014]: https://img.shields.io/static/v1?label=&message=IEEE%20HPCA%202014&color=grey
[ieee isca 2012]: https://img.shields.io/static/v1?label=&message=IEEE%20ISCA%202012&color=grey
[ieee isca 2014]: https://img.shields.io/static/v1?label=&message=IEEE%20ISCA%202014&color=grey
[ieee information theory workshop 2015]: https://img.shields.io/static/v1?label=&message=IEEE%20Information%20Theory%20Workshop%202015&color=grey
[ieee international memory worksop 2013]: https://img.shields.io/static/v1?label=&message=IEEE%20International%20Memory%20Worksop%202013&color=grey
[ieee micro 2007]: https://img.shields.io/static/v1?label=&message=IEEE%20MICRO%202007&color=grey
[ieee symposium on security and privacy 2017]: https://img.shields.io/static/v1?label=&message=IEEE%20Symposium%20on%20Security%20and%20Privacy%202017&color=grey
[ieee taslp 2019]: https://img.shields.io/static/v1?label=&message=IEEE%20TASLP%202019&color=grey
[ieee taslp 2022]: https://img.shields.io/static/v1?label=&message=IEEE%20TASLP%202022&color=grey
[ijcai 2018]: https://img.shields.io/static/v1?label=&message=IJCAI%202018&color=grey
[ijcai 2022]: https://img.shields.io/static/v1?label=&message=IJCAI%202022&color=grey
[jmlr 2008]: https://img.shields.io/static/v1?label=&message=JMLR%202008&color=grey
[jmlr 2020]: https://img.shields.io/static/v1?label=&message=JMLR%202020&color=grey
[journal of visual communication and image representation 2016]: https://img.shields.io/static/v1?label=&message=Journal%20of%20Visual%20Communication%20and%20Image%20Representation%202016&color=grey
[journal of visual communication and image representation 2019]: https://img.shields.io/static/v1?label=&message=Journal%20of%20Visual%20Communication%20and%20Image%20Representation%202019&color=grey
[mcss 1989]: https://img.shields.io/static/v1?label=&message=MCSS%201989&color=grey
[medium]: https://img.shields.io/static/v1?label=&message=Medium&color=grey
[meta]: https://img.shields.io/static/v1?label=&message=Meta&color=grey
[microsoft]: https://img.shields.io/static/v1?label=&message=Microsoft&color=grey
[naacl 2019]: https://img.shields.io/static/v1?label=&message=NAACL%202019&color=grey
[naacl 2021]: https://img.shields.io/static/v1?label=&message=NAACL%202021&color=grey
[nature 1982]: https://img.shields.io/static/v1?label=&message=Nature%201982&color=grey
[nature 1986]: https://img.shields.io/static/v1?label=&message=Nature%201986&color=grey
[neurips 1987]: https://img.shields.io/static/v1?label=&message=NeurIPS%201987&color=grey
[neurips 2012]: https://img.shields.io/static/v1?label=&message=NeurIPS%202012&color=grey
[neurips 2013]: https://img.shields.io/static/v1?label=&message=NeurIPS%202013&color=grey
[neurips 2014]: https://img.shields.io/static/v1?label=&message=NeurIPS%202014&color=grey
[neurips 2015]: https://img.shields.io/static/v1?label=&message=NeurIPS%202015&color=grey
[neurips 2016]: https://img.shields.io/static/v1?label=&message=NeurIPS%202016&color=grey
[neurips 2017]: https://img.shields.io/static/v1?label=&message=NeurIPS%202017&color=grey
[neurips 2018]: https://img.shields.io/static/v1?label=&message=NeurIPS%202018&color=grey
[neurips 2020]: https://img.shields.io/static/v1?label=&message=NeurIPS%202020&color=grey
[neurips 2022]: https://img.shields.io/static/v1?label=&message=NeurIPS%202022&color=grey
[openai]: https://img.shields.io/static/v1?label=&message=OpenAI&color=grey
[pnas 1982]: https://img.shields.io/static/v1?label=&message=PNAS%201982&color=grey
[physical review journals 1935]: https://img.shields.io/static/v1?label=&message=Physical%20Review%20Journals%201935&color=grey
[physical review journals 1993]: https://img.shields.io/static/v1?label=&message=Physical%20Review%20Journals%201993&color=grey
[physical review letters 1998]: https://img.shields.io/static/v1?label=&message=Physical%20Review%20Letters%201998&color=grey
[proceedings of the royal society 1985]: https://img.shields.io/static/v1?label=&message=Proceedings%20of%20the%20Royal%20Society%201985&color=grey
[proceedings of the royal society 1992]: https://img.shields.io/static/v1?label=&message=Proceedings%20of%20the%20Royal%20Society%201992&color=grey
[pytorch]: https://img.shields.io/static/v1?label=&message=PyTorch&color=grey
[sc 2020]: https://img.shields.io/static/v1?label=&message=SC%202020&color=grey
[sc 2021]: https://img.shields.io/static/v1?label=&message=SC%202021&color=grey
[siam journal on computing 1997]: https://img.shields.io/static/v1?label=&message=SIAM%20Journal%20on%20Computing%201997&color=grey
[sigdial 2020]: https://img.shields.io/static/v1?label=&message=SIGDIAL%202020&color=grey
[sigir 2020]: https://img.shields.io/static/v1?label=&message=SIGIR%202020&color=grey
[stoc 1996]: https://img.shields.io/static/v1?label=&message=STOC%201996&color=grey
[stanford crfm]: https://img.shields.io/static/v1?label=&message=Stanford%20CRFM&color=grey
[sysml 2018]: https://img.shields.io/static/v1?label=&message=SysML%202018&color=grey
[the bell system technical journal 1950]: https://img.shields.io/static/v1?label=&message=The%20Bell%20System%20Technical%20Journal%201950&color=grey
[towards data science]: https://img.shields.io/static/v1?label=&message=Towards%20Data%20Science&color=grey
[tsinghua university]: https://img.shields.io/static/v1?label=&message=Tsinghua%20University&color=grey
[usenix security symposium 2007]: https://img.shields.io/static/v1?label=&message=USENIX%20Security%20Symposium%202007&color=grey
[yandex]: https://img.shields.io/static/v1?label=&message=Yandex&color=grey

### Table of contents

-   [AI, DL, NLP and RL](#ai-dl-nlp-and-rl)
-   [Calculus](#calculus)
-   [Computer architecture](#computer-architecture)
-   [Computer graphics](#computer-graphics)
-   [Data structures and algorithms](#data-structures-and-algorithms)
-   [Digital electronics](#digital-electronics)
-   [Graph theory](#graph-theory)
-   [Information theory](#information-theory)
-   [Linear algebra](#linear-algebra)
-   [Measure theory](#measure-theory)
-   [Optimization theory](#optimization-theory)
-   [Probability and stochastic processes](#probability-and-stochastic-processes)
-   [Quantum computing](#quantum-computing)
-   [Signal processing](#signal-processing)

# AI, DL, NLP and RL

1. [1-bit Adam: communication efficient large-scale training with Adam’s convergence speed](https://arxiv.org/abs/2102.02888)  
   _Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He_  
   ![image][paper] ![image][deep learning] ![image][distributed training]
1. [8-bit optimizers via block-wise quantization](https://arxiv.org/abs/2110.02861)  
   _Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer_  
   ![image][paper] ![image][iclr 2022] ![image][deep learning] ![image][quantization]
1. [A 'neural' network that learns to play Backgammon](https://papers.nips.cc/paper/30-a-neural-network-that-learns-to-play-backgammon)  
   _Gerald Tesauro, Terrence J. Sejnowski_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning]
1. [A BetterTransformer for fast transformer inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)  
   _Michael Gschwind, Eric Han, Scott Wolchok, Rui Zhu, Christian Puhrsch_  
   ![image][blog] ![image][pytorch] ![image][deep learning] ![image][speedup] ![image][systems] ![image][transformers]
1. [A deep reinforced model for abstractive summarization](https://arxiv.org/abs/1705.04304)  
   _Romain Paulus, Caiming Xiong, Richard Socher_  
   ![image][paper] ![image][iclr 2018] ![image][nlp] ![image][reinforcement learning] ![image][summarization]
1. [A dynamical approach to temporal pattern processing](https://papers.nips.cc/paper/76-a-dynamical-approach-to-temporal-pattern-processing)  
   _W. Scott Stornetta, Tad Hogg, Bernardo A. Huberman_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning]
1. [A few more examples may be worth billions of parameters](https://arxiv.org/abs/2110.04374)  
   _Yuval Kirstain, Patrick Lewis, Sebastian Riedel, Omer Levy_  
   ![image][paper] ![image][few shot] ![image][nlp]
1. [A general and adaptive robust loss function](https://arxiv.org/abs/1701.03077)  
   _Jonathan T. Barron_  
   ![image][paper] ![image][cvpr 2019] ![image][deep learning]
1. [A gentle introduction to 8-bit matrix multiplication for transformers at scale using Hugging Face transformers, accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)  
   _Younes Belkada, Tim Dettmers_  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][quantization] ![image][transformers]
1. [A note on the evaluation of generative models](https://arxiv.org/abs/1511.01844)  
   _Lucas Theis, Aäron van den Oord, Matthias Bethge_  
   ![image][paper] ![image][iclr 2016] ![image][deep learning] ![image][generative models]
1. [A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings](https://arxiv.org/abs/1805.06297)  
   _Mikel Artetxe, Gorka Labaka, Eneko Agirre_  
   ![image][paper] ![image][acl 2018] ![image][embeddings] ![image][nlp]
1. [A simple but tough-to-beat baseline for sentence embeddings](https://openreview.net/forum?id=SyK00v5xx)  
   _Sanjeev Arora, Yingyu Liang, Tengyu Ma_  
   ![image][paper] ![image][iclr 2017] ![image][embeddings] ![image][nlp]
1. [A simple language model for task-oriented dialogue](https://arxiv.org/abs/2005.00796)  
   _Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, Richard Socher_  
   ![image][paper] ![image][neurips 2020] ![image][dialog] ![image][nlp]
1. [A simple neural attentive meta-learner](https://arxiv.org/abs/1707.03141)  
   _Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel_  
   ![image][paper] ![image][iclr 2018] ![image][deep learning] ![image][few shot] ![image][meta learning]
1. [A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)  
   _Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap_  
   ![image][paper] ![image][neurips 2017] ![image][deep learning] ![image][relational reasoning]
1. [A style-based generator architecture for generative adversarial networks](https://arxiv.org/abs/1812.04948)  
   _Tero Karras, Samuli Laine, Timo Aila_  
   ![image][paper] ![image][cvpr 2019] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [A stylometric inquiry into hyperpartisan and fake news](https://arxiv.org/abs/1702.05638)  
   _Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, Benno Stein_  
   ![image][paper] ![image][acl 2018] ![image][fake content detection] ![image][nlp]
1. [A3T: adversarially augmented adversarial training](https://arxiv.org/abs/1801.04055)  
   _Akram Erraqabi, Aristide Baratin, Yoshua Bengio, Simon Lacoste-Julien_  
   ![image][paper] ![image][neurips 2017] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [AdapterHub: a framework for adapting transformers](https://arxiv.org/abs/2007.07779)  
   _Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych_  
   ![image][paper] ![image][emnlp 2020] ![image][efficient finetuning] ![image][nlp] ![image][transformers]
1. [Adversarial approximate inference for speech to electroglottograph conversion](https://arxiv.org/abs/1903.12248)  
   _Prathosh A. P., Varun Srivastava, Mayank Mishra_  
   ![image][paper] ![image][ieee taslp 2019] ![image][adversarial learning] ![image][approximate inference] ![image][deep learning] ![image][generative models] ![image][speech]
1. [Adversarial autoencoders](https://arxiv.org/abs/1511.05644)  
   _Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey_  
   ![image][paper] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Adversarial examples that fool both computer vision and time-limited humans](https://arxiv.org/abs/1802.08195)  
   _Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein_  
   ![image][paper] ![image][neurips 2018] ![image][adversarial examples] ![image][deep learning]
1. [Adversarial feature learning](https://arxiv.org/abs/1605.09782)  
   _Jeff Donahue, Philipp Krähenbühl, Trevor Darrell_  
   ![image][paper] ![image][iclr 2017] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Adversarial generation of natural language](https://arxiv.org/abs/1705.10929)  
   _Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, Aaron Courville_  
   ![image][paper] ![image][acl 2017] ![image][adversarial learning] ![image][generative models] ![image][nlp]
1. [Adversarial information factorization](https://arxiv.org/abs/1711.05175)  
   _Antonia Creswell, Yumnah Mohamied, Biswa Sengupta, Anil A Bharath_  
   ![image][paper] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Adversarially learned inference](https://arxiv.org/abs/1606.00704)  
   _Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, Aaron Courville_  
   ![image][paper] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [AlexaTM 20B: few-shot learning using a large-scale multilingual seq2seq model](https://arxiv.org/abs/2208.01448)  
   _Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan_  
   ![image][paper] ![image][large models] ![image][nlp] ![image][transformers]
1. [Amazon SageMaker model parallelism: a general and flexible framework for large model training](https://arxiv.org/abs/2111.05972)  
   _Can Karakus, Rahul Huilgol, Fei Wu, Anirudh Subramanian, Cade Daniel, Derya Cavdar, Teng Xu, Haohan Chen, Arash Rahnama, Luis Quintela_  
   ![image][paper] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][systems]
1. [An image is worth 16x16 words: transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)  
   _Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby_  
   ![image][paper] ![image][iclr 2021] ![image][computer vision]
1. [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)  
   _Sebastian Ruder_  
   ![image][paper] ![image][deep learning] ![image][optimization]
1. [Analysing mathematical reasoning abilities of neural models](https://arxiv.org/abs/1904.01557)  
   _David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli_  
   ![image][paper] ![image][iclr 2019] ![image][deep learning]
1. [Approximation by superpositions of sigmoidal function](https://link.springer.com/article/10.1007/BF02551274)  
   _George Cybenko_  
   ![image][paper] ![image][mcss 1989] ![image][deep learning]
1. Artificial Intelligence: a modern approach  
   _Stuart Russell, Peter Norvig_  
   ![image][book]
1. [Aspect based sentiment analysis with gated convolutional networks](https://arxiv.org/abs/1805.07043)  
   _Wei Xue, Tao Li_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][text classification]
1. [Attention is all you need](https://arxiv.org/abs/1706.03762)  
   _Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin_  
   ![image][paper] ![image][neurips 2017] ![image][deep learning] ![image][transformers]
1. [Auto-encoding variational Bayes](https://arxiv.org/abs/1312.6114)  
   _Diederik P. Kingma, Max Welling_  
   ![image][paper] ![image][iclr 2014] ![image][deep learning] ![image][generative models] ![image][variational inference]
1. [Backpropagation through the void: optimizing control variates for black-box gradient estimation](https://arxiv.org/abs/1711.00123)  
   _Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, David Duvenaud_  
   ![image][paper] ![image][iclr 2018] ![image][backpropagation] ![image][deep learning] ![image][discrete optimization] ![image][gradient estimation] ![image][optimization] ![image][reinforcement learning] ![image][variational inference]
1. [BART: denoising sequence-to-sequence pre-training for natural language generation, translation and comprehension](https://arxiv.org/abs/1910.13461)  
   _Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer_  
   ![image][paper] ![image][acl 2020] ![image][nlp] ![image][transformers]
1. [Batch normalization: accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167)  
   _Sergey Ioffe, Christian Szegedy_  
   ![image][paper] ![image][icml 2015] ![image][deep learning] ![image][normalization] ![image][optimization]
1. [Behavioral cloning from observation](https://arxiv.org/abs/1805.01954)  
   _Faraz Torabi, Garrett Warnell, Peter Stone_  
   ![image][paper] ![image][ijcai 2018] ![image][behavior and control] ![image][deep learning] ![image][genetic algorithms] ![image][robotics]
1. [BERT: pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)  
   _Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova_  
   ![image][paper] ![image][naacl 2019] ![image][nlp] ![image][transformers]
1. [Beyond domain APIs: Task-oriented conversational modeling with unstructured knowledge access](https://arxiv.org/abs/2006.03533)  
   _Seokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, Dilek Hakkani-Tur_  
   ![image][paper] ![image][sigdial 2020] ![image][dialog] ![image][nlp]
1. [BLOOM: A 176B-parameter open-access multilingual language model](https://arxiv.org/abs/2211.05100)  
   _Aaron Gokaslan, Abheesht Sharma, Abhinav Ramesh Kashyap, Adam Roberts, Adi Simhi, Ahmed Baruwa, Aitor Soroa, Albert Villanova del Moral, Albert Webson, Alexander M. Rush, Alexandra Sasha Luccioni, Alfredo Palasciano, Alham Fikri Aji, Alice Rueda, Alison Callahan, Amanda Pestana, Amanpreet Singh, Amir Feizpour, Amit Alfassy, Ammar Khan, Amy Faranak, Ana Santos, Anastasia Cheveleva, Andrea Santilli, Angela Fan, Angelina McMillan-Major, Anima Shukla, Anna Rogers, Anne-Laure Ligozat, Anthony Hevia, Antigona Unldreaj, Antoine Chaffin, Antonio Miranda-Escalada, Arash Aghagol, Arezoo Abdollahi, Ariel Kreisberg Nitzav, Arjun Subramonian, Arnaud Stiegler, Arun Raja, Aurélie Névéol, Aycha Tammour, Ayush Singh, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Benjamin Beilharz, Benjamin Heinzerling, Benoît Sagot, Bharat Saxena, Bo Wang, Caio Brito, Canwen Xu, Carlos Muñoz Ferrandis, Charles Lovering, Chenghao Mou, Chenglei Si, Chenxi Zhou, Chirag Jain, Chris Emezue, Christopher Akiki, Christopher Klamm, Chuxin Xu, Clémentine Fourrier, Colin Leong, Colin Raffel, Conglong Li, Dan Garrette, Daniel Hesslow, Daniel León Periñán, Daniel Molano, Daniel van Strien, Danish Contractor, David Ifeoluwa Adelani, David Lansky, Davis David, Davut Emre Taşar, Debajyoti Datta, Deepak Narayanan, Deepak Tunuguntla, Dian Yu, Douwe Kiela, Dragomir Radev, Duong A. Nguyen, Eduardo González Ponferrada, Edward Tan, Efrat Levkovizh, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Eliza Szczechla, Elizabeth Salesky, Ellie Pavlick, Emi Baylor, Enrique Manjavacas, Ethan Kim, Eyal Bar Natan, Ezinwanne Ozoani, Fabio Barth, Fatima Mirza, Florian Fuhrimann, Francesco De Toni, Frankline Ononiwu, François Yvon, Gabriel Altay, Genta Indra Winata, Germán Kruszewski, Giada Pistilli, Giyaseddin Bayrak, Gully Burns, Gunjan Chhablani, Gérard Dupont, Habib Rezanejad, Hadar Tojarieh, Hady Elsahar, Hailey Schoelkopf, Hamza Benyamina, Han Wang, Harshit Pandey, Hatim Bourfoune, Helena U. Vrabec, Hendrik Strobelt, Hessie Jones, Hieu Tran, Hugo Laurençon, Huu Nguyen, Hyung Won Chung, Ian Yu, Idris Abdulmumin, Imane Bello, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isaac Johnson, Isar Nejadgholi, Ishani Dash, Itziar Gonzalez-Dios, Iz Beltagy, Jaesung Tae, Jan-Christoph Kalo, Jared Casper, Jason Alan Fries, Jason Phang, Javier de la Rosa, Jeff Rasley, Jekaterina Novikova, Jenny Chim, Jesse Dodge, Jesse Passmore, Jessica Zosa Forde, Jian Zhu, Jihyun Kang, John Giorgi, Jonas Golde, Jonathan Chang, Jonathan Tow, Jordan Clive, Jos Rozen, Jose David Posada, Joseph Tobing, Josh Seltzer, Joydeep Bhattacharjee, Julien Launay, Julio Bonis Sanz, Jungo Kasai, Jörg Frohberg, Karthik Rangasai Sivaraman, Ken Kawamura, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leo Gao, Leon Weber, Liam Hazan, Lintang Sutawika, Livia Dutra, Lokesh Bulchandani, Long Phan, Loubna Ben allal, Lu Liu, Lucile Saulnier, Ludovic Tanguy, Luisa Shinzato, M Saiful Bari, Madeleine Hahn de Bykhovetz, Maged S. Al-shaibani, Maiko Takeuchi, Mairon Samagaio, Manan Dey, Manuel Romero Muñoz, Maraim Elbadri, Maraim Masoud, Marc Pàmies, Margaret Mitchell, Margot Mieskes, Maria A Castillo, Marianna Nezhurina, Marine Carpuat, Mario Sänger, Mario Šaško, Marissa Gerchick, Martha Akinlolu, María Grandury, Mathilde Bras, Matteo Manica, Matthias Gallé, Matthias Samwald, Max Huang, Max Ryabinin, Maximin Coavoux, Mayank Mishra, Mayank Singh, Michael Cullan, Michael McKenna, Michael Weinberg, Michiel De Wolf, Mike Qiu, Mike Tian-Jian Jiang, Mina Mihaljcic, Minh Chien Vu, Minjia Zhang, Minna Liu, Miruna Clinciu, Mohammad A. Jauhar, Mohammad Shoeybi, Moritz Freidank, Muhammed Ghauri, Mustafa Ghaleb, Mykola Burynok, Myriam Peyrounette, Myungsun Kang, Nafis Abrar, Najoung Kim, Natasha Seelam, Nathan Dahlberg, Nazneen Rajani, Newton Cheng, Nicholas Michio Broad, Nicolas Patry, Nihal Nayak, Niklas Muennighoff, Nikolaus Muellner, Nishant Subramani, Nora Kassner, Nouamane Tazi, Nour Elkott, Nour Fahmy, Nurulaqilla Khamis, Ofir Press, Olanrewaju Samuel, Olatunji Ruwase, Oleg Serikov, Olivier Nguyen, Omar Espejel, Omar Sanseviero, Omer Antverg, Ona de Gibert, Oskar van der Wal, Pascale Fung, Patrick Haller, Patrick von Platen, Paulo Villegas, Pawan Sasanka Ammanamanchi, Pedro Ortiz Suarez, Peter Henderson, Pierre Colombo, Pierre Cornette, Pierre François Lavallée, Priscilla Amuok, Quentin Lhoest, Rachel Bawden, Ramya Chandrasekhar, Ran An, Rasmus Kromann, Renata Eisenberg, Rheza Harliman, Rishi Bommasani, Robert Martin, Roberto Luis López, Rodrigo Canalli, Roman Castagné, Rosaline Su, Rui Ribeiro, Rui Zhang, Ruisi Su, Ruochen Zhang, Ryan Hao, Ryan Teehan, Rémi Lacroix, Sabrina J. Mielke, Salomey Osei, Samira Alizadeh, Sampo Pyysalo, Samson Tan, Samuel Albanie, Samuel Cahyawijaya, Samuele Garda, Samyam Rajbhandari, Sanchit Gandhi, Sarmad Shubber, Sebastian Gehrmann, Sebastian Nagel, Shachar Mirkin, Shaden Smith, Shaked Brody, Shamik Bose, Shamsuddeen Hassan Muhammad, Shani Pais, Shanya Sharma, Shayne Longpre, Sheng Shen, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Silas Wang, Simon Ott, Sinee Sang-aroonsiri, Somaieh Nikpoor, Sourav Roy, Srishti Kumar, Srulik Ben-David, Stanislav Silberberg, Stas Bekman, Stefan Schweter, Stella Biderman, Stephen H. Bach, Stéphane Requena, Suhas Pai, Suraj Patil, Sushil Bharati, Suzana Ilić, Sydney Zink, Sylvain Viguier, Taewoon Kim, Tali Bers, Tanmay Laud, Tatiana Shavrina, Teven Le Scao, Thanh Le, Thibault Fevry, Thomas Scialom, Thomas Wang, Thomas Wolf, Théo Gigant, Tiago Timponi Torrent, Tian Yun, Tim Dettmers, Timo Schick, Tobi Oyebade, Tomasz Limisiewicz, Tomoya Kainuma, Trieu Le, Trishala Neeraj, Tristan Thrush, Urmish Thakker, Valentin Danchev, Vassilina Nikoulina, Verena Rieser, Veronika Laippala, Victor Sanh, Vikas Raunak, Violette Lepercq, Vitaly Protasov, Vladislav Mikhailov, Vrinda Prabhu, Wilson Y. Lee, Wojciech Kusa, Xiangru Tang, Yacine Jernite, Yada Pruksachatkun, Yallow Uri, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yonatan Belinkov, Younes Belkada, Yoyo Yang, Yu Xu, Zach Nguyen, Zachary Bamberger, Zaid Alyafeai, Zdeněk Kasner, Zeerak Talat, Zhe Tan, Zheng-Xin Yong, Zhiqing Sun, Zhongli Xie, Zifan Ye_  
   ![image][paper] ![image][distributed training] ![image][large models] ![image][nlp] ![image][transformers]
1. [Bootstrapping entity alignment with knowledge graph embedding](https://www.ijcai.org/proceedings/2018/611)  
   _Zequn Sun, Wei Hu, Qingheng Zhang, Yuzhong Qu_  
   ![image][paper] ![image][ijcai 2018] ![image][embeddings] ![image][knowledge graphs] ![image][nlp]
1. [Bridging the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation](https://www.aclweb.org/anthology/2020.emnlp-main.275/)  
   _Xiuyi Chen, Fandong Meng, Peng Li, Feilong Chen, Shuang Xu, Bo Xu, Jie Zhou_  
   ![image][paper] ![image][emnlp 2020] ![image][dialog] ![image][nlp] ![image][variational inference]
1. [ChatGPT: optimizing language models for dialogue](https://openai.com/blog/chatgpt/)  
   ![image][blog] ![image][openai] ![image][dialog] ![image][nlp]
1. [ColBERT: efficient and effective passage search via contextualized late interaction over BERT](https://arxiv.org/abs/2004.12832)  
   _Omar Khattab, Matei Zaharia_  
   ![image][paper] ![image][sigir 2020] ![image][information retrieval] ![image][nlp]
1. [Colossal-AI: a unified deep learning system for large-scale parallel training](https://arxiv.org/abs/2110.14883)  
   _Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuanrui Wang, Fan Cui, Yang You_  
   ![image][paper] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][systems]
1. [Compiling machine learning programs via high-level tracing](https://research.google/pubs/pub47008/)  
   _Roy Frostig, Matthew Johnson, Chris Leary_  
   ![image][paper] ![image][sysml 2018] ![image][deep learning] ![image][systems]
1. [Conceptual captions: a cleaned, hypernymed, image alt-text dataset for automatic image captioning](https://aclanthology.org/P18-1238/)  
   _Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut_  
   ![image][paper] ![image][acl 2018] ![image][computer vision] ![image][dataset] ![image][nlp]
1. [Conditional image synthesis with auxilliary classifier GANs](https://arxiv.org/abs/1610.09585)  
   _Augustus Odena, Christopher Olah, Jonathon Shlens_  
   ![image][paper] ![image][icml 2017] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Connectivity versus entropy](https://papers.nips.cc/paper/63-connectivity-versus-entropy)  
   _Yaser S. Abu-Mostafa_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning]
1. [Constituency parsing with a self-attentive encoder](https://arxiv.org/abs/1805.01052)  
   _Nikita Kitaev, Dan Klein_  
   ![image][paper] ![image][acl 2018] ![image][constituency parsing] ![image][nlp]
1. [Constraint based knowledge base distillation in end-to-end task oriented dialogs](https://arxiv.org/abs/2109.07396)  
   _Dinesh Raghu, Atishya Jain, Mausam, Sachindra Joshi_  
   ![image][paper] ![image][acl 2021] ![image][dialog] ![image][nlp]
1. [Context generation improves open domain question answering](https://arxiv.org/abs/2210.06349)  
   _Dan Su, Mostofa Patwary, Shrimai Prabhumoye, Peng Xu, Ryan Prenger, Mohammad Shoeybi, Pascale Fung, Anima Anandkumar, Bryan Catanzaro_  
   ![image][paper] ![image][few shot] ![image][nlp] ![image][question answering]
1. [Convert transformers to ONNX with hugging face optimum](https://huggingface.co/blog/convert-transformers-to-onnx)  
   _Philipp Schmid_  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][model saving] ![image][systems]
1. [Convolutional networks for graphs for learning molecular fingerprints](https://arxiv.org/abs/1509.09292)  
   _David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, Ryan P. Adams_  
   ![image][paper] ![image][neurips 2015] ![image][deep learning] ![image][molecular chemistry]
1. [Convolutional neural network language models](https://aclanthology.org/D16-1123/)  
   _Ngoc-Quan Pham, Germán Kruszewski, Gemma Boleda_  
   ![image][paper] ![image][emnlp 2016] ![image][nlp]
1. [Countering adversarial images using input transformations](https://arxiv.org/abs/1711.00117)  
   _Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial examples] ![image][deep learning] ![image][robustness]
1. [Crosslingual generalization through multitask finetuning](https://arxiv.org/abs/2211.01786)  
   _Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel_  
   ![image][paper] ![image][distributed training] ![image][instruction finetuning] ![image][large models] ![image][nlp] ![image][transformers]
1. [Curriculum learning](https://dl.acm.org/citation.cfm?id=1553380)  
   _Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston_  
   ![image][paper] ![image][icml 2009] ![image][curriculum learning] ![image][deep learning]
1. [Cutting down on prompts and parameters: simple few-shot learning with language models](https://arxiv.org/abs/2106.13353)  
   _Robert L. Logan IV, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel_  
   ![image][paper] ![image][acl 2022] ![image][efficient finetuning] ![image][few shot] ![image][nlp]
1. [Deep Boltzmann machines](https://proceedings.mlr.press/v5/salakhutdinov09a.html)  
   _Ruslan Salakhutdinov, Geoffrey Hinton_  
   ![image][paper] ![image][aistats 2009] ![image][boltzmann machines] ![image][deep learning] ![image][energy-based models]
1. [Deep complex networks](https://arxiv.org/abs/1705.09792)  
   _Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, João Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, Christopher J Pal_  
   ![image][paper] ![image][iclr 2018] ![image][complex numbers] ![image][deep learning]
1. Deep learning  
   _Ian Goodfellow, Yoshua Bengio, Aaron Courville_  
   ![image][book]
1. [Deep learning and the information bottleneck principle](https://arxiv.org/abs/1503.02406)  
   _Naftali Tishby, Noga Zaslavsky_  
   ![image][paper] ![image][ieee information theory workshop 2015] ![image][deep learning] ![image][information theory]
1. [Deep learning techniques for super-resolution in video games](https://arxiv.org/abs/2012.09810)  
   _Alexander Watson_  
   ![image][paper] ![image][computer graphics] ![image][computer vision] ![image][deep learning] ![image][image super resolution]
1. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385)  
   _Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun_  
   ![image][paper] ![image][cvpr 2016] ![image][computer vision] ![image][image classification]
1. [Deep text classification can be fooled](https://arxiv.org/abs/1704.08006)  
   _Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, Wenchang Shi_  
   ![image][paper] ![image][ijcai 2018] ![image][adversarial examples] ![image][nlp]
1. [DeepSpeed compression: a composable library for extreme compression and zero-cost quantization](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)  
   _DeepSpeed Team, Andrey Proskurin_  
   ![image][blog] ![image][microsoft] ![image][compression] ![image][deep learning] ![image][quantization]
1. [DeepSpeed Inference: enabling efficient inference of transformer models at unprecedented scale](https://arxiv.org/abs/2207.00032)  
   _Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He_  
   ![image][paper] ![image][deep learning] ![image][large models] ![image][systems]
1. [DeepSpeed powers 8x larger MoE model training with high performance](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)  
   _DeepSpeed Team, Z-code Team_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][large models] ![image][mixture of experts] ![image][transformers]
1. [DeepSpeed: accelerating large-scale model inference and training via system optimizations and compression](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/#:~:text=DeepSpeed%20Inference%20also%20supports%20fast,multiple%20GPUs%20for%20parallel%20execution.)  
   _DeepSpeed Team, Rangan Majumder, Andrey Proskurin_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][large models] ![image][systems]
1. [DeepSpeed: advancing MoE inference and training to power next-generation AI scale](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)  
   _DeepSpeed Team, Andrey Proskurin_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][large models] ![image][mixture of experts] ![image][transformers]
1. [Denoising distantly supervised open-domain question answering](https://aclanthology.org/P18-1161/)  
   _Yankai Lin, Haozhe Ji, Zhiyuan Liu, Maosong Sun_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][question answering]
1. [Diffusion convolutional recurrent neural network: data-driven traffic forecasting](https://arxiv.org/abs/1707.01926)  
   _Yaguang Li, Rose Yu, Cyrus Shahabi, Yan Liu_  
   ![image][paper] ![image][iclr 2018] ![image][deep learning] ![image][graph neural networks] ![image][spatio-temporal] ![image][time series]
1. [Discrete variational autoencoders](https://arxiv.org/abs/1609.02200)  
   _Jason Tyler Rolfe_  
   ![image][paper] ![image][iclr 2017] ![image][deep learning] ![image][generative models] ![image][variational inference]
1. [Disentangling by factorising](https://arxiv.org/abs/1802.05983)  
   _Hyunjik Kim, Andriy Mnih_  
   ![image][paper] ![image][icml 2018] ![image][deep learning] ![image][disentanglement] ![image][generative models] ![image][variational inference]
1. [Disentangling language and knowledge in task-oriented dialogs](https://arxiv.org/abs/1805.01216)  
   _Dinesh Raghu, Nikhil Gupta, Mausam_  
   ![image][paper] ![image][naacl 2019] ![image][dialog] ![image][disentanglement] ![image][nlp]
1. [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781)  
   _Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean_  
   ![image][paper] ![image][iclr 2013] ![image][embeddings] ![image][nlp]
1. [Efficient large scale language modeling with mixtures of experts](https://arxiv.org/abs/2112.10684)  
   _Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov_  
   ![image][paper] ![image][deep learning] ![image][large models] ![image][nlp] ![image][transformers]
1. [Efficient large-scale language model training on GPU clusters using Megatron-LM](https://arxiv.org/abs/2104.04473)  
   _Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia_  
   ![image][paper] ![image][sc 2021] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][transformers]
1. [Enchancing the reliability of out-of-distribution image detection in neural networks](https://arxiv.org/abs/1706.02690)  
   _Shiyu Liang, Yixuan Li, R. Srikant_  
   ![image][paper] ![image][iclr 2018] ![image][deep learning] ![image][out-of-distribution detection]
1. [End-to-end task-oriented dialog modeling with semi-structured knowledge management](https://arxiv.org/abs/2106.11796)  
   _Silin Gao, Ryuichi Takanobu, Antoine Bosselut, Minlie Huang_  
   ![image][paper] ![image][ieee taslp 2022] ![image][dialog] ![image][nlp]
1. [Ensemble adversarial training: attacks and defenses](https://arxiv.org/abs/1705.07204)  
   _Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial examples] ![image][deep learning] ![image][robustness]
1. [Equilibrium propagation: bridging the gap between energy-based models and backpropagation](https://arxiv.org/abs/1602.05179)  
   _Benjamin Scellier, Yoshua Bengio_  
   ![image][paper] ![image][frontiers in computational neuroscience 2017] ![image][backpropagation] ![image][deep learning] ![image][enery based models]
1. [Exemplar encoder-decoder for neural conversation generation](https://www.aclweb.org/anthology/P18-1123/)  
   _Gaurav Pandey, Danish Contractor, Vineet Kumar, Sachindra Joshi_  
   ![image][paper] ![image][acl 2018] ![image][dialog] ![image][nlp]
1. [Exploring deep recurrent models with reinforcement learning for molecule design](https://openreview.net/forum?id=HkcTe-bR-)  
   _Daniel Neil, Marwin Segler, Laura Guasch, Mohamed Ahmed, Dean Plumbley, Matthew Sellwood, Nathan Brown_  
   ![image][paper] ![image][iclr 2018] ![image][molecular chemistry] ![image][reinforcement learning]
1. [Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/abs/1910.10683)  
   _Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu_  
   ![image][paper] ![image][jmlr 2020] ![image][nlp] ![image][transformers]
1. [Extreme compression for pre-trained transformers made simple and efficient](https://arxiv.org/abs/2206.01859)  
   _Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He_  
   ![image][paper] ![image][compression] ![image][deep learning] ![image][distillation] ![image][large models] ![image][pruning] ![image][transformers]
1. [Fast abstractive summarization with reinforce-selected sentence rewriting](https://arxiv.org/abs/1805.11080)  
   _Yen-Chun Chen, Mohit Bansal_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][reinforcement learning] ![image][summarization]
1. [Fast transformer decoding: one write-head is all you need](https://arxiv.org/abs/1911.02150)  
   _Noam Shazeer_  
   ![image][paper] ![image][deep] ![image][transformers]
1. [FFJORD: Free-form continuous dynamics for scalable reversible generative models](https://arxiv.org/abs/1810.01367)  
   _Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud_  
   ![image][paper] ![image][iclr 2019] ![image][deep learning] ![image][generative models]
1. [Finetuned language models are zero-shot learners](https://arxiv.org/abs/2109.01652)  
   _Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le_  
   ![image][paper] ![image][iclr 2022] ![image][nlp] ![image][transformers] ![image][zero shot]
1. [FlashAttention: fast and memory-efficient exact attention with IO-awareness](https://arxiv.org/abs/2205.14135)  
   _Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré_  
   ![image][paper] ![image][deep learning] ![image][speedup] ![image][transformers]
1. [FlashAttention: fast transformer training with long sequences](https://crfm.stanford.edu/2023/01/13/flashattention.html)  
   _Tri Dao_  
   ![image][blog] ![image][stanford crfm] ![image][deep learning] ![image][systems] ![image][transformers]
1. [Foundations of NLP explained visually: beam search, how it works](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24)  
   _Ketan Doshi_  
   ![image][blog] ![image][towards data science] ![image][algorithms] ![image][nlp]
1. [Generating adversarial examples with adversarial networks](https://www.ijcai.org/proceedings/2018/543)  
   _Chaowei Xiao, Bo Li, Jun-yan Zhu, Warren He, Mingyan Liu, Dawn Song_  
   ![image][paper] ![image][ijcai 2018] ![image][adversarial examples] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Generating sentences from a continuous space](https://arxiv.org/abs/1511.06349)  
   _Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio_  
   ![image][paper] ![image][conll 2016] ![image][nlp]
1. [Generation-augmented retrieval for open-domain question answering](https://arxiv.org/abs/2009.08553)  
   _Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen_  
   ![image][paper] ![image][acl 2021] ![image][nlp] ![image][question answering]
1. [Generative adversarial nets](https://arxiv.org/abs/1406.2661)  
   _Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio_  
   ![image][paper] ![image][neurips 2014] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. Genetic algorithms in search, optimization and machine learning  
   _David E. Goldberg_  
   ![image][book]
1. [GeoMAN: multi-level attention networks for geo-sensory time series prediction](https://www.ijcai.org/proceedings/2018/476)  
   _Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, Yu Zheng_  
   ![image][paper] ![image][ijcai 2018] ![image][deep learning] ![image][spatio-temporal] ![image][time series]
1. [GLaM: efficient scaling of language models with mixture-of-experts](https://arxiv.org/abs/2112.06905)  
   _Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui_  
   ![image][paper] ![image][icml 2022] ![image][deep learning] ![image][large models] ![image][mixture of experts] ![image][transformers]
1. [GLM-130B: an open bilingual pre-trained model](https://keg.cs.tsinghua.edu.cn/glm-130b/)  
   ![image][blog] ![image][tsinghua university] ![image][large models] ![image][nlp] ![image][transformers]
1. [Going deeper with convolutions](https://arxiv.org/abs/1409.4842)  
   _Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich_  
   ![image][paper] ![image][cvpr 2015] ![image][computer vision] ![image][image classification]
1. [GPT-NeoX-20B: an open-source autoregressive language model](https://arxiv.org/abs/2204.06745)  
   _Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach_  
   ![image][paper] ![image][acl 2022] ![image][large models] ![image][nlp] ![image][transformers]
1. [Gradient-based hyperparameter optimization through reversible learning](https://arxiv.org/abs/1502.03492)  
   _Dougal Maclaurin, David Duvenaud, Ryan P. Adams_  
   ![image][paper] ![image][icml 2015] ![image][deep learning] ![image][meta learning]
1. [Graph attention networks](https://arxiv.org/abs/1710.10903)  
   _Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio_  
   ![image][paper] ![image][iclr 2018] ![image][deep learning] ![image][graph neural networks]
1. [Hierarchical neural story generation](https://arxiv.org/abs/1805.04833)  
   _Angela Fan, Mike Lewis, Yann Dauphin_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][story generation]
1. [Hindsight: posterior-guided training of retrievers for improved open-ended generation](https://arxiv.org/abs/2110.07752)  
   _Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning_  
   ![image][paper] ![image][iclr 2022] ![image][nlp] ![image][retrieval-augmented generation] ![image][variational inference]
1. [HotFlip: white-box adversarial examples for text classification](https://arxiv.org/abs/1712.06751)  
   _Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou_  
   ![image][paper] ![image][acl 2018] ![image][adversarial examples] ![image][nlp]
1. [How big should my language model be?](https://huggingface.co/calculator/)  
   _Teven Le Scao_  
   ![image][blog] ![image][huggingface] ![image][large models] ![image][nlp] ![image][scaling laws]
1. [How we sped up transformer inference 100x for 🤗 API customers](https://huggingface.co/blog/accelerated-inference)  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][transformers]
1. [How 🤗 Accelerate runs very large models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models)  
   _Sylvain Gugger_  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][transformers]
1. [HyKnow: end-to-end task-oriented dialog modeling with hybrid knowledge management](https://arxiv.org/abs/2105.06041)  
   _Silin Gao, Ryuichi Takanobu, Wei Peng, Qun Liu, Minlie Huang_  
   ![image][paper] ![image][acl 2021] ![image][dialog] ![image][nlp]
1. [Hyperparameter search with Transformers and Ray Tune](https://huggingface.co/blog/ray-tune)  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][hyperparameter search] ![image][transformers]
1. [Image-to-image translation with conditional generative adversarial networks](https://arxiv.org/abs/1611.07004)  
   _Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros_  
   ![image][paper]
1. [ImageNet classification using deep convolutional neural networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)  
   _Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton_  
   ![image][paper] ![image][neurips 2012] ![image][computer vision] ![image][image classification]
1. [Improving entity linking by modeling latent relations between mentions](https://arxiv.org/abs/1804.10637)  
   _Phong Le, Ivan Titov_  
   ![image][paper] ![image][acl 2018] ![image][entity linking] ![image][nlp]
1. [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)  
   _Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre_  
   ![image][paper] ![image][hallucination] ![image][information retrieval] ![image][nlp] ![image][retrieval-augmented generation]
1. [Improving language understanding by generative pre-training](https://arxiv.org/abs/2112.04426)  
   _Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever_  
   ![image][paper] ![image][icml 2022] ![image][nlp] ![image][retrieval-augmented generation]
1. [Incredibly fast BLOOM inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)  
   _Stas Bekman, Sylvain Gugger_  
   ![image][blog] ![image][huggingface] ![image][large models] ![image][nlp] ![image][transformers]
1. [Inference suboptimality in variational autoencoders](https://arxiv.org/abs/1801.03558)  
   _Chris Cremer, Xuechen Li, David Duvenaud_  
   ![image][paper] ![image][iclr 2018] ![image][approximate inference] ![image][deep learning] ![image][variational inference]
1. [InfoGAN: interpretable representation learning by information maximizing generative adversarial nets](https://arxiv.org/abs/1606.03657)  
   _Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel_  
   ![image][paper] ![image][neurips 2016] ![image][adversarial learning] ![image][deep learning] ![image][generative models] ![image][information theory]
1. [Interpretable convolutional neural networks via feedforward design](https://arxiv.org/abs/1810.02786)  
   _C.-C. Jay Kuo, Min Zhang, Siyang Li, Jiali Duan, Yueru Chen_  
   ![image][paper] ![image][journal of visual communication and image representation 2019] ![image][computer vision]
1. [Introducing Turing image super resolution: AI powered image enhancements for Microsoft Edge and Bing maps](https://blogs.bing.com/search-quality-insights/may-2022/Turing-Image-Super-Resolution)  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][image super resolution]
1. [Introducing 🤗 accelerate](https://huggingface.co/blog/accelerate-library)  
   _Sylvain Gugger_  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][distributed training] ![image][large models]
1. [Is the future of neural networks Sparse? An introduction (1/N)](https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70)  
   _François Lagunas_  
   ![image][blog] ![image][medium] ![image][deep learning] ![image][sparse matrices]
1. [Know what you don't know: unanswerable questions for SQuAD](https://arxiv.org/abs/1806.03822)  
   _Pranav Rajpurkar, Robin Jia, Percy Liang_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][question answering]
1. [Knowledge-grounded dialogue generation with pre-trained language models](https://arxiv.org/abs/2010.08824)  
   _Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan_  
   ![image][paper] ![image][emnlp 2020] ![image][dialog] ![image][nlp]
1. [Language modelling with pixels](https://arxiv.org/abs/2207.06991)  
   _Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, Desmond Elliott_  
   ![image][paper] ![image][computer vision] ![image][nlp]
1. [Language models (mostly) know what they know](https://arxiv.org/abs/2207.05221)  
   _Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, Jared Kaplan_  
   ![image][paper] ![image][hallucination] ![image][nlp]
1. [Language models are unsupervised multitask learners](https://openai.com/blog/better-language-models/)  
   _Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever_  
   ![image][paper] ![image][nlp] ![image][transformers]
1. [Learning activation functions to improve deep neural networks](https://arxiv.org/abs/1412.6830)  
   _Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi_  
   ![image][paper] ![image][deep learning]
1. [Learning discourse-level diversity for neural dialog models using conditional variational autoencoders](https://arxiv.org/abs/1703.10960)  
   _Tiancheng Zhao, Ran Zhao, Maxine Eskenazi_  
   ![image][paper] ![image][acl 2017] ![image][dialog] ![image][nlp] ![image][variational inference]
1. [Learning on a general network](https://papers.nips.cc/paper/9-learning-on-a-general-network)  
   _Amir F. Atiya_  
   ![image][paper] ![image][neurips 1987] ![image][backpropagation] ![image][deep learning]
1. [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)  
   _David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams_  
   ![image][paper] ![image][nature 1986] ![image][backpropagation] ![image][deep learning]
1. [Learning transferable visual models from natural language supervision](https://arxiv.org/abs/2103.00020)  
   _Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever_  
   ![image][paper] ![image][icml 2021] ![image][computer vision]
1. [Learning word embeddings efficiently with noise-contrastive estimation](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation)  
   _Andriy Mnih, Koray Kavukcuoglu_  
   ![image][paper] ![image][neurips 2013] ![image][embeddings] ![image][nlp]
1. [Lessons learned on language model safety and misuse](https://openai.com/blog/language-model-safety-and-misuse/)  
   ![image][blog] ![image][openai] ![image][ethical impacts of ai] ![image][nlp]
1. [Linformer: self-attention with linear complexity](https://arxiv.org/abs/2006.04768)  
   _Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma_  
   ![image][paper] ![image][deep learning] ![image][speedup] ![image][transformers]
1. [LLM.int8(): 8-bit matrix multiplication for transformers at scale](https://arxiv.org/abs/2208.07339)  
   _Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer_  
   ![image][paper] ![image][deep learning] ![image][quantization]
1. [M6-10T: a sharing-delinking paradigm for efficient multi-trillion parameter pretraining](https://arxiv.org/abs/2110.03888)  
   _Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang_  
   ![image][paper] ![image][deep learning] ![image][distributed training] ![image][green ai] ![image][large models] ![image][mixture of experts] ![image][multi-modal] ![image][transformers]
1. Machine learning  
   _Tom M. Mitchell_  
   ![image][book]
1. Machine learning: a probabilistic perspective  
   _Kevin P. Murphy_  
   ![image][book]
1. [Making deep learning go brrrr from first principles](https://horace.io/brrr_intro.html)  
   _Horace He_  
   ![image][blog] ![image][deep learning] ![image][systems]
1. [Making DeepSpeed ZeRO run efficiently on more-affordable hardware](https://www.amazon.science/blog/making-deepspeed-zero-run-efficiently-on-more-affordable-hardware)  
   _Justin Chiu, Shuai Zheng_  
   ![image][blog] ![image][amazon] ![image][deep learning] ![image][distributed training] ![image][large models]
1. [Mask & focus: conversation modelling by learning concepts](https://arxiv.org/abs/2003.04976)  
   _Gaurav Pandey, Dinesh Raghu, Sachindra Joshi_  
   ![image][paper] ![image][aaai 2020] ![image][dialog] ![image][nlp]
1. [Maximizing communication efficiency for large-scale training via 0/1 Adam](https://arxiv.org/abs/2202.06009)  
   _Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He_  
   ![image][paper] ![image][deep learning] ![image][distributed training]
1. [Megatron-LM: training multi-billion parameter language models using model parallelism](https://arxiv.org/abs/1909.08053)  
   _Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro_  
   ![image][paper] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][systems] ![image][transformers]
1. [Memory-efficient pipeline-parallel DNN training](https://arxiv.org/abs/2006.09503)  
   _Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, Matei Zaharia_  
   ![image][paper] ![image][icml 2021] ![image][distributed training] ![image][large models] ![image][nlp] ![image][systems] ![image][transformers]
1. [MinTL: minimalist transfer learning for task-oriented dialogue systems](https://arxiv.org/abs/2009.12005)  
   _Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung_  
   ![image][paper] ![image][emnlp 2020] ![image][dialog] ![image][nlp]
1. [Mix and match: learning-free controllable text generation using energy language models](https://arxiv.org/abs/2203.13299)  
   _Fatemehsadat Mireshghallah, Kartik Goyal, Taylor Berg-Kirkpatrick_  
   ![image][paper] ![image][acl 2022] ![image][energy-based models] ![image][nlp]
1. [Mixed precision training](https://arxiv.org/abs/1710.03740)  
   _Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu_  
   ![image][paper] ![image][iclr 2018] ![image][deep learning] ![image][large models]
1. [mixup: beyond empirical risk minimization](https://arxiv.org/abs/1710.09412v1)  
   _Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial examples] ![image][adversarial learning] ![image][deep learning] ![image][empirical risk minimization] ![image][generative models]
1. [MMCoQA: conversational question answering over text, tables and images](https://aclanthology.org/2022.acl-long.290/)  
   _Yongqi Li, Wenjie Li, Liqiang Nie_  
   ![image][paper] ![image][acl 2022] ![image][dataset] ![image][dialog] ![image][nlp] ![image][question answering]
1. [Mode matching in GANs through latent space learning and inversion](https://arxiv.org/abs/1811.03692)  
   _Deepak Mishra, Prathosh A. P., Aravind Jayendran, Varun Srivastava, Santanu Chaudhury_  
   ![image][paper] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Multi-level memory for task oriented dialogs](https://arxiv.org/abs/1810.10647)  
   _Revanth Reddy, Danish Contractor, Dinesh Raghu, Sachindra Joshi_  
   ![image][paper] ![image][naacl 2019] ![image][dialog] ![image][nlp]
1. [Multitask prompt tuning enables parameter-efficient transfer learning](https://openreview.net/forum?id=Nk2pDtuhTq)  
   _Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim_  
   ![image][paper] ![image][iclr 2023] ![image][efficient finetuning] ![image][nlp]
1. [MultiWOZ - A large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling](https://arxiv.org/abs/1810.00278)  
   _Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, Milica Gašić_  
   ![image][paper] ![image][emnlp 2018] ![image][dataset] ![image][dialog] ![image][nlp]
1. [Mutual information neural estimation](https://arxiv.org/abs/1801.04062)  
   _Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, R Devon Hjelm_  
   ![image][paper] ![image][icml 2018] ![image][deep learning] ![image][information theory]
1. [NeMo: a toolkit for building AI applications using neural modules](https://arxiv.org/abs/1909.09577)  
   _Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, Patrice Castonguay, Mariya Popova, Jocelyn Huang, Jonathan M. Cohen_  
   ![image][paper] ![image][deep learning] ![image][distributed training] ![image][large models]
1. [Neural GPUs learn algorithms](https://arxiv.org/abs/1511.08228)  
   _Łukasz Kaiser, Ilya Sutskever_  
   ![image][paper] ![image][deep learning] ![image][theory of computation]
1. Neural network methods for natural language processing  
   _Yaov Goldberg_  
   ![image][book]
1. [Neural networks and physical systems with emergent collective computational abilities](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/)  
   _J. J. Hopfield_  
   ![image][paper] ![image][pnas 1982] ![image][biology] ![image][deep learning] ![image][energy-based models]
1. Neural networks for pattern recognition  
   _Christopher M. Bishop_  
   ![image][book]
1. [Neural ordinary differential equations](https://arxiv.org/abs/1806.07366)  
   _Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud_  
   ![image][paper] ![image][neurips 2018] ![image][deep learning] ![image][differential equations]
1. [Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples](https://arxiv.org/abs/1802.00420)  
   _Anish Athalye, Nicholas Carlini, David Wagner_  
   ![image][paper] ![image][icml 2018] ![image][adversarial examples] ![image][deep learning]
1. [On the convergence of Adam and beyond](https://arxiv.org/abs/1904.09237)  
   _Sashank J. Reddi, Satyen Kale, Sanjiv Kumar_  
   ![image][paper] ![image][iclr 2018] ![image][convergence] ![image][deep learning] ![image][optimization]
1. [On the power of neural networks for solving hard problems](https://papers.nips.cc/paper/70-on-the-power-of-neural-networks-for-solving-hard-problems)  
   _Jehoshua Bruck, Joseph W. Goodman_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning] ![image][np-hard]
1. [One model to learn them all](https://arxiv.org/abs/1706.05137)  
   _Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, Jakob Uszkoreit_  
   ![image][paper] ![image][deep learning] ![image][multi-modal]
1. [Open domain question answering over tables via dense retrieval](https://arxiv.org/abs/2103.12011)  
   _Jonathan Herzig, Thomas Müller, Syrine Krichene, Julian Eisenschlos_  
   ![image][paper] ![image][naacl 2021] ![image][nlp] ![image][question answering]
1. [Open question answering over tables and text](https://openreview.net/forum?id=MmCRswl1UYl)  
   _Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Yang Wang, William W. Cohen_  
   ![image][paper] ![image][iclr 2021] ![image][nlp] ![image][question answering]
1. [OPT: open pre-trained transformer language models](https://arxiv.org/abs/2205.01068)  
   _Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer_  
   ![image][paper] ![image][large models] ![image][nlp] ![image][transformers]
1. [Optimal perceptual inference](https://www.researchgate.net/publication/260869405_Optimal_perceptual_inference)  
   _Geoffrey E. Hinton, Terrence J. Sejnowski_  
   ![image][paper] ![image][cvpr 1983] ![image][deep learning] ![image][energy-based models]
1. [Optimization story: Bloom inference](https://huggingface.co/blog/bloom-inference-optimization)  
   _Nicolas Patry_  
   ![image][blog] ![image][huggingface] ![image][deep learning] ![image][large models] ![image][transformers]
1. [Outer product-based neural collaborative filtering](https://arxiv.org/abs/1808.03912)  
   _Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang, Tat-Seng Chua_  
   ![image][paper] ![image][ijcai 2018] ![image][collaborative filtering] ![image][deep learning] ![image][recommender systems]
1. [PAL: Program-aided language models](https://arxiv.org/abs/2211.10435)  
   _Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig_  
   ![image][paper] ![image][mathematical reasoning] ![image][nlp]
1. [PaLM: scaling language modeling with pathways](https://arxiv.org/abs/2204.02311)  
   _Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel_  
   ![image][paper] ![image][distributed training] ![image][large models] ![image][nlp] ![image][transformers]
1. Pattern classification  
   _Richard O. Duda, Peter E. Hart, David G. Stork_  
   ![image][book]
1. Pattern recognition and machine learning  
   _Christopher M. Bishop_  
   ![image][book]
1. [Perceptual losses for real-time style transfer and super-resolution](https://arxiv.org/abs/1603.08155)  
   _Justin Johnson, Alexandre Alahi, Li Fei-Fei_  
   ![image][paper] ![image][eccv 2016] ![image][computer vision] ![image][image super resolution]
1. [Personalizing dialogue agents: I have a dog, do you have pets too?](https://arxiv.org/abs/1801.07243)  
   _Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston_  
   ![image][paper] ![image][acl 2018] ![image][dialog] ![image][nlp]
1. [Phase-functioned neural networks for character control](https://dl.acm.org/citation.cfm?id=3073663)  
   _Daniel Holden, Taku Komura, Jun Saito_  
   ![image][paper] ![image][acm transactions on graphics] ![image][computer graphics] ![image][deep learning]
1. [Playing Atari with deep reinforcement learning](https://arxiv.org/abs/1312.5602)  
   _Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller_  
   ![image][paper] ![image][reinforcement learning]
1. [Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing](https://arxiv.org/abs/2107.13586)  
   _Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig_  
   ![image][paper] ![image][efficient finetuning] ![image][few shot] ![image][nlp] ![image][prompting]
1. [Prefix-tuning: optimizing continuous prompts for generation](https://arxiv.org/abs/2101.00190)  
   _Xiang Lisa Li, Percy Liang_  
   ![image][paper] ![image][acl 2021] ![image][efficient finetuning] ![image][few shot] ![image][nlp]
1. [Probabilistic latent semantic analysis](https://arxiv.org/abs/1301.6705)  
   _Thomas Hofmann_  
   ![image][paper] ![image][information retrieval] ![image][nlp]
1. [Progressive growing of GANs from improved quality, stability and variation](https://arxiv.org/abs/1710.10196)  
   _Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [PullNet: open domain question answering with iterative retrieval on knowledge bases and text](https://arxiv.org/abs/1904.09537)  
   _Haitian Sun, Tania Bedrax-Weiss, William Cohen_  
   ![image][paper] ![image][emnlp 2019] ![image][nlp] ![image][question answering]
1. [Q-BERT: Hessian based ultra low precision quantization of BERT](https://arxiv.org/abs/1909.05840)  
   _Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer_  
   ![image][paper] ![image][aaai 2020] ![image][nlp] ![image][quantization]
1. [R<sup>3</sup>Net: recurrent residual refinement network for saliency detection](https://www.ijcai.org/proceedings/2018/95)  
   _Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guoqiang Han, Pheng-Ann Heng_  
   ![image][paper] ![image][ijcai 2018] ![image][computer vision] ![image][saliency detection]
1. [Reading Wikipedia to answer open-domain questions](https://arxiv.org/abs/1704.00051)  
   _Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes_  
   ![image][paper] ![image][acl 2017] ![image][nlp] ![image][question answering]
1. [REALM: Retrieval-augmented language model pretraining](https://arxiv.org/abs/2002.08909)  
   _Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang_  
   ![image][paper] ![image][icml 2020] ![image][information retrieval] ![image][nlp] ![image][retrieval-augmented generation]
1. [Recurrent models of visual attention](https://arxiv.org/abs/1406.6247)  
   _Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu_  
   ![image][paper]
1. [Reducing activation recomputation in large transformer models](https://arxiv.org/abs/2205.05198)  
   _Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, Bryan Catanzaro_  
   ![image][paper] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][transformers]
1. [Regularizing and optimizing LSTM language models](https://arxiv.org/abs/1708.02182)  
   _Stephen Merity, Nitish Shirish Keskar, Richard Socher_  
   ![image][paper] ![image][iclr 2018] ![image][deep learning] ![image][optimization] ![image][regularization]
1. Reinforcement Learning: An Introduction  
   _Richard S. Sutton, Andrew G. Barto_  
   ![image][book]
1. [Restricted Boltzmann machines for collaborative filtering](https://dl.acm.org/citation.cfm?doid=1273496.1273596)  
   _Ruslan Salakhutdinov, Andriy Mnih, Geoffrey Hinton_  
   ![image][paper] ![image][icml 2007] ![image][boltzmann machines] ![image][collaborative filtering] ![image][deep learning] ![image][energy-based models] ![image][recommender systems]
1. [Retrieval augmentation reduces hallucination in conversation](https://arxiv.org/abs/2104.07567)  
   _Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston_  
   ![image][paper] ![image][emnlp 2021] ![image][dialog] ![image][hallucination] ![image][nlp] ![image][retrieval-augmented generation]
1. [Retrieval-augmented generation for knowledge-intensive NLP tasks](https://arxiv.org/abs/2005.11401)  
   _Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela_  
   ![image][paper] ![image][neurips 2020] ![image][hallucination] ![image][information retrieval] ![image][nlp] ![image][retrieval-augmented generation]
1. [Revisiting classifier two-sample tests](https://arxiv.org/abs/1610.06545)  
   _David Lopez-Paz, Maxime Oquab_  
   ![image][paper] ![image][iclr 2017] ![image][deep learning] ![image][unsupervised learning]
1. [RoBERTa: a robustly optimized BERT pretraining approach](https://arxiv.org/abs/1907.11692)  
   _Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov_  
   ![image][paper] ![image][nlp] ![image][transformers]
1. [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)  
   _Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra_  
   ![image][paper] ![image][ai for code] ![image][distributed training] ![image][large models] ![image][transformers]
1. [Scaling instruction-finetuned language models](https://arxiv.org/abs/2210.11416)  
   _Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei_  
   ![image][paper] ![image][instruction finetuning] ![image][nlp]
1. [Scaling PyTorch FSDP for training foundation Models on IBM cloud](https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud/)  
   _Linsong Chu, Less Wright, Hamid Shojanazeri, Sophia Wen, Raghu Ganti, Geeta Chauhan_  
   ![image][blog] ![image][pytorch] ![image][distributed training] ![image][large models] ![image][systems]
1. [Self-instruct: aligning language model with self generated instructions](https://arxiv.org/abs/2212.10560)  
   _Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi_  
   ![image][paper] ![image][instruction finetuning] ![image][nlp]
1. [Self-normalizing neural networks](https://arxiv.org/abs/1706.02515)  
   _Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter_  
   ![image][paper] ![image][neurips 2017] ![image][activation function] ![image][deep learning] ![image][normalization]
1. [Semantically equivalent adversarial rules for debugging NLP models](https://aclanthology.org/P18-1079/)  
   _Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin_  
   ![image][paper] ![image][acl 2018] ![image][nlp]
1. [Sequential latent knowledge selection for knowledge-grounded dialogue](https://arxiv.org/abs/2002.07510)  
   _Byeongchang Kim, Jaewoo Ahn, Gunhee Kim_  
   ![image][paper] ![image][iclr 2020] ![image][dialog] ![image][nlp] ![image][variational inference]
1. [Simple and effective multi-paragraph reading comprehension](https://arxiv.org/abs/1710.10723)  
   _Christopher Clark, Matt Gardner_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][reading comprehension]
1. [SmoothQuant: accurate and efficient post-training quantization for large language models](https://arxiv.org/abs/2211.10438)  
   _Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han_  
   ![image][paper] ![image][deep learning] ![image][large models] ![image][quantization] ![image][transformers]
1. [Soft filter pruning for accelerating deep convolutional neural networks](https://arxiv.org/abs/1808.06866)  
   _Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang_  
   ![image][paper] ![image][ijcai 2018] ![image][deep learning] ![image][pruning]
1. [SOLOIST: building task bots at scale with transfer learning and machine teaching](https://arxiv.org/abs/2005.05298)  
   _Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Jianfeng Gao_  
   ![image][paper] ![image][dialog] ![image][few shot] ![image][nlp] ![image][transfer learning]
1. [Solving quantitative reasoning problems with language models](https://arxiv.org/abs/2206.14858)  
   _Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra_  
   ![image][paper] ![image][large models] ![image][nlp] ![image][transformers]
1. [Spatial temporal graph convolutional networks for skeleton-based action recognition](https://arxiv.org/abs/1801.07455)  
   _Sijie Yan, Yuanjun Xiong, Dahua Lin_  
   ![image][paper] ![image][aaai 2018] ![image][deep learning] ![image][graph neural networks] ![image][spatio-temporal]
1. [Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting](https://arxiv.org/abs/1709.04875)  
   _Bing Yu, Haoteng Yin, Zhanxing Zhu_  
   ![image][paper] ![image][ijcai 2018] ![image][deep learning] ![image][graph neural networks] ![image][spatio-temporal] ![image][time series]
1. [Spectral normalization for generative adversarial networks](https://arxiv.org/abs/1802.05957)  
   _Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial learning] ![image][deep learning]
1. Speech and language processing  
   _Daniel Jurafsky, James H. Martin_  
   ![image][book]
1. [Sticking the landing: simple, lower-variance gradient estimators for variational inference](https://arxiv.org/abs/1703.09194)  
   _Geoffrey Roeder, Yuhuai Wu, David K. Duvenaud_  
   ![image][paper] ![image][neurips 2017] ![image][deep learning] ![image][gradient estimation] ![image][variational inference]
1. [Stochastic hyperparameter optimization through hypernetworks](https://arxiv.org/abs/1802.09419)  
   _Jonathan Lorraine, David Duvenaud_  
   ![image][paper] ![image][neurips 2018] ![image][deep learning] ![image][meta learning]
1. [Strategies for teaching layered networks classification tasks](https://papers.nips.cc/paper/85-strategies-for-teaching-layered-networks-classification-tasks)  
   _Ben S. Wittner, John S. Denker_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning]
1. [Style transfer from non-parallel text by cross-alignment](https://arxiv.org/abs/1705.09655)  
   _Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola_  
   ![image][paper] ![image][neurips 2017] ![image][nlp] ![image][style transfer]
1. [Subword regularization: improving neural network translation models with multiple subword candidates](https://arxiv.org/abs/1804.10959)  
   _Taku Kudo_  
   ![image][paper] ![image][acl 2018] ![image][machine translation] ![image][nlp] ![image][regularization]
1. [Supervised learning of probability distributions by neural networks](https://papers.nips.cc/paper/3-supervised-learning-of-probability-distributions-by-neural-networks)  
   _Eric B. Baum, Frank Wilczek_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning]
1. [Supporting efficient large model training on AMD Instinct<sup>TM</sup> GPUs with DeepSpeed](https://cloudblogs.microsoft.com/opensource/2022/03/21/supporting-efficient-large-model-training-on-amd-instinct-gpus-with-deepspeed/)  
   _Olatunji Ruwase, Jeff Rasley_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][distributed training] ![image][large models]
1. [Synchronization in neural nets](https://papers.nips.cc/paper/32-synchronization-in-neural-nets)  
   _Jacques J. Vidal, John Haggerty_  
   ![image][paper] ![image][neurips 1987] ![image][deep learning]
1. [Tackling the poor assumptions of Naive Bayes text classifiers](https://dl.acm.org/citation.cfm?id=3041916)  
   _Jason D. M. Rennie, Lawrence Shih, Jaime Teevan, David R. Karger_  
   ![image][paper] ![image][icml 2003] ![image][nlp]
1. [The best of both worlds: combining recent advances in neural machine translation](https://arxiv.org/abs/1804.09849)  
   _Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, Macduff Hughes_  
   ![image][paper] ![image][acl 2018] ![image][machine translation] ![image][nlp]
1. The elements of statistical learning: data mining, inference and prediction  
   _Trevor Hastie, Robert Tibshirani, Jerome Friedman_  
   ![image][book]
1. [The Flan collection: designing data and methods for effective instruction tuning](https://arxiv.org/abs/2301.13688)  
   _Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, Adam Roberts_  
   ![image][paper] ![image][instruction finetuning] ![image][nlp]
1. [The information bottleneck method](https://arxiv.org/abs/physics/0004057)  
   _Naftali Tishby, Fernando C. Pereira, William Bialek_  
   ![image][paper] ![image][information theory]
1. [The Pile: an 800GB dataset of diverse text for language modeling](https://arxiv.org/abs/2101.00027)  
   _Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy_  
   ![image][paper] ![image][dataset] ![image][nlp]
1. [The power of scale for parameter-efficient prompt tuning](https://arxiv.org/abs/2104.08691)  
   _Brian Lester, Rami Al-Rfou, Noah Constant_  
   ![image][paper] ![image][emnlp 2021] ![image][efficient finetuning] ![image][nlp]
1. [The wisdom of hindsight makes language models better instruction followers](https://arxiv.org/abs/2302.05206)  
   _Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, Joseph E. Gonzalez_  
   ![image][paper] ![image][instruction finetuning] ![image][nlp]
1. [Thermometer encoding: one hot way to resist adversarial examples](https://openreview.net/forum?id=S18Su--CW)  
   _Jacob Buckman, Aurko Roy, Colin Raffel, Ian Goodfellow_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial examples] ![image][deep learning] ![image][robustness]
1. [To regularize or not to regularize? The bias variance trade-off in regularized AEs](https://arxiv.org/abs/2006.05838)  
   _Arnab Kumar Mondal, Himanshu Asnani, Parag Singla, Prathosh AP_  
   ![image][paper] ![image][deep learning] ![image][regularization]
1. [Towards crowdsourced training of large neural networks using decentralized mixture-of-experts](https://arxiv.org/abs/2002.04013)  
   _Max Ryabinin, Anton Gusev_  
   ![image][paper] ![image][neurips 2020] ![image][decentralized training] ![image][deep learning] ![image][large models]
1. [Towards deep learning models resilient to adversarial attacks](https://arxiv.org/abs/1706.06083)  
   _Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu_  
   ![image][paper] ![image][iclr 2018] ![image][adversarial examples] ![image][deep learning] ![image][robustness]
1. [Towards evaluating the robustness of neural networks](https://arxiv.org/abs/1608.04644)  
   _Nicholas Carlini, David Wagner_  
   ![image][paper] ![image][ieee symposium on security and privacy 2017] ![image][adversarial examples] ![image][deep learning] ![image][robustness]
1. [Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/abs/2108.12409)  
   _Ofir Press, Noah Smith, Mike Lewis_  
   ![image][paper] ![image][iclr 2022] ![image][embeddings] ![image][nlp] ![image][transformers]
1. [Training compute-optimal large language models](https://arxiv.org/abs/2203.15556)  
   _Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre_  
   ![image][paper] ![image][neurips 2022] ![image][deep learning] ![image][large models] ![image][transformers]
1. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)  
   _Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe_  
   ![image][paper] ![image][neurips 2022] ![image][instruction finetuning] ![image][nlp] ![image][reinforcement learning]
1. [Transformer memory as a differentiable search index](https://arxiv.org/abs/2202.06991)  
   _Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler_  
   ![image][paper] ![image][information retrieval] ![image][nlp] ![image][retrieval-augmented generation]
1. [Transformer quality in linear time](https://arxiv.org/abs/2202.10447)  
   _Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc Le_  
   ![image][paper] ![image][icml 2022] ![image][deep learning] ![image][transformers]
1. [Transformers explained visually (part 1): overview of functionality](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)  
   _Ketan Doshi_  
   ![image][blog] ![image][towards data science] ![image][deep learning] ![image][transformers]
1. [Transformers explained visually (part 2): how it works, step-by-step](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)  
   _Ketan Doshi_  
   ![image][blog] ![image][towards data science] ![image][deep learning] ![image][transformers]
1. [Transformers explained visually (part 3): multi-head attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)  
   _Ketan Doshi_  
   ![image][blog] ![image][towards data science] ![image][deep learning] ![image][transformers]
1. [Turing-NLG: a 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)  
   _Corby Rosset_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][large models] ![image][transformers]
1. [UL2: unifying language learning paradigms](https://arxiv.org/abs/2205.05131)  
   _Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler_  
   ![image][paper] ![image][large models] ![image][nlp] ![image][transformers]
1. [Understanding convolutional neural networks with a mathematical model](https://arxiv.org/abs/1609.04112)  
   _C.-C. Jay Kuo_  
   ![image][paper] ![image][journal of visual communication and image representation 2016] ![image][computer vision]
1. [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)  
   _Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, Alexander Lerchner_  
   ![image][paper] ![image][deep learning] ![image][disentanglement] ![image][variational inference]
1. [Understanding the Open Pre-Trained Transformers (OPT) library](https://towardsdatascience.com/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)  
   _Cameron Wolfe_  
   ![image][blog] ![image][meta] ![image][deep learning] ![image][transformers]
1. [Unit tests for stochastic optimization](https://arxiv.org/abs/1312.6055)  
   _Tom Schaul, Ioannis Antonoglou, David Silver_  
   ![image][paper] ![image][iclr 2014] ![image][deep learning] ![image][optimization]
1. [Universal language model fine-tuning for text classification](https://arxiv.org/abs/1801.06146)  
   _Jeremy Howard, Sebastian Ruder_  
   ![image][paper] ![image][acl 2018] ![image][nlp] ![image][text classification]
1. [Unpaired image-to-image translation using cycle-consistent adversarial networks](https://arxiv.org/abs/1703.10593)  
   _Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros_  
   ![image][paper] ![image][iccv 2017] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Unsupervised machine translation using monolingual corpora only](https://arxiv.org/abs/1711.00043)  
   _Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato_  
   ![image][paper] ![image][acl 2019] ![image][machine translation] ![image][nlp] ![image][unsupervised learning]
1. [Unsupervised representation learning by predicting image rotations](https://arxiv.org/abs/1803.07728)  
   _Spyros Gidaris, Praveer Singh, Nikos Komodakis_  
   ![image][paper] ![image][iclr 2018] ![image][computer vision] ![image][unsupervised learning]
1. [Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, the world’s largest and most powerful generative language model](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)  
   _Ali Alvi, Paresh Kharya_  
   ![image][blog] ![image][microsoft] ![image][distributed training] ![image][large models] ![image][nlp] ![image][transformers]
1. [Variational inference using implicit distributions](https://arxiv.org/abs/1702.08235)  
   _Ferenc Huszár_  
   ![image][paper] ![image][deep learning] ![image][variational inference]
1. [Variational inference with latent space quantization for adversarial resilience](https://arxiv.org/abs/1903.09940)  
   _Vinay Kyatham, Mayank Mishra, Tarun Kumar Yadav, Deepak Mishra, Prathosh AP_  
   ![image][paper] ![image][deep learning] ![image][quantization] ![image][robustness] ![image][variational inference]
1. [Variational learning for unsupervised knowledge grounded dialogs](https://arxiv.org/abs/2112.00653)  
   _Mayank Mishra, Dhiraj Madan, Gaurav Pandey, Danish Contractor_  
   ![image][paper] ![image][ijcai 2022] ![image][dialog] ![image][nlp] ![image][variational inference]
1. [Variational lossy autoencoder](https://arxiv.org/abs/1611.02731)  
   _Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel_  
   ![image][paper] ![image][iclr 2017] ![image][deep learning] ![image][variational inference]
1. [Vector-quantized input-contextualized soft prompts for natural language understanding](https://arxiv.org/abs/2205.11024)  
   _Rishabh Bhardwaj, Amrita Saha, Steven C.H. Hoi, Soujanya Poria_  
   ![image][paper] ![image][emnlp 2022] ![image][efficient finetuning] ![image][nlp]
1. [VEEGAN: reducing mode collapse in GANs using implicit variational learning](https://arxiv.org/abs/1705.07761)  
   _Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, Charles Sutton_  
   ![image][paper] ![image][neurips 2017] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556)  
   _Karen Simonyan, Andrew Zisserman_  
   ![image][paper] ![image][iclr 2015] ![image][computer vision] ![image][image classification]
1. [Visualizing data using t-SNE](http://www.jmlr.org/papers/v9/vandermaaten08a.html)  
   _Laurens van der Maaten, Geoffrey Hinton_  
   ![image][paper] ![image][jmlr 2008] ![image][data visualization] ![image][deep learning]
1. [Wasserstein GAN](https://arxiv.org/abs/1701.07875)  
   _Martin Arjovsky, Soumith Chintala, Léon Bottou_  
   ![image][paper] ![image][icml 2017] ![image][adversarial learning] ![image][deep learning] ![image][generative models]
1. [wav2vec 2.0: a framework for self-supervised learning of speech representations](https://arxiv.org/abs/2006.11477)  
   _Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli_  
   ![image][paper] ![image][neurips 2020] ![image][deep learning] ![image][speech] ![image][transformers]
1. [Wavenet: a generative model for raw audio](https://arxiv.org/abs/1609.03499)  
   _Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu_  
   ![image][paper] ![image][audio] ![image][deep learning] ![image][generative models]
1. [WebGPT: browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)  
   _Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman_  
   ![image][paper] ![image][human feedback] ![image][reinforcement learning] ![image][transformers]
1. [What language model to train if you have one million GPU hours?](https://openreview.net/forum?id=rI7BL3fHIZq)  
   _Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, Iz Beltagy_  
   ![image][paper] ![image][acl 2022] ![image][distributed training] ![image][large models] ![image][nlp] ![image][transformers]
1. [Word translation without parallel data](https://openreview.net/forum?id=H196sainb)  
   _Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, Hervé Jégou_  
   ![image][paper] ![image][iclr 2018] ![image][machine translation] ![image][nlp]
1. [Yandex publishes YaLM 100B. It’s the largest GPT-like neural network in open source](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6)  
   _Mikhail Khrushchev_  
   ![image][blog] ![image][yandex] ![image][large models] ![image][nlp] ![image][transformers]
1. [You only look once: unified, real-time object detection](https://arxiv.org/abs/1506.02640)  
   _Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi_  
   ![image][paper] ![image][cvpr 2016] ![image][computer vision] ![image][object detection]
1. [ZeRO & DeepSpeed: new system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)  
   _DeepSpeed Team, Rangan Majumder, Junhua Wang_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][distributed training] ![image][large models]
1. [ZeRO-2 & DeepSpeed: shattering barriers of deep learning speed & scale](https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/)  
   _DeepSpeed Team, Rangan Majumder, Junhua Wang_  
   ![image][blog] ![image][microsoft] ![image][deep learning] ![image][distributed training] ![image][large models]
1. [ZeRO-Infinity: breaking the GPU memory wall for extreme scale deep learning](https://arxiv.org/abs/2104.07857)  
   _Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He_  
   ![image][paper] ![image][sc 2021] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][transformers]
1. [Zero-shot text-to-image generation](https://arxiv.org/abs/2102.12092)  
   _Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever_  
   ![image][paper] ![image][icml 2021] ![image][deep learning] ![image][generative models] ![image][variational inference] ![image][zero shot]
1. [ZeRO: memory optimizations toward training trillion parameter models](https://arxiv.org/abs/1910.02054)  
   _Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He_  
   ![image][paper] ![image][sc 2020] ![image][deep learning] ![image][distributed training] ![image][large models] ![image][transformers]
1. [ZeroQuant: efficient and affordable post-training quantization for large-scale transformers](https://arxiv.org/abs/2206.01861)  
   _Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He_  
   ![image][paper] ![image][deep learning] ![image][quantization]
1. [β-VAE: learning basic visual concepts with a constrained variational framework](https://openreview.net/forum?id=Sy2fzU9gl)  
   _Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner_  
   ![image][paper] ![image][iclr 2017] ![image][deep learning] ![image][variational inference]

# Calculus

1. Calculus of variations  
   _I. M. Gelfand, S. V. Fomin_  
   ![image][book]
1. Thomas' calculus  
   _George B. Thomas Jr., Maurice D. Weir_  
   ![image][book]

# Computer architecture

1. Computer architecture: a quantitative approach  
   _John L. Hennessy, David A. Patterson_  
   ![image][book]
1. Computer organization and design ARM edition: the hardware software interface  
   _David A. Patterson, John L. Hennessy_  
   ![image][book]
1. [Flipping bits in memory without accessing them: an experimental study of DRAM disturbance errors](https://ieeexplore.ieee.org/document/6853210)  
   _Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, Donghyuk Lee, Chris Wilkerson, Konrad Lai, Onur Mutlu_  
   ![image][paper] ![image][ieee isca 2014] ![image][computer architecture] ![image][memory] ![image][security]
1. [Improving DRAM performance by parallelizing refreshes with accesses](https://ieeexplore.ieee.org/document/6835946)  
   _Kevin Kai-Wei Chang, Donghyuk Lee, Zeshan Chishti, Alaa R. Alameldeen, Chris Wilkerson, Yoongu Kim, Onur Mutlu_  
   ![image][paper] ![image][ieee hpca 2014] ![image][computer architecture] ![image][memory] ![image][security] ![image][systems]
1. [Memory performance attacks: denial of memory service in multi-core systems](https://www.usenix.org/conference/16th-usenix-security-symposium/memory-performance-attacks-denial-memory-service-multi)  
   _Thomas Moscibroda, Onur Mutlu_  
   ![image][paper] ![image][usenix security symposium 2007] ![image][computer architecture] ![image][memory] ![image][security]
1. [Memory scaling: a systems architecture perspective](https://ieeexplore.ieee.org/document/6582088)  
   _Onur Mutlu_  
   ![image][paper] ![image][ieee international memory worksop 2013] ![image][computer architecture] ![image][memory]
1. [Millicode in an IBM zSeries processor](https://ieeexplore.ieee.org/document/5388884)  
   _L. C. Heller, M. S. Farrell_  
   ![image][paper] ![image][ibm journal of research and development 2004] ![image][computer architecture]
1. [RAIDR: Retention-Aware Intelligent DRAM Refresh](https://dl.acm.org/doi/10.5555/2337159.2337161)  
   _Jamie Liu, Ben Jaiyen, Richard Veras, Onur Mutlu_  
   ![image][paper] ![image][ieee isca 2012] ![image][computer architecture] ![image][memory]
1. [Stall-time fair memory access scheduling for chip multiprocessors](https://ieeexplore.ieee.org/document/4408252)  
   _Onur Mutlu, Thomas Moscibroda_  
   ![image][paper] ![image][ieee micro 2007] ![image][computer architecture] ![image][memory]

# Computer graphics

1. [Principles of traditional animation applied to 3D computer animation](https://dl.acm.org/doi/10.1145/37402.37407)  
   _John Lasseter_  
   ![image][paper] ![image][acm siggraph computer graphics 1987] ![image][animation] ![image][computer graphics]

# Data structures and algorithms

1. Data structures and algorithms in Java  
   _Michael T. Goodrich, Roberto Tamassia, Michael H. Goldwasser_  
   ![image][book]
1. Introduction to algorithms  
   _Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein_  
   ![image][book]

# Digital electronics

1. Digital design: with an introduction to the Verilog HDL  
   _M. Morris Mano, Michael D. Ciletti_  
   ![image][book]

# Graph theory

1. Introduction to graph theory  
   _Robin Wilson_  
   ![image][book]

# Information theory

1. Elements of information theory  
   _Thomas M. Cover, Joy A. Thomas_  
   ![image][book]
1. [Error detecting and error correcting codes](https://ieeexplore.ieee.org/document/6772729)  
   _R. W. Hamming_  
   ![image][paper] ![image][the bell system technical journal 1950] ![image][error correction] ![image][error detection] ![image][information theory]

# Linear algebra

1. Linear algebra and its applications  
   _Gilbert Strang_  
   ![image][book]
1. Matrix analysis and applied linear algebra  
   _Carl D. Meyer_  
   ![image][book]
1. The matrix cookbook  
   _Kaare Brandt Petersen, Michael Syskind Pedersen_  
   ![image][book]

# Measure theory

1. Measure theory  
   _Donald L. Cohn_  
   ![image][book]

# Optimization theory

1. Convex Optimization  
   _Stephen Boyd, Lieven Vandenberghe_  
   ![image][book]
1. [Distributed optimization and statistical learning via the alternating direction method of multipliers](https://ieeexplore.ieee.org/document/8186925)  
   _Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein_  
   ![image][book]

# Probability and stochastic processes

1. Introduction to probability and stochastic processes with applications  
   _Liliana Blanco Castaneda, Viswanathan Arunachalam, Delvamuthu Dharmaraja_  
   ![image][book]

# Quantum computing

1. [A fast quantum mechanical algorithm for database search](https://arxiv.org/abs/quant-ph/9605043)  
   _Lov K. Grover_  
   ![image][paper] ![image][stoc 1996] ![image][quantum algorithms] ![image][quantum computing]
1. [A single quantum cannot be cloned](https://www.nature.com/articles/299802a0)  
   _W. K. Wootters, W. H. Zurek_  
   ![image][paper] ![image][nature 1982] ![image][quantum computing]
1. [Can quantum-mechanical description of physical reality be considered complete](https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777)  
   _Albert Einstein, Boris Podolsky, Nathan Rosen_  
   ![image][paper] ![image][physical review journals 1935] ![image][quantum computing]
1. [Image recognition with an adiabatic quantum computer I. mapping to quadratic unconstrained binary optimization](https://arxiv.org/abs/0804.4457)  
   _Hartmut Neven, Geordie Rose, William G. Macready_  
   ![image][paper] ![image][image classification] ![image][qubo] ![image][quantum computing]
1. [Integer optimization toolbox (minimizing polynomials over integer lattices using quantum annealing)](https://1qbit.com/whitepaper/integer-optimization-toolbox/)  
   _Pooya Ronagh_  
   ![image][whitepaper]
1. [Limits on parallel speedup for classical Ising model solvers](https://www.dwavesys.com/resources/white-paper/limits-on-parallel-speedup-for-classical-ising-model-solvers/)  
   ![image][whitepaper]
1. [Partitioning optimization problems for hybrid classical/quantum execution](https://docs.ocean.dwavesys.com/projects/qbsolv/en/latest/_downloads/bd15a2d8f32e587e9e5997ce9d5512cc/qbsolv_techReport.pdf)  
   _Michael Booth, Steven P. Reinhardt, Aidan Roy_  
   ![image][whitepaper]
1. [Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer](https://arxiv.org/abs/quant-ph/9508027)  
   _Peter W. Shor_  
   ![image][paper] ![image][siam journal on computing 1997] ![image][quantum algorithms] ![image][quantum computing]
1. [Probabilistic cloning and identification of linearly independent quantum states](https://arxiv.org/abs/quant-ph/9804064)  
   _Lu-Ming Duan, Guang-Can Guo_  
   ![image][paper] ![image][physical review letters 1998] ![image][cloning] ![image][quantum computing]
1. [Programming with D-Wave: map coloring problem](https://www.dwavesys.com/resources/white-paper/programming-with-d-wave-map-coloring-problem/)  
   _E. D. Dahl_  
   ![image][whitepaper]
1. Quantum computation and quantum information  
   _Michael A. Nielsen, Isaac L. Chuang_  
   ![image][book]
1. Quantum computing: a gentle introduction  
   _Eleanor Rieffel, Wolfgang Polak_  
   ![image][book]
1. [Quantum performance evaluation: a short reading list](https://www.dwavesys.com/resources/white-paper/quantum-performance-evaluation-a-short-reading-list/)  
   ![image][whitepaper]
1. [Quantum theory, the Church-Turing principle and the universal quantum computer](https://royalsocietypublishing.org/doi/10.1098/rspa.1985.0070)  
   _David Deutsch_  
   ![image][paper] ![image][proceedings of the royal society 1985] ![image][quantum computing] ![image][theory of computation]
1. [Rapid solution of problems by quantum computation](https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1992.0167)  
   _David Deutsche, Richard Jozsa_  
   ![image][paper] ![image][proceedings of the royal society 1992] ![image][quantum algorithms] ![image][quantum computing]
1. [Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.70.1895)  
   _Charles H. Bennett, Gilles Brassard, Claude Crépeau, Richard Jozsa, Asher Peres, William K. Wootters_  
   ![image][paper] ![image][physical review journals 1993] ![image][quantum computing] ![image][quantum teleportation]

# Signal processing

1. Discrete-time signal processing  
   _Alan V. Oppenheim, Ronald W. Schafer_  
   ![image][book]
1. Foundations of Signal Processing  
   _Martin Vetterli, Jelena Kovačević, Vivek K Goyal_  
   ![image][book]
1. Signals and systems  
   _Alan V. Oppenheim_  
   ![image][book]
1. Understanding digital signal processing  
   _Richard G. Lyons_  
   ![image][book]
