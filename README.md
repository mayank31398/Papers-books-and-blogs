This repository contains a list of the books, blogs, research papers and white papers that I have read and found interesting.

[Book]: https://img.shields.io/static/v1?label=&message=Book&color=red
[Blog]: https://img.shields.io/static/v1?label=&message=Blog&color=brightgreen
[Paper]: https://img.shields.io/static/v1?label=&message=Paper&color=blueviolet
[Whitepaper]: https://img.shields.io/static/v1?label=&message=Whitepaper&color=yellow
[AI for code]: https://img.shields.io/static/v1?label=&message=AI%20for%20code&color=blue
[ASIC]: https://img.shields.io/static/v1?label=&message=ASIC&color=blue
[Activation Function]: https://img.shields.io/static/v1?label=&message=Activation%20Function&color=blue
[Adversarial Examples]: https://img.shields.io/static/v1?label=&message=Adversarial%20Examples&color=blue
[Adversarial Learning]: https://img.shields.io/static/v1?label=&message=Adversarial%20Learning&color=blue
[Algorithms]: https://img.shields.io/static/v1?label=&message=Algorithms&color=blue
[Animation]: https://img.shields.io/static/v1?label=&message=Animation&color=blue
[Approximate Inference]: https://img.shields.io/static/v1?label=&message=Approximate%20Inference&color=blue
[Attention Mechanism]: https://img.shields.io/static/v1?label=&message=Attention%20Mechanism&color=blue
[Audio]: https://img.shields.io/static/v1?label=&message=Audio&color=blue
[Backpropagation]: https://img.shields.io/static/v1?label=&message=Backpropagation&color=blue
[Behavior and Control]: https://img.shields.io/static/v1?label=&message=Behavior%20and%20Control&color=blue
[Biology]: https://img.shields.io/static/v1?label=&message=Biology&color=blue
[Boltzmann Machines]: https://img.shields.io/static/v1?label=&message=Boltzmann%20Machines&color=blue
[Catastrophic Forgetting]: https://img.shields.io/static/v1?label=&message=Catastrophic%20Forgetting&color=blue
[Cloning]: https://img.shields.io/static/v1?label=&message=Cloning&color=blue
[Collaborative Filtering]: https://img.shields.io/static/v1?label=&message=Collaborative%20Filtering&color=blue
[Complex Numbers]: https://img.shields.io/static/v1?label=&message=Complex%20Numbers&color=blue
[Compression]: https://img.shields.io/static/v1?label=&message=Compression&color=blue
[Computer Architecture]: https://img.shields.io/static/v1?label=&message=Computer%20Architecture&color=blue
[Computer Graphics]: https://img.shields.io/static/v1?label=&message=Computer%20Graphics&color=blue
[Computer Vision]: https://img.shields.io/static/v1?label=&message=Computer%20Vision&color=blue
[Constituency Parsing]: https://img.shields.io/static/v1?label=&message=Constituency%20Parsing&color=blue
[Convergence]: https://img.shields.io/static/v1?label=&message=Convergence&color=blue
[Curriculum Learning]: https://img.shields.io/static/v1?label=&message=Curriculum%20Learning&color=blue
[Data Curation]: https://img.shields.io/static/v1?label=&message=Data%20Curation&color=blue
[Data Mixtures]: https://img.shields.io/static/v1?label=&message=Data%20Mixtures&color=blue
[Data Visualization]: https://img.shields.io/static/v1?label=&message=Data%20Visualization&color=blue
[Dataflow Architecture]: https://img.shields.io/static/v1?label=&message=Dataflow%20Architecture&color=blue
[Dataset]: https://img.shields.io/static/v1?label=&message=Dataset&color=blue
[Decentralized Training]: https://img.shields.io/static/v1?label=&message=Decentralized%20Training&color=blue
[Deep Learning]: https://img.shields.io/static/v1?label=&message=Deep%20Learning&color=blue
[Deep Learning Compiler]: https://img.shields.io/static/v1?label=&message=Deep%20Learning%20Compiler&color=blue
[Deployment]: https://img.shields.io/static/v1?label=&message=Deployment&color=blue
[Dialog]: https://img.shields.io/static/v1?label=&message=Dialog&color=blue
[Differential Equations]: https://img.shields.io/static/v1?label=&message=Differential%20Equations&color=blue
[Discrete Optimization]: https://img.shields.io/static/v1?label=&message=Discrete%20Optimization&color=blue
[Disentanglement]: https://img.shields.io/static/v1?label=&message=Disentanglement&color=blue
[Distillation]: https://img.shields.io/static/v1?label=&message=Distillation&color=blue
[Distributed Training]: https://img.shields.io/static/v1?label=&message=Distributed%20Training&color=blue
[Efficient Inference]: https://img.shields.io/static/v1?label=&message=Efficient%20Inference&color=blue
[Efficient Training]: https://img.shields.io/static/v1?label=&message=Efficient%20Training&color=blue
[Embeddings]: https://img.shields.io/static/v1?label=&message=Embeddings&color=blue
[Empirical Risk Minimization]: https://img.shields.io/static/v1?label=&message=Empirical%20Risk%20Minimization&color=blue
[Energy-based Models]: https://img.shields.io/static/v1?label=&message=Energy-based%20Models&color=blue
[Enery based Models]: https://img.shields.io/static/v1?label=&message=Enery%20based%20Models&color=blue
[Ensemble]: https://img.shields.io/static/v1?label=&message=Ensemble&color=blue
[Entity Linking]: https://img.shields.io/static/v1?label=&message=Entity%20Linking&color=blue
[Error Correction]: https://img.shields.io/static/v1?label=&message=Error%20Correction&color=blue
[Error Detection]: https://img.shields.io/static/v1?label=&message=Error%20Detection&color=blue
[Ethical Impacts of AI]: https://img.shields.io/static/v1?label=&message=Ethical%20Impacts%20of%20AI&color=blue
[Evaluation using LLMs]: https://img.shields.io/static/v1?label=&message=Evaluation%20using%20LLMs&color=blue
[Fake Content Detection]: https://img.shields.io/static/v1?label=&message=Fake%20Content%20Detection&color=blue
[Faster Inference]: https://img.shields.io/static/v1?label=&message=Faster%20Inference&color=blue
[Faster Training]: https://img.shields.io/static/v1?label=&message=Faster%20Training&color=blue
[Few Shot]: https://img.shields.io/static/v1?label=&message=Few%20Shot&color=blue
[GAN]: https://img.shields.io/static/v1?label=&message=GAN&color=blue
[Gaming]: https://img.shields.io/static/v1?label=&message=Gaming&color=blue
[Generative Models]: https://img.shields.io/static/v1?label=&message=Generative%20Models&color=blue
[Genetic Algorithms]: https://img.shields.io/static/v1?label=&message=Genetic%20Algorithms&color=blue
[Gradient Estimation]: https://img.shields.io/static/v1?label=&message=Gradient%20Estimation&color=blue
[Graph Fusion]: https://img.shields.io/static/v1?label=&message=Graph%20Fusion&color=blue
[Graph Neural Networks]: https://img.shields.io/static/v1?label=&message=Graph%20Neural%20Networks&color=blue
[Green AI]: https://img.shields.io/static/v1?label=&message=Green%20AI&color=blue
[Growing Neural Networks]: https://img.shields.io/static/v1?label=&message=Growing%20Neural%20Networks&color=blue
[HPC Compiler]: https://img.shields.io/static/v1?label=&message=HPC%20Compiler&color=blue
[Hallucination]: https://img.shields.io/static/v1?label=&message=Hallucination&color=blue
[Human Feedback]: https://img.shields.io/static/v1?label=&message=Human%20Feedback&color=blue
[Hyperparameter Search]: https://img.shields.io/static/v1?label=&message=Hyperparameter%20Search&color=blue
[Image Classification]: https://img.shields.io/static/v1?label=&message=Image%20Classification&color=blue
[Image Super Resolution]: https://img.shields.io/static/v1?label=&message=Image%20Super%20Resolution&color=blue
[In Context Learning]: https://img.shields.io/static/v1?label=&message=In%20Context%20Learning&color=blue
[Information Retrieval]: https://img.shields.io/static/v1?label=&message=Information%20Retrieval&color=blue
[Information Theory]: https://img.shields.io/static/v1?label=&message=Information%20Theory&color=blue
[Instruction Finetuning]: https://img.shields.io/static/v1?label=&message=Instruction%20Finetuning&color=blue
[Knowledge Graphs]: https://img.shields.io/static/v1?label=&message=Knowledge%20Graphs&color=blue
[Large Models]: https://img.shields.io/static/v1?label=&message=Large%20Models&color=blue
[Linear Attention]: https://img.shields.io/static/v1?label=&message=Linear%20Attention&color=blue
[Long Context Length]: https://img.shields.io/static/v1?label=&message=Long%20Context%20Length&color=blue
[Machine Learning Compilation]: https://img.shields.io/static/v1?label=&message=Machine%20Learning%20Compilation&color=blue
[Machine Translation]: https://img.shields.io/static/v1?label=&message=Machine%20Translation&color=blue
[Mathematical Reasoning]: https://img.shields.io/static/v1?label=&message=Mathematical%20Reasoning&color=blue
[Memory]: https://img.shields.io/static/v1?label=&message=Memory&color=blue
[Memory Reduction]: https://img.shields.io/static/v1?label=&message=Memory%20Reduction&color=blue
[Meta Learning]: https://img.shields.io/static/v1?label=&message=Meta%20Learning&color=blue
[Miscellaneous]: https://img.shields.io/static/v1?label=&message=Miscellaneous&color=blue
[Mixture of Experts]: https://img.shields.io/static/v1?label=&message=Mixture%20of%20Experts&color=blue
[Model Editing]: https://img.shields.io/static/v1?label=&message=Model%20Editing&color=blue
[Model Saving]: https://img.shields.io/static/v1?label=&message=Model%20Saving&color=blue
[Model Upcycling]: https://img.shields.io/static/v1?label=&message=Model%20Upcycling&color=blue
[Molecular Chemistry]: https://img.shields.io/static/v1?label=&message=Molecular%20Chemistry&color=blue
[Multi-modal]: https://img.shields.io/static/v1?label=&message=Multi-modal&color=blue
[NLP]: https://img.shields.io/static/v1?label=&message=NLP&color=blue
[NP-Hard]: https://img.shields.io/static/v1?label=&message=NP-Hard&color=blue
[Networking]: https://img.shields.io/static/v1?label=&message=Networking&color=blue
[Normalization]: https://img.shields.io/static/v1?label=&message=Normalization&color=blue
[Object Detection]: https://img.shields.io/static/v1?label=&message=Object%20Detection&color=blue
[Optimization]: https://img.shields.io/static/v1?label=&message=Optimization&color=blue
[Out-of-Distribution]: https://img.shields.io/static/v1?label=&message=Out-of-Distribution&color=blue
[Planning]: https://img.shields.io/static/v1?label=&message=Planning&color=blue
[Position Embeddings]: https://img.shields.io/static/v1?label=&message=Position%20Embeddings&color=blue
[Prompting]: https://img.shields.io/static/v1?label=&message=Prompting&color=blue
[Pruning]: https://img.shields.io/static/v1?label=&message=Pruning&color=blue
[QUBO]: https://img.shields.io/static/v1?label=&message=QUBO&color=blue
[Quantization]: https://img.shields.io/static/v1?label=&message=Quantization&color=blue
[Quantum Algorithms]: https://img.shields.io/static/v1?label=&message=Quantum%20Algorithms&color=blue
[Quantum Computing]: https://img.shields.io/static/v1?label=&message=Quantum%20Computing&color=blue
[Quantum Teleportation]: https://img.shields.io/static/v1?label=&message=Quantum%20Teleportation&color=blue
[Question Answering]: https://img.shields.io/static/v1?label=&message=Question%20Answering&color=blue
[RL environments]: https://img.shields.io/static/v1?label=&message=RL%20environments&color=blue
[Reading Comprehension]: https://img.shields.io/static/v1?label=&message=Reading%20Comprehension&color=blue
[Reasoning]: https://img.shields.io/static/v1?label=&message=Reasoning&color=blue
[Recommender Systems]: https://img.shields.io/static/v1?label=&message=Recommender%20Systems&color=blue
[Regularization]: https://img.shields.io/static/v1?label=&message=Regularization&color=blue
[Reinforcement Learning]: https://img.shields.io/static/v1?label=&message=Reinforcement%20Learning&color=blue
[Relational Reasoning]: https://img.shields.io/static/v1?label=&message=Relational%20Reasoning&color=blue
[Retrieval-Augmented Generation]: https://img.shields.io/static/v1?label=&message=Retrieval-Augmented%20Generation&color=blue
[Robotics]: https://img.shields.io/static/v1?label=&message=Robotics&color=blue
[Robustness]: https://img.shields.io/static/v1?label=&message=Robustness&color=blue
[Safety and Alignment]: https://img.shields.io/static/v1?label=&message=Safety%20and%20Alignment&color=blue
[Saliency Detection]: https://img.shields.io/static/v1?label=&message=Saliency%20Detection&color=blue
[Scaling Laws]: https://img.shields.io/static/v1?label=&message=Scaling%20Laws&color=blue
[Security]: https://img.shields.io/static/v1?label=&message=Security&color=blue
[Sparse Matrices]: https://img.shields.io/static/v1?label=&message=Sparse%20Matrices&color=blue
[Spatio-Temporal]: https://img.shields.io/static/v1?label=&message=Spatio-Temporal&color=blue
[Speech]: https://img.shields.io/static/v1?label=&message=Speech&color=blue
[State Space Models]: https://img.shields.io/static/v1?label=&message=State%20Space%20Models&color=blue
[Story Generation]: https://img.shields.io/static/v1?label=&message=Story%20Generation&color=blue
[Style Transfer]: https://img.shields.io/static/v1?label=&message=Style%20Transfer&color=blue
[Summarization]: https://img.shields.io/static/v1?label=&message=Summarization&color=blue
[Synthetic Data Generation]: https://img.shields.io/static/v1?label=&message=Synthetic%20Data%20Generation&color=blue
[Systems]: https://img.shields.io/static/v1?label=&message=Systems&color=blue
[Text Classification]: https://img.shields.io/static/v1?label=&message=Text%20Classification&color=blue
[Theory of Computation]: https://img.shields.io/static/v1?label=&message=Theory%20of%20Computation&color=blue
[Time Series]: https://img.shields.io/static/v1?label=&message=Time%20Series&color=blue
[Transfer Learning]: https://img.shields.io/static/v1?label=&message=Transfer%20Learning&color=blue
[Transformers]: https://img.shields.io/static/v1?label=&message=Transformers&color=blue
[Unsupervised Learning]: https://img.shields.io/static/v1?label=&message=Unsupervised%20Learning&color=blue
[Variational Inference]: https://img.shields.io/static/v1?label=&message=Variational%20Inference&color=blue
[Zero Shot]: https://img.shields.io/static/v1?label=&message=Zero%20Shot&color=blue

### Table of contents

-   [AI, DL, NLP and RL](#ai-dl-nlp-and-rl)
-   [Calculus](#calculus)
-   [Computer Architecture](#computer-architecture)
-   [Computer Graphics](#computer-graphics)
-   [Data Structures and Algorithms](#data-structures-and-algorithms)
-   [Digital Electronics](#digital-electronics)
-   [Graph Theory](#graph-theory)
-   [Information Theory](#information-theory)
-   [Linear Algebra](#linear-algebra)
-   [Measure Theory](#measure-theory)
-   [Optimization Theory](#optimization-theory)
-   [Probability and Stochastic Processes](#probability-and-stochastic-processes)
-   [Quantum Computing](#quantum-computing)
-   [Signal Processing](#signal-processing)

# AI, DL, NLP and RL

1. [1-bit Adam: communication efficient large-scale training with Adamâ€™s convergence speed](https://arxiv.org/abs/2102.02888)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training]
1. [5 best practices for efficient model training](https://www.mosaicml.com/blog/5-best-practices-for-efficient-model-training)  
   ![image][Blog] ![image][Deep Learning] ![image][Faster Training] ![image][Systems]
1. [8-bit approximations for parallelism in deep learning](https://arxiv.org/abs/1511.04561)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Quantization] ![image][Systems]
1. [8-bit optimizers via block-wise quantization](https://arxiv.org/abs/2110.02861)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [A 'neural' network that learns to play Backgammon](https://papers.nips.cc/paper/30-a-neural-network-that-learns-to-play-backgammon)  
   ![image][Paper] ![image][Deep Learning]
1. [A BetterTransformer for fast transformer inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [A deep reinforced model for abstractive summarization](https://arxiv.org/abs/1705.04304)  
   ![image][Paper] ![image][NLP] ![image][Reinforcement Learning] ![image][Summarization]
1. [A dynamical approach to temporal pattern processing](https://papers.nips.cc/paper/76-a-dynamical-approach-to-temporal-pattern-processing)  
   ![image][Paper] ![image][Deep Learning]
1. [A few more examples may be worth billions of parameters](https://arxiv.org/abs/2110.04374)  
   ![image][Paper] ![image][Few Shot] ![image][NLP]
1. [A general and adaptive robust loss function](https://arxiv.org/abs/1701.03077)  
   ![image][Paper] ![image][Deep Learning]
1. [A generalist agent](https://arxiv.org/abs/2205.06175)  
   ![image][Paper] ![image][Reinforcement Learning]
1. [A gentle introduction to 8-bit matrix multiplication for transformers at scale using Hugging Face transformers, accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)  
   ![image][Blog] ![image][Deep Learning] ![image][Quantization] ![image][Transformers]
1. [A note on the evaluation of generative models](https://arxiv.org/abs/1511.01844)  
   ![image][Paper] ![image][Deep Learning] ![image][Generative Models]
1. [A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings](https://arxiv.org/abs/1805.06297)  
   ![image][Paper] ![image][Embeddings] ![image][NLP]
1. [A simple but tough-to-beat baseline for sentence embeddings](https://openreview.net/forum?id=SyK00v5xx)  
   ![image][Paper] ![image][Embeddings] ![image][NLP]
1. [A simple language model for task-oriented dialogue](https://arxiv.org/abs/2005.00796)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [A simple neural attentive meta-learner](https://arxiv.org/abs/1707.03141)  
   ![image][Paper] ![image][Deep Learning] ![image][Few Shot] ![image][Meta Learning]
1. [A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)  
   ![image][Paper] ![image][Deep Learning] ![image][Relational Reasoning]
1. [A study of BFLOAT16 for deep learning training](https://arxiv.org/abs/1905.12322)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models]
1. [A style-based generator architecture for generative adversarial networks](https://arxiv.org/abs/1812.04948)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [A stylometric inquiry into hyperpartisan and fake news](https://arxiv.org/abs/1702.05638)  
   ![image][Paper] ![image][Fake Content Detection] ![image][NLP]
1. [A3T: adversarially augmented adversarial training](https://arxiv.org/abs/1801.04055)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Accelerated PyTorch 2 transformers](https://pytorch.org/blog/accelerated-pytorch-2/)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [Accelerating large language model training with variable sparse pre-training and dense fine-tuning](https://www.cerebras.net/blog/accelerating-llm-training-with-variable-sparse-pre-training-and-dense-fine-tuning/)  
   ![image][Blog] ![image][Large Models] ![image][Sparse Matrices]
1. [Accelerating PyTorch with CUDA graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)  
   ![image][Blog] ![image][Deep Learning] ![image][Large Models] ![image][Systems]
1. [AdapterHub: a framework for adapting transformers](https://arxiv.org/abs/2007.07779)  
   ![image][Paper] ![image][Efficient Training] ![image][NLP] ![image][Transformers]
1. [Adversarial approximate inference for speech to electroglottograph conversion](https://arxiv.org/abs/1903.12248)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Approximate Inference] ![image][Deep Learning] ![image][GAN] ![image][Speech]
1. [Adversarial autoencoders](https://arxiv.org/abs/1511.05644)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Adversarial examples that fool both computer vision and time-limited humans](https://arxiv.org/abs/1802.08195)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning]
1. [Adversarial feature learning](https://arxiv.org/abs/1605.09782)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Adversarial generation of natural language](https://arxiv.org/abs/1705.10929)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Generative Models] ![image][NLP]
1. [Adversarial information factorization](https://arxiv.org/abs/1711.05175)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Adversarially learned inference](https://arxiv.org/abs/1606.00704)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [AlexaTM 20B: few-shot learning using a large-scale multilingual seq2seq model](https://arxiv.org/abs/2208.01448)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Amazon SageMaker model parallelism: a general and flexible framework for large model training](https://arxiv.org/abs/2111.05972)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Systems]
1. [An image is worth 16x16 words: transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)  
   ![image][Paper] ![image][Computer Vision] ![image][Deep Learning] ![image][Embeddings]
1. [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)  
   ![image][Paper] ![image][Deep Learning] ![image][Optimization]
1. [Analysing mathematical reasoning abilities of neural models](https://arxiv.org/abs/1904.01557)  
   ![image][Paper] ![image][Deep Learning]
1. [Approximation by superpositions of sigmoidal function](https://link.springer.com/article/10.1007/BF02551274)  
   ![image][Paper] ![image][Deep Learning]
1. Artificial Intelligence: a modern approach  
   ![image][Book]
1. [Aspect based sentiment analysis with gated convolutional networks](https://arxiv.org/abs/1805.07043)  
   ![image][Paper] ![image][NLP] ![image][Text Classification]
1. [Attention is all you need](https://arxiv.org/abs/1706.03762)  
   ![image][Paper] ![image][Attention Mechanism] ![image][Deep Learning] ![image][Transformers]
1. [Attention is off by one](https://www.evanmiller.org/attention-is-off-by-one.html)  
   ![image][Blog] ![image][Attention Mechanism] ![image][NLP]
1. [Auto-encoding variational Bayes](https://arxiv.org/abs/1312.6114)  
   ![image][Paper] ![image][Deep Learning] ![image][Variational Inference]
1. [Backpropagation through the void: optimizing control variates for black-box gradient estimation](https://arxiv.org/abs/1711.00123)  
   ![image][Paper] ![image][Backpropagation] ![image][Deep Learning] ![image][Discrete Optimization] ![image][Gradient Estimation] ![image][Optimization] ![image][Reinforcement Learning] ![image][Variational Inference]
1. [BART: denoising sequence-to-sequence pre-training for natural language generation, translation and comprehension](https://arxiv.org/abs/1910.13461)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [Batch normalization: accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167)  
   ![image][Paper] ![image][Deep Learning] ![image][Normalization] ![image][Optimization]
1. [Behavioral cloning from observation](https://arxiv.org/abs/1805.01954)  
   ![image][Paper] ![image][Behavior and Control] ![image][Deep Learning] ![image][Genetic Algorithms] ![image][Robotics]
1. [BERT: pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [Better & faster large language models via multi-token prediction](https://arxiv.org/abs/2404.19737)  
   ![image][Paper] ![image][Faster Inference] ![image][NLP]
1. [Beyond domain APIs: Task-oriented conversational modeling with unstructured knowledge access](https://arxiv.org/abs/2006.03533)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Beyond KV caching: shared attention for efficient LLMs](https://arxiv.org/abs/2407.12866)  
   ![image][Paper] ![image][Attention Mechanism] ![image][Efficient Inference] ![image][NLP] ![image][Transformers]
1. [BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation](https://arxiv.org/abs/2201.12086)  
   ![image][Paper] ![image][Computer Vision] ![image][Embeddings]
1. [Blockwise parallel transformer for large context models](https://arxiv.org/abs/2305.19370)  
   ![image][Paper] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [BLOOM: A 176B-parameter open-access multilingual language model](https://arxiv.org/abs/2211.05100)  
   ![image][Paper] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Bootstrapping entity alignment with knowledge graph embedding](https://www.ijcai.org/proceedings/2018/611)  
   ![image][Paper] ![image][Embeddings] ![image][Knowledge Graphs] ![image][NLP]
1. [Bridging the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation](https://www.aclweb.org/anthology/2020.emnlp-main.275/)  
   ![image][Paper] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Bringing open large language models to consumer devices](https://mlc.ai/blog/2023/05/22/bringing-open-large-language-models-to-consumer-devices)  
   ![image][Blog] ![image][Deep Learning] ![image][Deployment] ![image][Machine Learning Compilation]
1. [BTLM-3B-8K: 7B performance in a 3 billion parameter model](https://www.cerebras.net/machine-learning/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/)  
   ![image][Blog] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Building blocks for a complex-valued transformer architecture](https://arxiv.org/abs/2306.09827)  
   ![image][Paper] ![image][Complex Numbers] ![image][Deep Learning] ![image][Transformers]
1. [CATS: contextually-aware thresholding for sparsity in large language models](https://arxiv.org/abs/2404.08763)  
   ![image][Paper] ![image][Efficient Inference] ![image][Sparse Matrices] ![image][Systems]
1. [ChatGPT: optimizing language models for dialogue](https://openai.com/blog/chatgpt/)  
   ![image][Blog] ![image][Dialog] ![image][NLP]
1. [ColBERT: efficient and effective passage search via contextualized late interaction over BERT](https://arxiv.org/abs/2004.12832)  
   ![image][Paper] ![image][Information Retrieval] ![image][NLP]
1. [Colossal-AI: a unified deep learning system for large-scale parallel training](https://arxiv.org/abs/2110.14883)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Systems]
1. [Compiling machine learning programs via high-level tracing](https://research.google/pubs/pub47008/)  
   ![image][Paper] ![image][Deep Learning] ![image][Systems]
1. [Complex transformer: a framework for modeling complex-valued sequence](https://arxiv.org/abs/1910.10202)  
   ![image][Paper] ![image][Complex Numbers] ![image][Deep Learning] ![image][Transformers]
1. [Conceptual captions: a cleaned, hypernymed, image alt-text dataset for automatic image captioning](https://aclanthology.org/P18-1238/)  
   ![image][Paper] ![image][Computer Vision] ![image][Dataset] ![image][NLP]
1. [Conditional image synthesis with auxilliary classifier GANs](https://arxiv.org/abs/1610.09585)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Conformal nucleus sampling](https://arxiv.org/abs/2305.02633)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [Connecting large language models with evolutionary algorithms yields powerful prompt optimizers](https://arxiv.org/abs/2309.08532)  
   ![image][Paper] ![image][Genetic Algorithms] ![image][NLP] ![image][Prompting]
1. [Connectivity versus entropy](https://papers.nips.cc/paper/63-connectivity-versus-entropy)  
   ![image][Paper] ![image][Deep Learning]
1. [Constituency parsing with a self-attentive encoder](https://arxiv.org/abs/1805.01052)  
   ![image][Paper] ![image][Constituency Parsing] ![image][NLP]
1. [Constraint based knowledge base distillation in end-to-end task oriented dialogs](https://arxiv.org/abs/2109.07396)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Context generation improves open domain question answering](https://arxiv.org/abs/2210.06349)  
   ![image][Paper] ![image][Few Shot] ![image][NLP] ![image][Question Answering]
1. [Convert transformers to ONNX with hugging face optimum](https://huggingface.co/blog/convert-transformers-to-onnx)  
   ![image][Blog] ![image][Deep Learning] ![image][Model Saving] ![image][Systems]
1. [Convolutional networks for graphs for learning molecular fingerprints](https://arxiv.org/abs/1509.09292)  
   ![image][Paper] ![image][Deep Learning] ![image][Molecular Chemistry]
1. [Convolutional neural network language models](https://aclanthology.org/D16-1123/)  
   ![image][Paper] ![image][NLP]
1. [Countering adversarial images using input transformations](https://arxiv.org/abs/1711.00117)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Cramming: training a language model on a single GPU in one day](https://arxiv.org/abs/2212.14034)  
   ![image][Paper] ![image][Green AI] ![image][NLP]
1. [Crosslingual generalization through multitask finetuning](https://arxiv.org/abs/2211.01786)  
   ![image][Paper] ![image][Distributed Training] ![image][Instruction Finetuning] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Curriculum learning](https://dl.acm.org/citation.cfm?id=1553380)  
   ![image][Paper] ![image][Curriculum Learning] ![image][Deep Learning]
1. [Cutting down on prompts and parameters: simple few-shot learning with language models](https://arxiv.org/abs/2106.13353)  
   ![image][Paper] ![image][Efficient Training] ![image][Few Shot] ![image][NLP]
1. [Data engineering for scaling language models to 128K context](https://arxiv.org/abs/2402.10171)  
   ![image][Paper] ![image][Data Mixtures] ![image][Long Context Length] ![image][NLP]
1. [Deep Boltzmann machines](https://proceedings.mlr.press/v5/salakhutdinov09a.html)  
   ![image][Paper] ![image][Boltzmann Machines] ![image][Deep Learning] ![image][Energy-based Models]
1. [Deep complex networks](https://arxiv.org/abs/1705.09792)  
   ![image][Paper] ![image][Complex Numbers] ![image][Deep Learning]
1. Deep learning  
   ![image][Book]
1. [Deep learning and the information bottleneck principle](https://arxiv.org/abs/1503.02406)  
   ![image][Paper] ![image][Deep Learning] ![image][Information Theory]
1. [Deep learning techniques for super-resolution in video games](https://arxiv.org/abs/2012.09810)  
   ![image][Paper] ![image][Computer Graphics] ![image][Computer Vision] ![image][Deep Learning] ![image][Image Super Resolution]
1. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385)  
   ![image][Paper] ![image][Computer Vision] ![image][Image Classification]
1. [Deep text classification can be fooled](https://arxiv.org/abs/1704.08006)  
   ![image][Paper] ![image][Adversarial Examples] ![image][NLP]
1. [DeepSpeed compression: a composable library for extreme compression and zero-cost quantization](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)  
   ![image][Blog] ![image][Compression] ![image][Deep Learning] ![image][Quantization]
1. [DeepSpeed Inference: enabling efficient inference of transformer models at unprecedented scale](https://arxiv.org/abs/2207.00032)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Systems]
1. [DeepSpeed powers 8x larger MoE model training with high performance](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)  
   ![image][Blog] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts] ![image][Transformers]
1. [DeepSpeed Ulysses: system optimizations for enabling training of extreme long sequence transformer models](https://arxiv.org/abs/2309.14509)  
   ![image][Paper] ![image][Deep Learning] ![image][Long Context Length] ![image][Systems]
1. [DeepSpeed: accelerating large-scale model inference and training via system optimizations and compression](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/#:~:text=DeepSpeed%20Inference%20also%20supports%20fast,multiple%20GPUs%20for%20parallel%20execution.)  
   ![image][Blog] ![image][Deep Learning] ![image][Large Models] ![image][Systems]
1. [DeepSpeed: advancing MoE inference and training to power next-generation AI scale](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)  
   ![image][Blog] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts] ![image][Transformers]
1. [Denoising distantly supervised open-domain question answering](https://aclanthology.org/P18-1161/)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [Differential transformer](https://arxiv.org/abs/2410.05258)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [Diffusion convolutional recurrent neural network: data-driven traffic forecasting](https://arxiv.org/abs/1707.01926)  
   ![image][Paper] ![image][Deep Learning] ![image][Graph Neural Networks] ![image][Spatio-Temporal] ![image][Time Series]
1. [Discrete variational autoencoders](https://arxiv.org/abs/1609.02200)  
   ![image][Paper] ![image][Deep Learning] ![image][Variational Inference]
1. [Disentangling by factorising](https://arxiv.org/abs/1802.05983)  
   ![image][Paper] ![image][Deep Learning] ![image][Disentanglement] ![image][Variational Inference]
1. [Disentangling language and knowledge in task-oriented dialogs](https://arxiv.org/abs/1805.01216)  
   ![image][Paper] ![image][Dialog] ![image][Disentanglement] ![image][NLP]
1. [Distributionally robust language modeling](https://arxiv.org/abs/1909.02060)  
   ![image][Paper] ![image][NLP] ![image][Out-of-Distribution] ![image][Robustness]
1. [Editing models with task arithmetic](https://arxiv.org/abs/2212.04089)  
   ![image][Paper] ![image][Deep Learning] ![image][Efficient Training] ![image][Model Editing]
1. [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781)  
   ![image][Paper] ![image][Embeddings] ![image][NLP]
1. [Efficient large scale language modeling with mixtures of experts](https://arxiv.org/abs/2112.10684)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Efficient large-scale language model training on GPU clusters using Megatron-LM](https://arxiv.org/abs/2104.04473)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Enabling float8 all-gather in FSDP2](https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359)  
   ![image][Blog] ![image][Efficient Training] ![image][Systems]
1. [Enchancing the reliability of out-of-distribution image detection in neural networks](https://arxiv.org/abs/1706.02690)  
   ![image][Paper] ![image][Deep Learning] ![image][Out-of-Distribution]
1. [End-to-end task-oriented dialog modeling with semi-structured knowledge management](https://arxiv.org/abs/2106.11796)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Enhance reasoning for large language models in the game Werewolf](https://arxiv.org/abs/2402.02330)  
   ![image][Paper] ![image][Gaming] ![image][Reasoning] ![image][Reinforcement Learning]
1. [Ensemble adversarial training: attacks and defenses](https://arxiv.org/abs/1705.07204)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Equilibrium propagation: bridging the gap between energy-based models and backpropagation](https://arxiv.org/abs/1602.05179)  
   ![image][Paper] ![image][Backpropagation] ![image][Deep Learning] ![image][Enery based Models]
1. [Estimating or propagating gradients through stochastic neurons for conditional computation](https://arxiv.org/abs/1308.3432)  
   ![image][Paper] ![image][Backpropagation] ![image][Deep Learning] ![image][Gradient Estimation]
1. [Exemplar encoder-decoder for neural conversation generation](https://www.aclweb.org/anthology/P18-1123/)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Expert human-level driving in gran turismo sport using deep reinforcement learning with image-based representation](https://arxiv.org/abs/2111.06449)  
   ![image][Paper] ![image][Reinforcement Learning]
1. [Exploring deep recurrent models with reinforcement learning for molecule design](https://openreview.net/forum?id=HkcTe-bR-)  
   ![image][Paper] ![image][Molecular Chemistry] ![image][Reinforcement Learning]
1. [Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/abs/1910.10683)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [Extreme compression for pre-trained transformers made simple and efficient](https://arxiv.org/abs/2206.01859)  
   ![image][Paper] ![image][Compression] ![image][Deep Learning] ![image][Distillation] ![image][Large Models] ![image][Pruning] ![image][Transformers]
1. [Fast abstractive summarization with reinforce-selected sentence rewriting](https://arxiv.org/abs/1805.11080)  
   ![image][Paper] ![image][NLP] ![image][Reinforcement Learning] ![image][Summarization]
1. [Fast benchmarking of accuracy vs. training time with cyclic learning rates](https://arxiv.org/abs/2206.00832)  
   ![image][Paper] ![image][Deep Learning] ![image][Hyperparameter Search]
1. [Fast transformer decoding: one write-head is all you need](https://arxiv.org/abs/1911.02150)  
   ![image][Paper] ![image][Attention Mechanism] ![image][Efficient Inference] ![image][NLP] ![image][Transformers]
1. [Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning](https://arxiv.org/abs/2205.05638)  
   ![image][Paper] ![image][Deep Learning] ![image][Efficient Training]
1. [FFJORD: Free-form continuous dynamics for scalable reversible generative models](https://arxiv.org/abs/1810.01367)  
   ![image][Paper] ![image][Deep Learning] ![image][Generative Models]
1. [Finetuned language models are zero-shot learners](https://arxiv.org/abs/2109.01652)  
   ![image][Paper] ![image][NLP] ![image][Transformers] ![image][Zero Shot]
1. [Finetuning pretrained transformers into RNNs](https://arxiv.org/abs/2103.13076)  
   ![image][Paper] ![image][Linear Attention] ![image][NLP] ![image][Transformers]
1. [Flash-decoding for long-context inference](https://pytorch.org/blog/flash-decoding/)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [FlashAttention: fast and memory-efficient exact attention with IO-awareness](https://arxiv.org/abs/2205.14135)  
   ![image][Paper] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [FlashAttention: fast transformer training with long sequences](https://crfm.stanford.edu/2023/01/13/flashattention.html)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [Foundations of NLP explained visually: beam search, how it works](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24)  
   ![image][Blog] ![image][Algorithms] ![image][NLP]
1. [FP8 formats for deep learning](https://arxiv.org/abs/2209.05433)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization] ![image][Systems]
1. [FP8-LM: training FP8 large language models](https://arxiv.org/abs/2310.18313)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization] ![image][Systems]
1. [Gemini: a family of highly capable multimodal models](https://arxiv.org/abs/2312.11805)  
   ![image][Paper] ![image][Large Models] ![image][NLP]
1. [Gemma: open models based on Gemini research and technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Generating adversarial examples with adversarial networks](https://www.ijcai.org/proceedings/2018/543)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Generating sentences from a continuous space](https://arxiv.org/abs/1511.06349)  
   ![image][Paper] ![image][NLP]
1. [Generation-augmented retrieval for open-domain question answering](https://arxiv.org/abs/2009.08553)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [Generative adversarial nets](https://arxiv.org/abs/1406.2661)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Generative pretraining from pixels](https://proceedings.mlr.press/v119/chen20s.html)  
   ![image][Paper] ![image][Computer Vision] ![image][Deep Learning]
1. Genetic algorithms in search, optimization and machine learning  
   ![image][Book]
1. [GeoMAN: multi-level attention networks for geo-sensory time series prediction](https://www.ijcai.org/proceedings/2018/476)  
   ![image][Paper] ![image][Deep Learning] ![image][Spatio-Temporal] ![image][Time Series]
1. [Getting the most out of the NVIDIA A100 GPU with Multi-Instance GPU](https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems]
1. [GLaM: efficient scaling of language models with mixture-of-experts](https://arxiv.org/abs/2112.06905)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts] ![image][Transformers]
1. [GLM-130B: an open bilingual pre-trained model](https://keg.cs.tsinghua.edu.cn/glm-130b/)  
   ![image][Blog] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [GLU variants improve transformer](https://arxiv.org/abs/2002.05202)  
   ![image][Paper] ![image][Activation Function] ![image][Deep Learning]
1. [Going deeper with convolutions](https://arxiv.org/abs/1409.4842)  
   ![image][Paper] ![image][Computer Vision] ![image][Image Classification]
1. [GPT-4 architecture, infrastructure, training dataset, costs, vision, MoE](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)  
   ![image][Blog] ![image][Distributed Training] ![image][Large Models] ![image][Mixture of Experts] ![image][NLP] ![image][Systems] ![image][Transformers]
1. [GPT-NeoX-20B: an open-source autoregressive language model](https://arxiv.org/abs/2204.06745)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [GQA: training generalized multi-query transformer models from multi-head checkpoints](https://arxiv.org/abs/2305.13245)  
   ![image][Paper] ![image][Attention Mechanism] ![image][Efficient Inference] ![image][NLP] ![image][Transformers]
1. [Gradient-based hyperparameter optimization through reversible learning](https://arxiv.org/abs/1502.03492)  
   ![image][Paper] ![image][Deep Learning] ![image][Meta Learning]
1. [Graph attention networks](https://arxiv.org/abs/1710.10903)  
   ![image][Paper] ![image][Deep Learning] ![image][Graph Neural Networks]
1. [Grounding large language models in interactive environments with online reinforcement learning](https://arxiv.org/abs/2302.02662)  
   ![image][Paper] ![image][RL environments] ![image][Reinforcement Learning]
1. [Hierarchical neural story generation](https://arxiv.org/abs/1805.04833)  
   ![image][Paper] ![image][NLP] ![image][Story Generation]
1. [Hindsight: posterior-guided training of retrievers for improved open-ended generation](https://arxiv.org/abs/2110.07752)  
   ![image][Paper] ![image][NLP] ![image][Retrieval-Augmented Generation] ![image][Variational Inference]
1. [HiPPO: recurrent memory with optimal polynomial projections](https://hazyresearch.stanford.edu/blog/2020-12-05-hippo)  
   ![image][Blog] ![image][Deep Learning] ![image][State Space Models]
1. [HotFlip: white-box adversarial examples for text classification](https://arxiv.org/abs/1712.06751)  
   ![image][Paper] ![image][Adversarial Examples] ![image][NLP]
1. [How big should my language model be?](https://huggingface.co/calculator/)  
   ![image][Blog] ![image][Large Models] ![image][NLP] ![image][Scaling Laws]
1. [How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generation](https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26)  
   ![image][Blog] ![image][Deep Learning Compiler] ![image][Systems]
1. [How should AI systems behave, and who should decide?](https://openai.com/blog/how-should-ai-systems-behave)  
   ![image][Blog] ![image][NLP] ![image][Safety and Alignment]
1. [How we sped up transformer inference 100x for ðŸ¤— API customers](https://huggingface.co/blog/accelerated-inference)  
   ![image][Blog] ![image][Deep Learning] ![image][Transformers]
1. [How ðŸ¤— Accelerate runs very large models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Hydragen: high-throughput LLM inference with shared prefixes](https://arxiv.org/abs/2402.05099)  
   ![image][Paper] ![image][Efficient Inference] ![image][Systems]
1. [HyKnow: end-to-end task-oriented dialog modeling with hybrid knowledge management](https://arxiv.org/abs/2105.06041)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Hyperparameter search with Transformers and Ray Tune](https://huggingface.co/blog/ray-tune)  
   ![image][Blog] ![image][Deep Learning] ![image][Hyperparameter Search] ![image][Transformers]
1. [Image-to-image translation with conditional generative adversarial networks](https://arxiv.org/abs/1611.07004)  
   ![image][Paper]
1. [ImageNet classification using deep convolutional neural networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)  
   ![image][Paper] ![image][Computer Vision] ![image][Image Classification]
1. [Implementing block-sparse matrix multiplication kernels using triton](https://openreview.net/forum?id=doa11nN5vG)  
   ![image][Paper] ![image][Deep Learning] ![image][Mixture of Experts] ![image][Systems]
1. [Improving entity linking by modeling latent relations between mentions](https://arxiv.org/abs/1804.10637)  
   ![image][Paper] ![image][Entity Linking] ![image][NLP]
1. [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)  
   ![image][Paper] ![image][Hallucination] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Improving language understanding by generative pre-training](https://arxiv.org/abs/2112.04426)  
   ![image][Paper] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Improving reinforcement learning from human feedback with efficient reward model ensemble](https://arxiv.org/abs/2401.16635)  
   ![image][Paper] ![image][Ensemble] ![image][NLP] ![image][Reinforcement Learning]
1. [Incredibly fast BLOOM inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)  
   ![image][Blog] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Inference suboptimality in variational autoencoders](https://arxiv.org/abs/1801.03558)  
   ![image][Paper] ![image][Approximate Inference] ![image][Deep Learning] ![image][Variational Inference]
1. [InfoGAN: interpretable representation learning by information maximizing generative adversarial nets](https://arxiv.org/abs/1606.03657)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN] ![image][Information Theory]
1. [Interpretable convolutional neural networks via feedforward design](https://arxiv.org/abs/1810.02786)  
   ![image][Paper] ![image][Computer Vision]
1. [Introducing async tensor parallelism in PyTorch](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487)  
   ![image][Blog] ![image][Efficient Training] ![image][Systems]
1. [Introducing MPT-7B: a new standard for open-source, commercially usable LLMs](https://www.mosaicml.com/blog/mpt-7b)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Introducing nvFuser, a deep learning compiler for PyTorch](https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/)  
   ![image][Blog] ![image][Deep Learning] ![image][Graph Fusion] ![image][HPC Compiler] ![image][Systems]
1. [Introducing Turing image super resolution: AI powered image enhancements for Microsoft Edge and Bing maps](https://blogs.bing.com/search-quality-insights/may-2022/Turing-Image-Super-Resolution)  
   ![image][Blog] ![image][Deep Learning] ![image][Image Super Resolution]
1. [Introducing ðŸ¤— accelerate](https://huggingface.co/blog/accelerate-library)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Is ChatGPT 175 billion parameters? Technical analysis](https://orenleung.com/is-chatgpt-175-billion-parameters-technical-analysis)  
   ![image][Paper] ![image][NLP] ![image][Systems]
1. [Is Flash Attention stable](https://arxiv.org/abs/2405.02803)  
   ![image][Paper] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [Is the future of neural networks Sparse? An introduction (1/N)](https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70)  
   ![image][Blog] ![image][Deep Learning] ![image][Sparse Matrices]
1. [Jack of all trades, master of some, a multi-purpose transformer agent](https://huggingface.co/blog/jat)  
   ![image][Blog] ![image][Reinforcement Learning]
1. [Jack of all trades, master of some, a multi-purpose transformer agent](https://arxiv.org/abs/2402.09844)  
   ![image][Paper] ![image][Reinforcement Learning]
1. [Joint reasoning on hybrid-knowledge sources for task-oriented dialog](https://arxiv.org/abs/2210.07295)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Judging LLM-as-a-judge with MT-bench and chatbot arena](https://arxiv.org/abs/2306.05685)  
   ![image][Paper] ![image][Deep Learning] ![image][Evaluation using LLMs]
1. [Know what you don't know: unanswerable questions for SQuAD](https://arxiv.org/abs/1806.03822)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [Knowledge-grounded dialogue generation with pre-trained language models](https://arxiv.org/abs/2010.08824)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Language is not all you need: aligning perception with language models](https://arxiv.org/abs/2302.14045)  
   ![image][Paper] ![image][Deep Learning] ![image][Multi-modal]
1. [Language modeling with gated convolutional networks](https://arxiv.org/abs/1612.08083)  
   ![image][Paper] ![image][Activation Function] ![image][NLP]
1. [Language modelling with pixels](https://arxiv.org/abs/2207.06991)  
   ![image][Paper] ![image][Computer Vision] ![image][NLP]
1. [Language models (mostly) know what they know](https://arxiv.org/abs/2207.05221)  
   ![image][Paper] ![image][Hallucination] ![image][NLP]
1. [Language models are unsupervised multitask learners](https://openai.com/blog/better-language-models/)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [Language models as compilers: simulating pseudocode execution improves algorithmic reasoning in language models](https://arxiv.org/abs/2404.02575)  
   ![image][Paper] ![image][NLP] ![image][Prompting]
1. [Large language models are not fair evaluators](https://arxiv.org/abs/2305.17926)  
   ![image][Paper] ![image][Evaluation using LLMs] ![image][NLP]
1. [Layer normalization](http://arxiv.org/abs/1607.06450)  
   ![image][Paper] ![image][Deep Learning] ![image][Normalization] ![image][Optimization]
1. [Layer-condensed KV cache for efficient inference of large language models](https://arxiv.org/abs/2405.10637)  
   ![image][Blog] ![image][Memory Reduction] ![image][NLP] ![image][Transformers]
1. [Learning activation functions to improve deep neural networks](https://arxiv.org/abs/1412.6830)  
   ![image][Paper] ![image][Deep Learning]
1. [Learning associative inference using fast weight memory](https://arxiv.org/abs/2011.07831)  
   ![image][Paper] ![image][Long Context Length] ![image][NLP]
1. [Learning discourse-level diversity for neural dialog models using conditional variational autoencoders](https://arxiv.org/abs/1703.10960)  
   ![image][Paper] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Learning on a general network](https://papers.nips.cc/paper/9-learning-on-a-general-network)  
   ![image][Paper] ![image][Backpropagation] ![image][Deep Learning]
1. [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)  
   ![image][Paper] ![image][Backpropagation] ![image][Deep Learning]
1. [Learning transferable visual models from natural language supervision](https://arxiv.org/abs/2103.00020)  
   ![image][Paper] ![image][Computer Vision] ![image][Embeddings]
1. [Learning transferable visual models from natural language supervision](https://arxiv.org/abs/2103.00020)  
   ![image][Paper] ![image][Computer Vision] ![image][Deep Learning] ![image][Embeddings]
1. [Learning word embeddings efficiently with noise-contrastive estimation](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation)  
   ![image][Paper] ![image][Embeddings] ![image][NLP]
1. [Leave no context behind: efficient infinite context transformers with infini-attention](https://arxiv.org/abs/2404.07143)  
   ![image][Paper] ![image][Long Context Length] ![image][NLP]
1. [Lessons learned on language model safety and misuse](https://openai.com/blog/language-model-safety-and-misuse/)  
   ![image][Blog] ![image][Ethical Impacts of AI] ![image][NLP]
1. [Lifelong language pretraining with distribution-specialized experts](https://arxiv.org/abs/2305.12281)  
   ![image][Paper] ![image][Catastrophic Forgetting] ![image][Deep Learning] ![image][Mixture of Experts]
1. [Linear scaling made possible with weight streaming](https://www.cerebras.net/blog/linear-scaling-made-possible-with-weight-streaming)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Systems]
1. [Linformer: self-attention with linear complexity](https://arxiv.org/abs/2006.04768)  
   ![image][Paper] ![image][Attention Mechanism] ![image][Deep Learning] ![image][Transformers]
1. [LLM in a flash: efficient large language model inference with limited memory](https://arxiv.org/abs/2312.11514)  
   ![image][Paper] ![image][Deep Learning] ![image][Systems] ![image][Transformers]
1. [LLM.int8(): 8-bit matrix multiplication for transformers at scale](https://arxiv.org/abs/2208.07339)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [Long sequence modeling with XGen: a 7B LLM trained on 8K input sequence length](https://blog.salesforceairesearch.com/xgen/)  
   ![image][Blog] ![image][Long Context Length] ![image][NLP] ![image][Transformers]
1. [LoRA: Low-Rank Adaptation of large language models](https://arxiv.org/abs/2106.09685)  
   ![image][Paper] ![image][Deep Learning] ![image][Efficient Training]
1. [Lost in the middle: how language models use long contexts](https://arxiv.org/abs/2307.03172)  
   ![image][Paper] ![image][Long Context Length] ![image][Transformers]
1. [M6-10T: a sharing-delinking paradigm for efficient multi-trillion parameter pretraining](https://arxiv.org/abs/2110.03888)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Green AI] ![image][Large Models] ![image][Mixture of Experts] ![image][Multi-modal] ![image][Transformers]
1. Machine learning  
   ![image][Book]
1. Machine learning: a probabilistic perspective  
   ![image][Book]
1. [Making deep learning go brrrr from first principles](https://horace.io/brrr_intro.html)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems]
1. [Making DeepSpeed ZeRO run efficiently on more-affordable hardware](https://www.amazon.science/blog/making-deepspeed-zero-run-efficiently-on-more-affordable-hardware)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Mask & focus: conversation modelling by learning concepts](https://arxiv.org/abs/2003.04976)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Matryoshka representation learning](https://arxiv.org/abs/2205.13147)  
   ![image][Paper] ![image][Deep Learning] ![image][Embeddings]
1. [Maximizing communication efficiency for large-scale training via 0/1 Adam](https://arxiv.org/abs/2202.06009)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training]
1. [MCR-DL: mix-and-match communication runtime for deep learning](https://arxiv.org/abs/2303.08374)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Systems]
1. [MegaBlocks: efficient sparse training with mixture-of-experts](https://arxiv.org/abs/2211.15841)  
   ![image][Paper] ![image][Deep Learning] ![image][Mixture of Experts] ![image][Systems]
1. [Megatron-LM: training multi-billion parameter language models using model parallelism](https://arxiv.org/abs/1909.08053)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Systems] ![image][Transformers]
1. [Memory-efficient pipeline-parallel DNN training](https://arxiv.org/abs/2006.09503)  
   ![image][Paper] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Systems] ![image][Transformers]
1. [MinTL: minimalist transfer learning for task-oriented dialogue systems](https://arxiv.org/abs/2009.12005)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Mix and match: learning-free controllable text generation using energy language models](https://arxiv.org/abs/2203.13299)  
   ![image][Paper] ![image][Energy-based Models] ![image][NLP]
1. [Mixed precision training](https://arxiv.org/abs/1710.03740)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [Mixture of attention heads: selecting attention heads per token](https://arxiv.org/abs/2210.05144)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts] ![image][Transformers]
1. [Mixture-of-Experts meets instruction tuning: a winning combination for large language models](https://arxiv.org/abs/2305.14705)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][Mixture of Experts] ![image][NLP]
1. [mixup: beyond empirical risk minimization](https://arxiv.org/abs/1710.09412v1)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Adversarial Learning] ![image][Deep Learning] ![image][Empirical Risk Minimization] ![image][Generative Models]
1. [MMCoQA: conversational question answering over text, tables and images](https://aclanthology.org/2022.acl-long.290/)  
   ![image][Paper] ![image][Dataset] ![image][Dialog] ![image][NLP] ![image][Question Answering]
1. [Mode matching in GANs through latent space learning and inversion](https://arxiv.org/abs/1811.03692)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Multi-level memory for task oriented dialogs](https://arxiv.org/abs/1810.10647)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Multitask prompt tuning enables parameter-efficient transfer learning](https://openreview.net/forum?id=Nk2pDtuhTq)  
   ![image][Paper] ![image][Efficient Training] ![image][NLP]
1. [MultiWOZ - A large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling](https://arxiv.org/abs/1810.00278)  
   ![image][Paper] ![image][Dataset] ![image][Dialog] ![image][NLP]
1. [Mutual information neural estimation](https://arxiv.org/abs/1801.04062)  
   ![image][Paper] ![image][Deep Learning] ![image][Information Theory]
1. [NeMo: a toolkit for building AI applications using neural modules](https://arxiv.org/abs/1909.09577)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Neural GPUs learn algorithms](https://arxiv.org/abs/1511.08228)  
   ![image][Paper] ![image][Deep Learning] ![image][Theory of Computation]
1. Neural network methods for natural language processing  
   ![image][Book]
1. [Neural networks and physical systems with emergent collective computational abilities](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/)  
   ![image][Paper] ![image][Biology] ![image][Deep Learning] ![image][Energy-based Models]
1. Neural networks for pattern recognition  
   ![image][Book]
1. [Neural ordinary differential equations](https://arxiv.org/abs/1806.07366)  
   ![image][Paper] ![image][Deep Learning] ![image][Differential Equations]
1. [No train no gain: revisiting efficient training algorithms for transformer-based language models](https://arxiv.org/abs/2307.06440)  
   ![image][Paper] ![image][Deep Learning] ![image][Miscellaneous]
1. [Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples](https://arxiv.org/abs/1802.00420)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning]
1. [OctoPack: instruction tuning code large language models](https://arxiv.org/abs/2308.07124)  
   ![image][Paper] ![image][AI for code] ![image][Instruction Finetuning] ![image][NLP]
1. [On the convergence of Adam and beyond](https://arxiv.org/abs/1904.09237)  
   ![image][Paper] ![image][Convergence] ![image][Deep Learning] ![image][Optimization]
1. [On the power of neural networks for solving hard problems](https://papers.nips.cc/paper/70-on-the-power-of-neural-networks-for-solving-hard-problems)  
   ![image][Paper] ![image][Deep Learning] ![image][NP-Hard]
1. [One model to learn them all](https://arxiv.org/abs/1706.05137)  
   ![image][Paper] ![image][Deep Learning] ![image][Multi-modal]
1. [Open domain question answering over tables via dense retrieval](https://arxiv.org/abs/2103.12011)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [Open question answering over tables and text](https://openreview.net/forum?id=MmCRswl1UYl)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [OPT: open pre-trained transformer language models](https://arxiv.org/abs/2205.01068)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Optimal brain compression: a framework for accurate post-training quantization and pruning](https://arxiv.org/abs/2208.11580)  
   ![image][Paper] ![image][Deep Learning] ![image][Pruning] ![image][Quantization]
1. [Optimal perceptual inference](https://www.researchgate.net/publication/260869405_Optimal_perceptual_inference)  
   ![image][Paper] ![image][Deep Learning] ![image][Energy-based Models]
1. [Optimization story: Bloom inference](https://huggingface.co/blog/bloom-inference-optimization)  
   ![image][Blog] ![image][Deep Learning] ![image][Large Models] ![image][Transformers]
1. [Orca 2: teaching small language models how to reason](https://arxiv.org/abs/2311.11045)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP]
1. [Orca: progressive learning from complex explanation traces of GPT-4](https://arxiv.org/abs/2306.02707)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP]
1. [Outer product-based neural collaborative filtering](https://arxiv.org/abs/1808.03912)  
   ![image][Paper] ![image][Collaborative Filtering] ![image][Deep Learning] ![image][Recommender Systems]
1. [Outrageously large neural networks: the sparsely-gated mixture-of-experts layer](https://arxiv.org/abs/1701.06538)  
   ![image][Paper] ![image][Deep Learning] ![image][Mixture of Experts]
1. [Overcoming oscillations in quantization-aware training](https://arxiv.org/abs/2203.11086)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [PAL: Program-aided language models](https://arxiv.org/abs/2211.10435)  
   ![image][Paper] ![image][Mathematical Reasoning] ![image][NLP]
1. [PaLM: scaling language modeling with pathways](https://arxiv.org/abs/2204.02311)  
   ![image][Paper] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Parallel context windows improve in-context learning of large language models](https://arxiv.org/abs/2212.10947)  
   ![image][Paper] ![image][In Context Learning] ![image][Long Context Length] ![image][NLP]
1. Pattern classification  
   ![image][Book]
1. Pattern recognition and machine learning  
   ![image][Book]
1. [Perceptual losses for real-time style transfer and super-resolution](https://arxiv.org/abs/1603.08155)  
   ![image][Paper] ![image][Computer Vision] ![image][Image Super Resolution]
1. [Personalizing dialogue agents: I have a dog, do you have pets too?](https://arxiv.org/abs/1801.07243)  
   ![image][Paper] ![image][Dialog] ![image][NLP]
1. [Phase-functioned neural networks for character control](https://dl.acm.org/citation.cfm?id=3073663)  
   ![image][Paper] ![image][Computer Graphics] ![image][Deep Learning]
1. [Playing Atari with deep reinforcement learning](https://arxiv.org/abs/1312.5602)  
   ![image][Paper] ![image][Reinforcement Learning]
1. [Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing](https://arxiv.org/abs/2107.13586)  
   ![image][Paper] ![image][Efficient Training] ![image][Few Shot] ![image][NLP] ![image][Prompting]
1. [Prefix-tuning: optimizing continuous prompts for generation](https://arxiv.org/abs/2101.00190)  
   ![image][Paper] ![image][Efficient Training] ![image][Few Shot] ![image][NLP]
1. [Probabilistic latent semantic analysis](https://arxiv.org/abs/1301.6705)  
   ![image][Paper] ![image][Information Retrieval] ![image][NLP]
1. [Progressive growing of GANs from improved quality, stability and variation](https://arxiv.org/abs/1710.10196)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Prompting with pseudo-code instructions](https://arxiv.org/abs/2305.11790)  
   ![image][Paper] ![image][NLP] ![image][Prompting]
1. [Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347)  
   ![image][Paper] ![image][Reinforcement Learning]
1. [PullNet: open domain question answering with iterative retrieval on knowledge bases and text](https://arxiv.org/abs/1904.09537)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [PyTorch trace analysis for the masses](https://pytorch.org/blog/trace-analysis-for-masses/)  
   ![image][Blog] ![image][Deep Learning] ![image][Systems]
1. [Q-BERT: Hessian based ultra low precision quantization of BERT](https://arxiv.org/abs/1909.05840)  
   ![image][Paper] ![image][NLP] ![image][Quantization]
1. [R<sup>3</sup>Net: recurrent residual refinement network for saliency detection](https://www.ijcai.org/proceedings/2018/95)  
   ![image][Paper] ![image][Computer Vision] ![image][Saliency Detection]
1. [Reading Wikipedia to answer open-domain questions](https://arxiv.org/abs/1704.00051)  
   ![image][Paper] ![image][NLP] ![image][Question Answering]
1. [REALM: Retrieval-augmented language model pretraining](https://arxiv.org/abs/2002.08909)  
   ![image][Paper] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Recurrent models of visual attention](https://arxiv.org/abs/1406.6247)  
   ![image][Paper]
1. [Reducing activation recomputation in large transformer models](https://arxiv.org/abs/2205.05198)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Reducing transformer key-value cache size with cross-layer attention](https://arxiv.org/abs/2405.12981)  
   ![image][Paper] ![image][Attention Mechanism] ![image][Efficient Inference] ![image][NLP] ![image][Transformers]
1. [Regularizing and optimizing LSTM language models](https://arxiv.org/abs/1708.02182)  
   ![image][Paper] ![image][Deep Learning] ![image][Optimization] ![image][Regularization]
1. Reinforcement Learning: An Introduction  
   ![image][Book]
1. [ReLoRA: high-rank training through low-rank updates](https://arxiv.org/abs/2307.05695)  
   ![image][Paper] ![image][Deep Learning] ![image][Efficient Training]
1. [Restricted Boltzmann machines for collaborative filtering](https://dl.acm.org/citation.cfm?doid=1273496.1273596)  
   ![image][Paper] ![image][Boltzmann Machines] ![image][Collaborative Filtering] ![image][Deep Learning] ![image][Energy-based Models] ![image][Recommender Systems]
1. [Retrieval augmentation reduces hallucination in conversation](https://arxiv.org/abs/2104.07567)  
   ![image][Paper] ![image][Dialog] ![image][Hallucination] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Retrieval-augmented generation for knowledge-intensive NLP tasks](https://arxiv.org/abs/2005.11401)  
   ![image][Paper] ![image][Hallucination] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Revisiting classifier two-sample tests](https://arxiv.org/abs/1610.06545)  
   ![image][Paper] ![image][Deep Learning] ![image][Unsupervised Learning]
1. [Revisiting MoE and dense speed-accuracy comparisons for LLM training](https://arxiv.org/abs/2405.15052)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Mixture of Experts] ![image][Transformers]
1. [RoBERTa: a robustly optimized BERT pretraining approach](https://arxiv.org/abs/1907.11692)  
   ![image][Paper] ![image][NLP] ![image][Transformers]
1. [RoFormer: enhanced transformer with rotary position embedding](https://arxiv.org/abs/2104.09864)  
   ![image][Paper] ![image][NLP] ![image][Position Embeddings] ![image][Transformers]
1. [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)  
   ![image][Paper] ![image][AI for code] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Scalable hierarchical aggregation protocol (SHArP): a hardware architecture for efficient data reduction](https://ieeexplore.ieee.org/document/7830486)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Networking] ![image][Systems]
1. [Scaling instruction-finetuned language models](https://arxiv.org/abs/2210.11416)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP]
1. [Scaling PyTorch FSDP for training foundation Models on IBM cloud](https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud/)  
   ![image][Blog] ![image][Distributed Training] ![image][Large Models] ![image][Systems]
1. [Scaling transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062)  
   ![image][Paper] ![image][Long Context Length] ![image][NLP]
1. [Scattered mixture-of-experts implementation](https://arxiv.org/abs/2403.08245)  
   ![image][Paper] ![image][Deep Learning] ![image][Mixture of Experts] ![image][Systems]
1. [Self-instruct: aligning language model with self generated instructions](https://arxiv.org/abs/2212.10560)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP]
1. [Self-normalizing neural networks](https://arxiv.org/abs/1706.02515)  
   ![image][Paper] ![image][Activation Function] ![image][Deep Learning] ![image][Normalization]
1. [Semantically equivalent adversarial rules for debugging NLP models](https://aclanthology.org/P18-1079/)  
   ![image][Paper] ![image][NLP]
1. [Seq2seq model and the exposure bias problem](https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097)  
   ![image][Blog] ![image][Generative Models] ![image][NLP]
1. [Sequence parallelism: long sequence training from system perspective](https://arxiv.org/abs/2105.13120)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Long Context Length] ![image][Systems]
1. [Sequential latent knowledge selection for knowledge-grounded dialogue](https://arxiv.org/abs/2002.07510)  
   ![image][Paper] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Simple and effective multi-paragraph reading comprehension](https://arxiv.org/abs/1710.10723)  
   ![image][Paper] ![image][NLP] ![image][Reading Comprehension]
1. [Simplifying transformer blocks](https://arxiv.org/abs/2311.01906)  
   ![image][Paper] ![image][Deep Learning] ![image][Transformers]
1. [SlimPajama-DC: understanding data combinations for LLM training](https://arxiv.org/abs/2309.10818)  
   ![image][Paper] ![image][Data Mixtures] ![image][NLP]
1. [SmoothQuant: accurate and efficient post-training quantization for large language models](https://arxiv.org/abs/2211.10438)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Quantization] ![image][Transformers]
1. [Soft filter pruning for accelerating deep convolutional neural networks](https://arxiv.org/abs/1808.06866)  
   ![image][Paper] ![image][Deep Learning] ![image][Pruning]
1. [SOLAR 10.7B: scaling large language models with simple yet effective depth up-scaling](https://arxiv.org/abs/2312.15166)  
   ![image][Paper] ![image][Model Upcycling] ![image][NLP] ![image][Transformers]
1. [SOLOIST: building task bots at scale with transfer learning and machine teaching](https://arxiv.org/abs/2005.05298)  
   ![image][Paper] ![image][Dialog] ![image][Few Shot] ![image][NLP] ![image][Transfer Learning]
1. [Solving quantitative reasoning problems with language models](https://arxiv.org/abs/2206.14858)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Spatial temporal graph convolutional networks for skeleton-based action recognition](https://arxiv.org/abs/1801.07455)  
   ![image][Paper] ![image][Deep Learning] ![image][Graph Neural Networks] ![image][Spatio-Temporal]
1. [Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting](https://arxiv.org/abs/1709.04875)  
   ![image][Paper] ![image][Deep Learning] ![image][Graph Neural Networks] ![image][Spatio-Temporal] ![image][Time Series]
1. [Spectral normalization for generative adversarial networks](https://arxiv.org/abs/1802.05957)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning]
1. Speech and language processing  
   ![image][Book]
1. [ST-MoE: designing stable and transferable sparse expert models](https://arxiv.org/abs/2202.08906)  
   ![image][Paper] ![image][Mixture of Experts] ![image][Transformers]
1. [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)  
   ![image][Paper] ![image][AI for code] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Sticking the landing: simple, lower-variance gradient estimators for variational inference](https://arxiv.org/abs/1703.09194)  
   ![image][Paper] ![image][Deep Learning] ![image][Gradient Estimation] ![image][Variational Inference]
1. [StitchNet: composing neural networks from pre-trained fragments](https://arxiv.org/abs/2301.01947)  
   ![image][Paper] ![image][Deep Learning] ![image][Growing Neural Networks]
1. [Stochastic hyperparameter optimization through hypernetworks](https://arxiv.org/abs/1802.09419)  
   ![image][Paper] ![image][Deep Learning] ![image][Meta Learning]
1. [Strategies for teaching layered networks classification tasks](https://papers.nips.cc/paper/85-strategies-for-teaching-layered-networks-classification-tasks)  
   ![image][Paper] ![image][Deep Learning]
1. [Structured prompting: scaling in-context learning to 1,000 examples](https://arxiv.org/abs/2212.06713)  
   ![image][Paper] ![image][Deep Learning] ![image][In Context Learning]
1. [Style transfer from non-parallel text by cross-alignment](https://arxiv.org/abs/1705.09655)  
   ![image][Paper] ![image][NLP] ![image][Style Transfer]
1. [Subword regularization: improving neural network translation models with multiple subword candidates](https://arxiv.org/abs/1804.10959)  
   ![image][Paper] ![image][Machine Translation] ![image][NLP] ![image][Regularization]
1. [Supervised learning of probability distributions by neural networks](https://papers.nips.cc/paper/3-supervised-learning-of-probability-distributions-by-neural-networks)  
   ![image][Paper] ![image][Deep Learning]
1. [Supporting efficient large model training on AMD Instinct<sup>TM</sup> GPUs with DeepSpeed](https://cloudblogs.microsoft.com/opensource/2022/03/21/supporting-efficient-large-model-training-on-amd-instinct-gpus-with-deepspeed/)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [Switch transformers: scaling to trillion parameter models with simple and efficient sparsity](https://arxiv.org/abs/2101.03961)  
   ![image][Paper] ![image][Deep Learning] ![image][Mixture of Experts]
1. [Synchronization in neural nets](https://papers.nips.cc/paper/32-synchronization-in-neural-nets)  
   ![image][Paper] ![image][Deep Learning]
1. [Synthetic data (almost) from scratch: generalized instruction tuning for language models](https://arxiv.org/abs/2402.13064)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP] ![image][Synthetic Data Generation]
1. [Tackling the poor assumptions of Naive Bayes text classifiers](https://dl.acm.org/citation.cfm?id=3041916)  
   ![image][Paper] ![image][NLP]
1. [Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer](https://arxiv.org/abs/2203.03466)  
   ![image][Paper] ![image][Deep Learning] ![image][Hyperparameter Search]
1. [TextWorld: a learning environment for text-based games](https://arxiv.org/abs/1806.11532)  
   ![image][Paper] ![image][RL environments] ![image][Reinforcement Learning]
1. [The best of both worlds: combining recent advances in neural machine translation](https://arxiv.org/abs/1804.09849)  
   ![image][Paper] ![image][Machine Translation] ![image][NLP]
1. The elements of statistical learning: data mining, inference and prediction  
   ![image][Book]
1. [The Flan collection: designing data and methods for effective instruction tuning](https://arxiv.org/abs/2301.13688)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP]
1. [The information bottleneck method](https://arxiv.org/abs/physics/0004057)  
   ![image][Paper] ![image][Information Theory]
1. [The Pile: an 800GB dataset of diverse text for language modeling](https://arxiv.org/abs/2101.00027)  
   ![image][Paper] ![image][Dataset] ![image][NLP]
1. [The power of scale for parameter-efficient prompt tuning](https://arxiv.org/abs/2104.08691)  
   ![image][Paper] ![image][Efficient Training] ![image][NLP]
1. [The wisdom of hindsight makes language models better instruction followers](https://arxiv.org/abs/2302.05206)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP]
1. [Thermometer encoding: one hot way to resist adversarial examples](https://openreview.net/forum?id=S18Su--CW)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [To regularize or not to regularize? The bias variance trade-off in regularized AEs](https://arxiv.org/abs/2006.05838)  
   ![image][Paper] ![image][Deep Learning] ![image][Regularization]
1. [Towards crowdsourced training of large neural networks using decentralized mixture-of-experts](https://arxiv.org/abs/2002.04013)  
   ![image][Paper] ![image][Decentralized Training] ![image][Deep Learning] ![image][Large Models]
1. [Towards deep learning models resilient to adversarial attacks](https://arxiv.org/abs/1706.06083)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Towards evaluating the robustness of neural networks](https://arxiv.org/abs/1608.04644)  
   ![image][Paper] ![image][Adversarial Examples] ![image][Deep Learning] ![image][Robustness]
1. [Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/abs/2108.12409)  
   ![image][Paper] ![image][Embeddings] ![image][NLP] ![image][Transformers]
1. [Training compute-optimal large language models](https://arxiv.org/abs/2203.15556)  
   ![image][Paper] ![image][Deep Learning] ![image][Large Models] ![image][Transformers]
1. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)  
   ![image][Paper] ![image][Instruction Finetuning] ![image][NLP] ![image][Reinforcement Learning]
1. [Transformer memory as a differentiable search index](https://arxiv.org/abs/2202.06991)  
   ![image][Paper] ![image][Information Retrieval] ![image][NLP] ![image][Retrieval-Augmented Generation]
1. [Transformer quality in linear time](https://arxiv.org/abs/2202.10447)  
   ![image][Paper] ![image][Deep Learning] ![image][Transformers]
1. [Transformer-XL: attentive language models beyond a fixed-length context](https://arxiv.org/abs/1901.02860)  
   ![image][Paper] ![image][Long Context Length] ![image][NLP] ![image][Transformers]
1. [Transformers explained visually (part 1): overview of functionality](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)  
   ![image][Blog] ![image][Deep Learning] ![image][Transformers]
1. [Transformers explained visually (part 2): how it works, step-by-step](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)  
   ![image][Blog] ![image][Deep Learning] ![image][Transformers]
1. [Transformers explained visually (part 3): multi-head attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)  
   ![image][Blog] ![image][Deep Learning] ![image][Transformers]
1. [Turing-NLG: a 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)  
   ![image][Blog] ![image][Deep Learning] ![image][Large Models] ![image][Transformers]
1. [UL2: unifying language learning paradigms](https://arxiv.org/abs/2205.05131)  
   ![image][Paper] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Understanding convolutional neural networks with a mathematical model](https://arxiv.org/abs/1609.04112)  
   ![image][Paper] ![image][Computer Vision]
1. [Understanding disentangling in Î²-VAE](https://arxiv.org/abs/1804.03599)  
   ![image][Paper] ![image][Deep Learning] ![image][Disentanglement] ![image][Variational Inference]
1. [Understanding the Open Pre-Trained Transformers (OPT) library](https://towardsdatascience.com/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)  
   ![image][Blog] ![image][Deep Learning] ![image][Transformers]
1. [Unit tests for stochastic optimization](https://arxiv.org/abs/1312.6055)  
   ![image][Paper] ![image][Deep Learning] ![image][Optimization]
1. [Universal language model fine-tuning for text classification](https://arxiv.org/abs/1801.06146)  
   ![image][Paper] ![image][NLP] ![image][Text Classification]
1. [Unlimiformer: long-range transformers with unlimited length input](https://arxiv.org/abs/2305.01625)  
   ![image][Paper] ![image][Long Context Length] ![image][NLP] ![image][Transformers]
1. [Unpaired image-to-image translation using cycle-consistent adversarial networks](https://arxiv.org/abs/1703.10593)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Unsupervised machine translation using monolingual corpora only](https://arxiv.org/abs/1711.00043)  
   ![image][Paper] ![image][Machine Translation] ![image][NLP] ![image][Unsupervised Learning]
1. [Unsupervised representation learning by predicting image rotations](https://arxiv.org/abs/1803.07728)  
   ![image][Paper] ![image][Computer Vision] ![image][Unsupervised Learning]
1. [Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, the worldâ€™s largest and most powerful generative language model](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)  
   ![image][Blog] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Variational inference using implicit distributions](https://arxiv.org/abs/1702.08235)  
   ![image][Paper] ![image][Deep Learning] ![image][Variational Inference]
1. [Variational inference with latent space quantization for adversarial resilience](https://arxiv.org/abs/1903.09940)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization] ![image][Robustness] ![image][Variational Inference]
1. [Variational learning for unsupervised knowledge grounded dialogs](https://arxiv.org/abs/2112.00653)  
   ![image][Paper] ![image][Dialog] ![image][NLP] ![image][Variational Inference]
1. [Variational lossy autoencoder](https://arxiv.org/abs/1611.02731)  
   ![image][Paper] ![image][Deep Learning] ![image][Variational Inference]
1. [Vector-quantized input-contextualized soft prompts for natural language understanding](https://arxiv.org/abs/2205.11024)  
   ![image][Paper] ![image][Efficient Training] ![image][NLP]
1. [VEEGAN: reducing mode collapse in GANs using implicit variational learning](https://arxiv.org/abs/1705.07761)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556)  
   ![image][Paper] ![image][Computer Vision] ![image][Image Classification]
1. [Visual instruction tuning](https://arxiv.org/abs/2304.08485)  
   ![image][Paper] ![image][Computer Vision] ![image][Instruction Finetuning] ![image][Multi-modal]
1. [Visualizing data using t-SNE](http://www.jmlr.org/papers/v9/vandermaaten08a.html)  
   ![image][Paper] ![image][Data Visualization] ![image][Deep Learning]
1. [Wasserstein GAN](https://arxiv.org/abs/1701.07875)  
   ![image][Paper] ![image][Adversarial Learning] ![image][Deep Learning] ![image][GAN]
1. [wav2vec 2.0: a framework for self-supervised learning of speech representations](https://arxiv.org/abs/2006.11477)  
   ![image][Paper] ![image][Deep Learning] ![image][Speech] ![image][Transformers]
1. [Wavenet: a generative model for raw audio](https://arxiv.org/abs/1609.03499)  
   ![image][Paper] ![image][Audio] ![image][Deep Learning]
1. [WebGPT: browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)  
   ![image][Paper] ![image][Human Feedback] ![image][Reinforcement Learning] ![image][Transformers]
1. [What language model to train if you have one million GPU hours?](https://openreview.net/forum?id=rI7BL3fHIZq)  
   ![image][Paper] ![image][Distributed Training] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [Will GPT-4 run DOOM?](https://arxiv.org/abs/2403.05468)  
   ![image][Paper] ![image][Gaming] ![image][Planning]
1. [Word translation without parallel data](https://openreview.net/forum?id=H196sainb)  
   ![image][Paper] ![image][Machine Translation] ![image][NLP]
1. [Writing CUDA kernels for PyTorch](https://tinkerd.net/blog/machine-learning/cuda-basics/)  
   ![image][Blog] ![image][Systems]
1. [Yandex publishes YaLM 100B. Itâ€™s the largest GPT-like neural network in open source](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6)  
   ![image][Blog] ![image][Large Models] ![image][NLP] ![image][Transformers]
1. [You only cache once: decoder-decoder architectures for language models](https://arxiv.org/abs/2405.05254)  
   ![image][Paper] ![image][Efficient Inference]
1. [You only look once: unified, real-time object detection](https://arxiv.org/abs/1506.02640)  
   ![image][Paper] ![image][Computer Vision] ![image][Object Detection]
1. [ZeRO & DeepSpeed: new system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [ZeRO++: Extremely efficient collective communication for giant model training](https://arxiv.org/abs/2306.10209)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Systems]
1. [ZeRO-2 & DeepSpeed: shattering barriers of deep learning speed & scale](https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/)  
   ![image][Blog] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models]
1. [ZeRO-Infinity: breaking the GPU memory wall for extreme scale deep learning](https://arxiv.org/abs/2104.07857)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [Zero-shot text-to-image generation](https://arxiv.org/abs/2102.12092)  
   ![image][Paper] ![image][Deep Learning] ![image][Variational Inference] ![image][Zero Shot]
1. [ZeRO: memory optimizations toward training trillion parameter models](https://arxiv.org/abs/1910.02054)  
   ![image][Paper] ![image][Deep Learning] ![image][Distributed Training] ![image][Large Models] ![image][Transformers]
1. [ZeroQuant: efficient and affordable post-training quantization for large-scale transformers](https://arxiv.org/abs/2206.01861)  
   ![image][Paper] ![image][Deep Learning] ![image][Quantization]
1. [Î²-VAE: learning basic visual concepts with a constrained variational framework](https://openreview.net/forum?id=Sy2fzU9gl)  
   ![image][Paper] ![image][Deep Learning] ![image][Variational Inference]
1. [ðŸ· FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  
   ![image][Blog] ![image][Data Curation] ![image][NLP]

# Calculus

1. Calculus of variations  
   ![image][Book]
1. Thomas' calculus  
   ![image][Book]

# Computer Architecture

1. [Accelerated computing with a reconfigurable dataflow architecture](https://sambanova.ai/wp-content/uploads/2021/04/SambaNova_Accelerated-Computing-with-a-Reconfigurable-Dataflow-Architecture_Whitepaper_English.pdf)  
   ![image][Whitepaper] ![image][Dataflow Architecture] ![image][Deep Learning]
1. Computer architecture: a quantitative approach  
   ![image][Book]
1. Computer organization and design ARM edition: the hardware software interface  
   ![image][Book]
1. [Flipping bits in memory without accessing them: an experimental study of DRAM disturbance errors](https://ieeexplore.ieee.org/document/6853210)  
   ![image][Paper] ![image][Computer Architecture] ![image][Memory] ![image][Security]
1. [Improving DRAM performance by parallelizing refreshes with accesses](https://ieeexplore.ieee.org/document/6835946)  
   ![image][Paper] ![image][Computer Architecture] ![image][Memory] ![image][Security] ![image][Systems]
1. [Memory performance attacks: denial of memory service in multi-core systems](https://www.usenix.org/conference/16th-usenix-security-symposium/memory-performance-attacks-denial-memory-service-multi)  
   ![image][Paper] ![image][Computer Architecture] ![image][Memory] ![image][Security]
1. [Memory scaling: a systems architecture perspective](https://ieeexplore.ieee.org/document/6582088)  
   ![image][Paper] ![image][Computer Architecture] ![image][Memory]
1. [Millicode in an IBM zSeries processor](https://ieeexplore.ieee.org/document/5388884)  
   ![image][Paper] ![image][Computer Architecture]
1. [MTIA v1: Meta's first-generation AI inference accelerator](https://ai.facebook.com/blog/meta-training-inference-accelerator-AI-MTIA/)  
   ![image][Blog] ![image][ASIC] ![image][Deep Learning]
1. [RAIDR: Retention-Aware Intelligent DRAM Refresh](https://dl.acm.org/doi/10.5555/2337159.2337161)  
   ![image][Paper] ![image][Computer Architecture] ![image][Memory]
1. [Stall-time fair memory access scheduling for chip multiprocessors](https://ieeexplore.ieee.org/document/4408252)  
   ![image][Paper] ![image][Computer Architecture] ![image][Memory]

# Computer Graphics

1. [Principles of traditional animation applied to 3D computer animation](https://dl.acm.org/doi/10.1145/37402.37407)  
   ![image][Paper] ![image][Animation] ![image][Computer Graphics]

# Data Structures and Algorithms

1. Data structures and algorithms in Java  
   ![image][Book]
1. Introduction to algorithms  
   ![image][Book]

# Digital Electronics

1. Digital design: with an introduction to the Verilog HDL  
   ![image][Book]

# Graph Theory

1. Introduction to graph theory  
   ![image][Book]

# Information Theory

1. Elements of information theory  
   ![image][Book]
1. [Error detecting and error correcting codes](https://ieeexplore.ieee.org/document/6772729)  
   ![image][Paper] ![image][Error Correction] ![image][Error Detection] ![image][Information Theory]

# Linear Algebra

1. Linear algebra and its applications  
   ![image][Book]
1. Matrix analysis and applied linear algebra  
   ![image][Book]
1. The matrix cookbook  
   ![image][Book]

# Measure Theory

1. Measure theory  
   ![image][Book]

# Optimization Theory

1. Convex Optimization  
   ![image][Book]
1. [Distributed optimization and statistical learning via the alternating direction method of multipliers](https://ieeexplore.ieee.org/document/8186925)  
   ![image][Book]

# Probability and Stochastic Processes

1. Introduction to probability and stochastic processes with applications  
   ![image][Book]

# Quantum Computing

1. [A fast quantum mechanical algorithm for database search](https://arxiv.org/abs/quant-ph/9605043)  
   ![image][Paper] ![image][Quantum Algorithms] ![image][Quantum Computing]
1. [A single quantum cannot be cloned](https://www.nature.com/articles/299802a0)  
   ![image][Paper] ![image][Quantum Computing]
1. [Can quantum-mechanical description of physical reality be considered complete](https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777)  
   ![image][Paper] ![image][Quantum Computing]
1. [Image recognition with an adiabatic quantum computer I. mapping to quadratic unconstrained binary optimization](https://arxiv.org/abs/0804.4457)  
   ![image][Paper] ![image][Image Classification] ![image][QUBO] ![image][Quantum Computing]
1. [Integer optimization toolbox (minimizing polynomials over integer lattices using quantum annealing)](https://1qbit.com/whitepaper/integer-optimization-toolbox/)  
   ![image][Whitepaper]
1. [Limits on parallel speedup for classical Ising model solvers](https://www.dwavesys.com/resources/white-paper/limits-on-parallel-speedup-for-classical-ising-model-solvers/)  
   ![image][Whitepaper]
1. [Partitioning optimization problems for hybrid classical/quantum execution](https://docs.ocean.dwavesys.com/projects/qbsolv/en/latest/_downloads/bd15a2d8f32e587e9e5997ce9d5512cc/qbsolv_techReport.pdf)  
   ![image][Whitepaper]
1. [Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer](https://arxiv.org/abs/quant-ph/9508027)  
   ![image][Paper] ![image][Quantum Algorithms] ![image][Quantum Computing]
1. [Probabilistic cloning and identification of linearly independent quantum states](https://arxiv.org/abs/quant-ph/9804064)  
   ![image][Paper] ![image][Cloning] ![image][Quantum Computing]
1. [Programming with D-Wave: map coloring problem](https://www.dwavesys.com/resources/white-paper/programming-with-d-wave-map-coloring-problem/)  
   ![image][Whitepaper]
1. Quantum computation and quantum information  
   ![image][Book]
1. Quantum computing: a gentle introduction  
   ![image][Book]
1. [Quantum performance evaluation: a short reading list](https://www.dwavesys.com/resources/white-paper/quantum-performance-evaluation-a-short-reading-list/)  
   ![image][Whitepaper]
1. [Quantum theory, the Church-Turing principle and the universal quantum computer](https://royalsocietypublishing.org/doi/10.1098/rspa.1985.0070)  
   ![image][Paper] ![image][Quantum Computing] ![image][Theory of Computation]
1. [Rapid solution of problems by quantum computation](https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1992.0167)  
   ![image][Paper] ![image][Quantum Algorithms] ![image][Quantum Computing]
1. [Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.70.1895)  
   ![image][Paper] ![image][Quantum Computing] ![image][Quantum Teleportation]

# Signal Processing

1. Discrete-time signal processing  
   ![image][Book]
1. Foundations of Signal Processing  
   ![image][Book]
1. Signals and systems  
   ![image][Book]
1. Understanding digital signal processing  
   ![image][Book]
